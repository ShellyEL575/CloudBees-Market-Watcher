# main.py â€” Patched Unified Runner (Optional Convenience Entrypoint)

import os
import json
from datetime import datetime

from scraper.hn import fetch_hn_stories
from scraper.competitor import fetch_competitor_updates
from scraper.google_watcher import fetch_google_results
from scraper.trend_classifier import classify_trends

from summarizer import generate_summary, extract_insights_from_social
from utils import group_posts_by_topic


# -------------------------
# SCRAPER PHASE
# -------------------------
def run_scraper():
    print("\nğŸ“¥ Collecting posts...\n")
    all_posts = []

    # Hacker News
    try:
        hn_posts = fetch_hn_stories()
        print(f"âœ… HN posts: {len(hn_posts)}")
        all_posts.extend(hn_posts)
    except Exception as e:
        print(f"âŒ Hacker News scraping failed: {e}")

    # Competitor Blogs
    try:
        competitor_posts = fetch_competitor_updates()
        print(f"âœ… Competitor posts: {len(competitor_posts)}")
        all_posts.extend(competitor_posts)
    except Exception as e:
        print(f"âŒ Competitor scraping failed: {e}")

    # Google Search (Serper.dev)
    try:
        google_posts = fetch_google_results()
        print(f"âœ… Google posts: {len(google_posts)}")
        all_posts.extend(google_posts)
    except Exception as e:
        print(f"âŒ Google search failed: {e}")

    print(f"\nğŸ“Œ Total collected: {len(all_posts)}")

    # Trend classification
    for post in all_posts:
        matches = classify_trends([post])
        post["is_trend"] = len(matches) > 0

    os.makedirs("data", exist_ok=True)
    with open("data/posts.json", "w", encoding="utf-8") as f:
        json.dump(all_posts, f, indent=2)

    print("âœ… Saved posts to data/posts.json\n")


# -------------------------
# SUMMARIZATION PHASE
# -------------------------
def run_summarizer():
    print("ğŸ§  Loading posts...\n")

    with open("data/posts.json", "r", encoding="utf-8") as f:
        posts = json.load(f)

    print(f"âœ… Loaded {len(posts)} posts\n")

    print("ğŸ“Š Grouping posts...")
    grouped = group_posts_by_topic(posts)

    print("âœï¸ Generating summaries...\n")
    summary_sections = {
        "ğŸš€ Product Updates": generate_summary(grouped.get("ğŸš€ Product Updates", [])),
        "ğŸ’¬ Social Buzz": generate_summary(grouped.get("ğŸ’¬ Social Buzz", [])),
        "ğŸ“ˆ Trends": generate_summary(grouped.get("ğŸ“ˆ Trends", [])),
    }

    print("ğŸ” Extracting insights...\n")
    summary_sections["ğŸ§  Insights"] = extract_insights_from_social(posts)

    # Output result
    report_date = datetime.utcnow().strftime("%Y-%m-%d")
    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/{report_date}.md"

    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“° CloudBees Market Watch â€“ {report_date}\n\n")

        for section, content in summary_sections.items():
            if content.strip():
                f.write(f"## {section}\n{content}\n\n---\n\n")

    print(f"âœ… Report saved to {report_path}\n")


# -------------------------
# COMBINED RUNNER
# -------------------------
if __name__ == "__main__":
    run_scraper()
    run_summarizer()
