# main.py (optional legacy entrypoint â€“ now split into two phases)
import os
import json
from datetime import datetime
from scraper.competitor import fetch_competitor_updates
from scraper.reddit import fetch_reddit_discussions
from scraper.hn import fetch_hn_stories
from scraper.google_watcher import fetch_google_results
from summarizer import generate_summary, extract_insights_from_social
from utils import group_posts_by_topic


def run_scraper():
    print("\nğŸ“¥ Collecting posts...")
    all_posts = []

    try:
        competitor_posts = fetch_competitor_updates()
        print(f"âœ… Competitor posts: {len(competitor_posts)}")
        all_posts.extend(competitor_posts)
    except Exception as e:
        print("âŒ Competitor scraping failed:", e)

    try:
        reddit_posts = fetch_reddit_discussions()
        print(f"âœ… Reddit posts: {len(reddit_posts)}")
        all_posts.extend(reddit_posts)
    except Exception as e:
        print("âŒ Reddit scraping failed:", e)

    try:
        hn_posts = fetch_hn_stories()
        print(f"âœ… HN posts: {len(hn_posts)}")
        all_posts.extend(hn_posts)
    except Exception as e:
        print("âŒ HN scraping failed:", e)

    try:
        google_posts = fetch_google_results()
        print(f"âœ… Google posts: {len(google_posts)}")
        all_posts.extend(google_posts)
    except Exception as e:
        print("âŒ Google search failed:", e)

    os.makedirs("data", exist_ok=True)
    with open("data/raw_posts.json", "w") as f:
        json.dump(all_posts, f, indent=2)

    print(f"\nâœ… Saved {len(all_posts)} posts to data/raw_posts.json")


def run_summarizer():
    print("\nğŸ§  Loading raw posts for summarization...")

    with open("data/raw_posts.json") as f:
        all_posts = json.load(f)

    print(f"âœ… Loaded {len(all_posts)} posts")

    print("\nğŸ“Š Grouping posts...")
    grouped = group_posts_by_topic(all_posts)

    print("\nâœï¸ Generating summary...")
    summary = generate_summary(grouped)

    print("\nğŸ” Extracting insights from social buzz...")
    social_insights = extract_insights_from_social(grouped.get("ğŸ’¬ Social Buzz", []))

    report_date = datetime.utcnow().strftime("%Y-%m-%d")
    os.makedirs("reports", exist_ok=True)
    report_path = f"reports/{report_date}.md"

    with open(report_path, "w") as f:
        f.write(f"# Market Watch Report â€“ {report_date}\n\n")
        f.write(summary)
        f.write("\n\n===== ğŸ“Š Social Buzz Insights =====\n")
        f.write(social_insights)

    print(f"\nâœ… Report saved to {report_path}")


if __name__ == "__main__":
    run_scraper()
    run_summarizer()
