# üì∞ CloudBees Market Watch ‚Äì 2025-11-24

## üöÄ Product Updates
- [CD-i](https://en.wikipedia.org/wiki/CD-i) ‚Äî <p>Article URL: <a href="https://en.wikipedia.org/wiki/CD-i">https://en.wikipedia.org/wiki/CD-i</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=46021572">https://news.ycombinator.com/item?id=46021572</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [Show HN: Quick install script for self-hosted Forgejo (Git+CI) server](https://wkoszek.github.io/easyforgejo/) ‚Äî <p>I wanted to get Forgejo on my VM on a local NAS fast, and I realized that getting everything running with Git and CI working was ... harder than I anticipated. After spending more time than I wanted, and bugging lovely people at Forgejo's matrix, I came up with this:<p><a href="https://wkoszek.github.io/easyforgejo/" rel="nofollow">https://wkoszek.github.io/easyforgejo/</a><p>With this script, you should get Forgejo installed on your Linux computer in 2min. I tested this on a VM for now, and it works well enough for beta launch.<p>Repo is here:<p><a href="https://github.com/wkoszek/easyforgejo" rel="nofollow">https://github.com/wkoszek/easyforgejo</a><p>Let me know what you think and submit PRs if you find bugs. I'd not use it in production just yet.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45994118">https://news.ycombinator.com/item?id=45994118</a></p>
<p>Points: 4</p>
<p># Comments: 2</p>
- [Show HN: Haloy ‚Äì an open‚Äësource, lightweight deployment system for Docker apps](https://github.com/haloydev/haloy) ‚Äî <p>Haloy is a lightweight way to deploy Docker apps to your own servers without complex setups. One config file, one deploy command. Handles routing, HTTPS, rollbacks, and multi‚Äëserver environments.<p>Repo: <a href="https://github.com/haloydev/haloy" rel="nofollow">https://github.com/haloydev/haloy</a>
Docs: <a href="https://haloy.dev" rel="nofollow">https://haloy.dev</a></p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45992412">https://news.ycombinator.com/item?id=45992412</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [Emulator Bugs: Sega CD](https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/) ‚Äî <p>Article URL: <a href="https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/">https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45992158">https://news.ycombinator.com/item?id=45992158</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [Emulator Bugs: Sega CD](https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/) ‚Äî <p>Article URL: <a href="https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/">https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45981927">https://news.ycombinator.com/item?id=45981927</a></p>
<p>Points: 5</p>
<p># Comments: 0</p>
- [Show HN: MemBrowse - CI/CD memory footprint tracking for embedded firmware](https://membrowse.com) ‚Äî <p>Built this after working with Intel‚Äôs Bluetooth firmware team - we kept seeing builds suddenly fail because memory crept up a few bytes at a time until it overflowed. Then everyone lost hours/days bisecting symbols and linker maps to figure out which commit pushed it over.<p>So I made a CI tool that tracks firmware memory footprint across commits and flags bloat before it breaks the build.<p>How it works:<p>CLI parses ELF + DWARF, extracts per-section/per-symbol/per-file size info<p>CI uploads reports; the platform stores history and generates diffs<p>Shows exactly what changed between commits (what grew, where, and by how much)<p>Optional memory budgets via commit keywords (acts as a CI gate to block regressions)<p>The report generator is open source: <a href="https://github.com/membrowse/membrowse-action" rel="nofollow">https://github.com/membrowse/membrowse-action</a><p>Works with GitHub Actions or any CI, tested on ARM, ESP32, ARC, x86 (multiple toolchains).<p>There‚Äôs also a live demo analyzing MicroPython firmware builds at membrowse.com.<p>Curious to hear from embedded folks:
* How do you track code size / memory usage today?<p>* What would stop you from adding this type of check to CI?<p>* What‚Äôs missing that would make this genuinely useful for your workflow?</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45979698">https://news.ycombinator.com/item?id=45979698</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Runme: DevOps Notebooks Built with Markdown](https://runme.dev) ‚Äî <p>Article URL: <a href="https://runme.dev">https://runme.dev</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45969961">https://news.ycombinator.com/item?id=45969961</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Caching Playwright on CI [video]](https://www.youtube.com/watch?v=_BcCHW6OgQ4) ‚Äî <p>Article URL: <a href="https://www.youtube.com/watch?v=_BcCHW6OgQ4">https://www.youtube.com/watch?v=_BcCHW6OgQ4</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45959431">https://news.ycombinator.com/item?id=45959431</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [The Deployment Age](https://reactionwheel.net/2015/10/the-deployment-age.html) ‚Äî <p>Article URL: <a href="https://reactionwheel.net/2015/10/the-deployment-age.html">https://reactionwheel.net/2015/10/the-deployment-age.html</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45949585">https://news.ycombinator.com/item?id=45949585</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [ArkA ‚Äì an open video protocol with full CI/CD](https://github.com/baconpantsuppercut/arkA) ‚Äî <p>Article URL: <a href="https://github.com/baconpantsuppercut/arkA">https://github.com/baconpantsuppercut/arkA</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45947981">https://news.ycombinator.com/item?id=45947981</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Production-Grade Container Deployment with Podman Quadlets ‚Äì Larvitz Blog](https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html) ‚Äî <p>Article URL: <a href="https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html">https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45945200">https://news.ycombinator.com/item?id=45945200</a></p>
<p>Points: 62</p>
<p># Comments: 20</p>
- [Ask HN: Where to Migrate as an IT Support/DevOps Engineer for Work?](https://news.ycombinator.com/item?id=45942862) ‚Äî <p>Whatever I do and learn no progress is going to happen in my country(nepal).<p>There is no benefit of having merit in Nepal. Private jobs are already low paying. Remote jobs do not come generally to Nepal.I see remote jobs love India and south east asia. I do not get the point of remote job if they are hiring from specific country.
The only thing I can do in nepal is public service commission (civil services) and crack computer engineer. But the pay is meagre there as well, unless I am lucky enough to enter central bank of Nepal(NRB).<p>Honestly, it feels like I am pushing not just a wall but universe itself in Nepal. Because nothing is going to happen irrespective of my abilities. I am currently preparing for PSC and I do not believe I will be happy as a PSC engineer even if I end up at NRB(central bank).<p>Something feels missing inside me. I have took countless therapies and what not. They helped me a lot to be where I am at now. I feel scared to try opportunities out of my comfort zone (kathmandu is my comfort zone).<p>As an adult, nobody pushes you, you have to push yourself. I am in a serious deadlock internally. I can decrease the effect using yoga and meditation but that does not troubleshoot the cause. Personally, I want to pursue something academic away from nepal. I believe that would provide me the much needed confidence in my life.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45942862">https://news.ycombinator.com/item?id=45942862</a></p>
<p>Points: 3</p>
<p># Comments: 4</p>
- [Docker Compose Continuous Deployment](https://github.com/kimdre/doco-cd) ‚Äî <p>Article URL: <a href="https://github.com/kimdre/doco-cd">https://github.com/kimdre/doco-cd</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45937846">https://news.ycombinator.com/item?id=45937846</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [The short, happy reign of CD-ROM (2024)](https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst) ‚Äî <p>Article URL: <a href="https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst">https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45933285">https://news.ycombinator.com/item?id=45933285</a></p>
<p>Points: 3</p>
<p># Comments: 1</p>
- [Three things I've learned about Git while building a CI/CD tool](https://www.ocuroot.com/blog/things-i-learned-about-git/) ‚Äî <p>Article URL: <a href="https://www.ocuroot.com/blog/things-i-learned-about-git/">https://www.ocuroot.com/blog/things-i-learned-about-git/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45927686">https://news.ycombinator.com/item?id=45927686</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [What to do with 5900 blank CD-Rs?](https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/) ‚Äî <p>Article URL: <a href="https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/">https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45926312">https://news.ycombinator.com/item?id=45926312</a></p>
<p>Points: 4</p>
<p># Comments: 1</p>
- [Docker Compose Continuous Deployment](https://github.com/kimdre/doco-cd) ‚Äî <p>Article URL: <a href="https://github.com/kimdre/doco-cd">https://github.com/kimdre/doco-cd</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45895328">https://news.ycombinator.com/item?id=45895328</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Cut cloud costs with the CI Arcade (satire. enjoy. smile.)](https://blog.misfit.dev/the-ci-arcade/) ‚Äî <p>Article URL: <a href="https://blog.misfit.dev/the-ci-arcade/">https://blog.misfit.dev/the-ci-arcade/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45891421">https://news.ycombinator.com/item?id=45891421</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [Level up design-to-code collaboration with GitHub‚Äôs open source Annotation Toolkit](https://github.blog/enterprise-software/collaboration/level-up-design-to-code-collaboration-with-githubs-open-source-annotation-toolkit/) ‚Äî <p>Prevent accessibility issues before they reach production. The Annotation Toolkit brings clarity, compliance, and collaboration directly into your Figma workflow.</p>
<p>The post <a href="https://github.blog/enterprise-software/collaboration/level-up-design-to-code-collaboration-with-githubs-open-source-annotation-toolkit/">Level up design-to-code collaboration with GitHub‚Äôs open source Annotation Toolkit</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [Highlights from Git 2.52](https://github.blog/open-source/git/highlights-from-git-2-52/) ‚Äî <p>The open source Git project just released Git 2.52. Here is GitHub‚Äôs look at some of the most interesting features and changes introduced since last time.</p>
<p>The post <a href="https://github.blog/open-source/git/highlights-from-git-2-52/">Highlights from Git 2.52</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [GitHub Availability Report: October 2025](https://github.blog/news-insights/company-news/github-availability-report-october-2025/) ‚Äî <p>In October, we experienced four incidents that resulted in degraded performance across GitHub services. </p>
<p>The post <a href="https://github.blog/news-insights/company-news/github-availability-report-october-2025/">GitHub Availability Report: October 2025</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [TypeScript, Python, and the AI feedback loop changing software development](https://github.blog/news-insights/octoverse/typescript-python-and-the-ai-feedback-loop-changing-software-development/) ‚Äî <p>An interview with the leader of GitHub Next, Idan Gazit, on TypeScript, Python, and what comes next. </p>
<p>The post <a href="https://github.blog/news-insights/octoverse/typescript-python-and-the-ai-feedback-loop-changing-software-development/">TypeScript, Python, and the AI feedback loop changing software development</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [What 986 million code pushes say about the developer workflow in 2025](https://github.blog/news-insights/octoverse/what-986-million-code-pushes-say-about-the-developer-workflow-in-2025/) ‚Äî <p>Nearly a billion commits later, the way we ship code has changed for good. Here‚Äôs what the 2025 Octoverse data says about how devs really work now.</p>
<p>The post <a href="https://github.blog/news-insights/octoverse/what-986-million-code-pushes-say-about-the-developer-workflow-in-2025/">What 986 million code pushes say about the developer workflow in 2025</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [GitLab engineer: How I improved my onboarding experience with AI](https://about.gitlab.com/blog/gitlab-engineer-how-i-improved-my-onboarding-experience-with-ai/) ‚Äî <p>Starting a new job is exciting, and overwhelming. New teammates, new tools, and, in GitLab‚Äôs case, a lot of documentation. Six weeks ago, I joined GitLab‚Äôs Growth team as a fullstack engineer. Anyone who has gone through <a href="https://about.gitlab.com/the-source/platform/how-to-accelerate-developer-onboarding-and-why-it-matters/">onboarding at GitLab</a> knows it‚Äôs transparent, extensive, and thorough.</p>
<p>GitLab's onboarding process includes a lot of docs, videos, and trainings that will bring you up to speed. Also, in line with GitLab's values, my team encouraged me to start contributing right away. I quickly realized that onboarding here is both diligent and intense. Luckily, I had a secret helper: <a href="https://about.gitlab.com/gitlab-duo/">GitLab Duo</a>.</p>
<h2>My main use cases</h2>
<p>I‚Äôve found GitLab Duo's AI assistance, available throughout the software development lifecycle, useful in three key areas: exploration, reviewing, and debugging. With GitLab Duo, I was able to get my first tiny MR deployed to production in the first week and actively contribute to <a href="https://about.gitlab.com/releases/2025/10/16/gitlab-18-5-released/#pick-up-where-you-left-off-on-the-new-personal-homepage">the personal homepage</a> in GitLab 18.5 in the weeks after.</p>
<h3>Exploration</h3>
<p>Early in onboarding, I often remembered reading something but couldn‚Äôt recall where. GitLab has a <a href="https://handbook.gitlab.com/">public-facing handbook</a>, an internal handbook, and <a href="https://docs.gitlab.com/">GitLab Docs</a>. It can be difficult to search across all of them efficiently.</p>
<p>GitLab Duo simplifies this task: I can describe what I‚Äôm looking for in natural language via <a href="https://docs.gitlab.com/user/gitlab_duo_chat/">GitLab Duo Chat</a> and search across all resources at once.</p>
<p>Example prompt:</p>
<blockquote>
<p>I remember reading about how RSpec tests are done at GitLab. Can you find relevant documentation across the Handbook, the internal handbook and the GitLab Docs?</p>
</blockquote>
<p>Before starting work on an issue, I use GitLab Duo to identify edge cases and hidden dependencies. GitLab Duo will relate the requirements of the issue against the whole GitLab codebase, assess similar features, and prepare all the findings. Based on its output I am able to refine the issue with my product manager and designer, and make sure my implementation covers all edge cases or define future iterations.</p>
<p>Example prompt:</p>
<blockquote>
<p>Analyze this issue in the context of its epic and identify:</p>
<ul>
<li>Implementation questions to ask PM/design before coding</li>
<li>Edge cases not covered in requirements</li>
<li>Cross-feature dependencies that might be affected</li>
<li>Missing acceptance criteria</li>
</ul>
</blockquote>
<p>I also check that my planned solution follows GitLab best practices and common patterns.</p>
<p>Example prompt:</p>
<blockquote>
<p>I want to implement XZY behavior ‚Äî how is this usually done at GitLab, and what other options do I have?</p>
</blockquote>
<h3>Reviewing</h3>
<p>I always <a href="https://docs.gitlab.com/user/project/merge_requests/duo_in_merge_requests/#have-gitlab-duo-review-your-code">let GitLab Duo review my merge requests</a> before assigning human reviewers. It often catches small mistakes, suggests improvements, and highlights edge cases I missed. This shortens the review cycle and helps my teammates focus on more complex and bigger-picture feedback.</p>
<p>Since I‚Äôm still new to GitLab‚Äôs codebase and coding practices, some review comments are hard to interpret. In those cases, GitLab Duo helps me understand what a reviewer means and how it relates to my code.</p>
<p>Example prompt:</p>
<blockquote>
<p>I don‚Äôt understand the comment on this MR about following the user instead of testing component internals, what does it mean and how does it relate to my implementation?</p>
</blockquote>
<h3>Debugging</h3>
<p>Sometimes pipeline tests on my merge requests failed unexpectedly. If I can‚Äôt tell whether my changes are the cause, GitLab Duo helps me investigate and fix the failures. Using <a href="https://docs.gitlab.com/user/gitlab_duo_chat/agentic_chat/">GitLab Duo Agentic Chat</a>, Duo can apply changes to debug the failing job.</p>
<p>Example prompt:</p>
<blockquote>
<p>The pipeline job ‚Äúrspec system pg16 12/32‚Äù is failing, but I don‚Äôt know whether that relates to my changes. Can you check, if my changes are causing the pipeline failure and, if so, guide me through the steps of fixing it.</p>
</blockquote>
<h2>How Duo aligns with GitLab‚Äôs values</h2>
<p>Using GitLab Duo doesn‚Äôt just help me, it also supports <a href="https://handbook.gitlab.com/handbook/values/">GitLab‚Äôs CREDIT values</a>:</p>
<ul>
<li>
<p><strong>Collaboration:</strong> I ask teammates fewer basic questions. And when I do ask questions, they‚Äôre more thoughtful and informed. This respects their time.</p>
</li>
<li>
<p><strong>Results for customers:</strong> By identifying edge cases early and improving code quality, GitLab Duo helps me deliver better outcomes for customers.</p>
</li>
<li>
<p><strong>Efficiency:</strong> Streamlined preparation, faster reviews, and improved debugging make me more efficient.</p>
</li>
<li>
<p><strong>Diversity, inclusion &amp; belonging:</strong> AI guidance might mitigate misunderstandings and different barriers to entry based on differing backgrounds and abilities.</p>
</li>
<li>
<p><strong>Iteration:</strong> The ability to try ideas faster and identify potential improvements, enables faster iteration.</p>
</li>
<li>
<p><strong>Transparency:</strong> GitLab Duo makes the already transparent documentation at GitLab more accessible.</p>
</li>
</ul>
<h2>Staying cautious with AI</h2>
<p>It never has been as easy and difficult to be competent as in the days of AI. It can be a powerful tool, but AI does get things wrong. Therefore, I avoid <a href="https://link.springer.com/article/10.1007/s00146-025-02422-7">automation bias</a> by always validating AI's outputs. If I don‚Äôt understand the output, I don‚Äôt use it.
I‚Äôm also cautious of over-reliance. Studies suggest that heavy AI use can lead to <a href="https://www.mdpi.com/2075-4698/15/1/6">cognitive offloading</a> and worse outcomes in the long run. One study shows that users of AI <a href="https://arxiv.org/abs/2404.19699?utm_source=chatgpt.com">perform worse in exams</a>. To avoid negatively affecting my skills, I use AI as a discussion partner rather than just implementing the code it generates.</p>
<h2>Summary</h2>
<p>Onboarding is always a stressful time, but using GitLab Duo made mine smoother and less overwhelming. I learned more about GitLab‚Äôs codebase, culture, and best practices than I could have managed on my own.</p>
<blockquote>
<p>Want to make GitLab Duo part of your onboarding experience? Sign up for <a href="https://about.gitlab.com/gitlab-duo/">a free trial</a> today.</p>
</blockquote>
<h2>Resources</h2>
<ul>
<li><a href="https://docs.gitlab.com/user/get_started/getting_started_gitlab_duo/">Getting started with GitLab Duo</a></li>
<li><a href="https://about.gitlab.com/blog/get-started-with-gitlab-duo-agentic-chat-in-the-web-ui/">Get started with GitLab Duo Agentic Chat in the web UI</a></li>
<li><a href="https://about.gitlab.com/blog/10-best-practices-for-using-ai-powered-gitlab-duo-chat/">10 best practices for using AI-powered GitLab Duo Chat</a></li>
</ul>
- [What‚Äôs new in Git 2.52.0?](https://about.gitlab.com/blog/whats-new-in-git-2-52-0/) ‚Äî <p>The Git project recently released <a href="https://lore.kernel.org/git/xmqqh5usmvsd.fsf@gitster.g/">Git 2.52</a>. After a relatively short 8-week <a href="https://about.gitlab.com/blog/what-s-new-in-git-2-51-0/">release cycle for 2.51</a>, due to summer in the Northern Hemisphere, this release is back to the usual 12-week cycle. Let‚Äôs look at some notable changes, including contributions from the GitLab Git team and the wider Git community.</p>
<h2>New git-last-modified(1) command</h2>
<p>Many Git forges like GitLab display files in a tree view like this:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Last commit</th>
<th>Last update</th>
</tr>
</thead>
<tbody>
<tr>
<td>README.md</td>
<td>README: *.txt -&gt; *.adoc fixes</td>
<td>4 months ago</td>
</tr>
<tr>
<td>RelNotes</td>
<td>Start 2.51 cycle, the first batch</td>
<td>4 weeks ago</td>
</tr>
<tr>
<td>SECURITY.md</td>
<td>SECURITY: describe how to report vulnerabilities</td>
<td>4 years</td>
</tr>
<tr>
<td>abspath.c</td>
<td>abspath: move related functions to abspath</td>
<td>2 years</td>
</tr>
<tr>
<td>abspath.h</td>
<td>abspath: move related functions to abspath</td>
<td>2 years</td>
</tr>
<tr>
<td>aclocal.m4</td>
<td>configure: use AC_LANG_PROGRAM consistently</td>
<td>15 years ago</td>
</tr>
<tr>
<td>add-patch.c</td>
<td>pager: stop using <code>the_repository</code></td>
<td>7 months ago</td>
</tr>
<tr>
<td>advice.c</td>
<td>advice: allow disabling default branch name advice</td>
<td>4 months ago</td>
</tr>
<tr>
<td>advice.h</td>
<td>advice: allow disabling default branch name advice</td>
<td>4 months ago</td>
</tr>
<tr>
<td>alias.h</td>
<td>rebase -m: fix serialization of strategy options</td>
<td>2 years</td>
</tr>
<tr>
<td>alloc.h</td>
<td>git-compat-util: move alloc macros to git-compat-util.h</td>
<td>2 years ago</td>
</tr>
<tr>
<td>apply.c</td>
<td>apply: only write intents to add for new files</td>
<td>8 days ago</td>
</tr>
<tr>
<td>archive.c</td>
<td>Merge branch 'ps/parse-options-integers'</td>
<td>3 months ago</td>
</tr>
<tr>
<td>archive.h</td>
<td>archive.h: remove unnecessary include</td>
<td>1 year</td>
</tr>
<tr>
<td>attr.h</td>
<td>fuzz: port fuzz-parse-attr-line from OSS-Fuzz</td>
<td>9 months ago</td>
</tr>
<tr>
<td>banned.h</td>
<td>banned.h: mark <code>strtok()</code> and <code>strtok_r()</code> as banned</td>
<td>2 years</td>
</tr>
</tbody>
</table>
<p>&lt;br&gt;&lt;/br&gt;</p>
<p>Next to the files themselves, we also display which commit last modified each respective file. This information is easy to extract from Git by executing the following command:</p>
<pre><code class="language-shell">
$ git log --max-count=1 HEAD -- &lt;filename&gt;

</code></pre>
<p>While nice and simple, this has a significant catch: Git does not have a way to extract this information for each of these files in a single command. So to get the last commit for all the files in the tree, we'd need to run this command for each file separately. This results in a command pipeline similar to the following:</p>
<pre><code class="language-shell">
$ git ls-tree HEAD --name-only | xargs --max-args=1 git log --max-count=1 HEAD --

</code></pre>
<p>Naturally, this isn't very efficient:</p>
<ul>
<li>
<p>We need to spin up a fresh Git command for each file.</p>
</li>
<li>
<p>Git has to step through history for each file separately.</p>
</li>
</ul>
<p>As a consequence, this whole operation is quite costly and generates significant load for GitLab.</p>
<p>To overcome these issues, a new Git subcommand <code>git-last-modified(1)</code> has been introduced. This command returns the commit for each file of a given commit:</p>
<pre><code class="language-shell">
$ git last-modified HEAD


e56f6dcd7b4c90192018e848d0810f091d092913        add-patch.c
373ad8917beb99dc643b6e7f5c117a294384a57e        advice.h
e9330ae4b820147c98e723399e9438c8bee60a80        advice.c
5e2feb5ca692c5c4d39b11e1ffa056911dd7dfd3        alloc.h
954d33a9757fcfab723a824116902f1eb16e05f7        RelNotes
4ce0caa7cc27d50ee1bedf1dff03f13be4c54c1f        apply.c
5d215a7b3eb0a9a69c0cb9aa43dcae956a0aa03e        archive.c
c50fbb2dd225e7e82abba4380423ae105089f4d7        README.md
72686d4e5e9a7236b9716368d86fae5bf1ae6156        attr.h
c2c4138c07ca4d5ffc41ace0bfda0f189d3e262e        archive.h
5d1344b4973c8ea4904005f3bb51a47334ebb370        abspath.c
5d1344b4973c8ea4904005f3bb51a47334ebb370        abspath.h
60ff56f50372c1498718938ef504e744fe011ffb        banned.h
4960e5c7bdd399e791353bc6c551f09298746f61        alias.h
2e99b1e383d2da56c81d7ab7dd849e9dab5b7bf0        SECURITY.md
1e58dba142c673c59fbb9d10aeecf62217d4fc9c        aclocal.m4
</code></pre>
<p>The benefit of this is obviously that we only have to execute a single Git process now to derive all of that information. But even more importantly, it only requires us to walk the history once for all files together instead of having to walk it multiple times. This is achieved by:</p>
<ol>
<li>
<p>Start walking the history from the specified commit.</p>
</li>
<li>
<p>For each commit:</p>
<ol>
<li>If it doesn't modify any of the paths we're interested in we continue to the next commit.</li>
<li>If it does, we print the commit ID together with the path. Furthermore, we remove the path from the set of interesting paths.</li>
</ol>
</li>
<li>
<p>When the list of interesting paths becomes empty we stop.</p>
</li>
</ol>
<p>Gitaly has already been adjusted to use the new command, but the logic is still guarded by a feature flag. Preliminary testing has shown that <code>git-last-modified(1)</code> is in most situations at least twice as fast compared to using <code>git log --max-count=1</code>.</p>
<p><em>These changes were <a href="https://github.com/ttaylorr/git/tree/tb/blame-tree">originally written</a> by multiple developers from GitHub and were <a href="https://lore.kernel.org/git/20250805093358.1791633-1-toon@iotcl.com/">upstreamed</a> into Git by <a href="https://about.gitlab.com/gitlab.com/toon">Toon Claes</a>.</em></p>
<h2>git-fast-export(1) and git-fast-import(1) signature-related improvements</h2>
<p>The <code>git-fast-export(1)</code> and <code>git-fast-import(1)</code> commands are designed to be mostly used by interoperability or history rewriting tools. The goal of interoperability tools is to make Git interact nicely with other software, usually a different version control system, that stores data in a different format than Git. For example <a href="https://github.com/frej/fast-export">hg-fast-export.sh</a> is a ‚ÄúMercurial to Git converter using git-fast-import.&quot;</p>
<p>Alternately, history-rewriting tools let users ‚Äî usually admins ‚Äî make changes to the history of their repositories that are not possible or not allowed by the version control system. For example, <a href="http://www.catb.org/esr/reposurgeon/">reposurgeon</a> says in its <a href="https://gitlab.com/esr/reposurgeon/-/blob/master/repository-editing.adoc?ref_type=heads#introduction">introduction</a> that its purpose is ‚Äúto enable risky operations that version-control systems don't want to let you do, such as (a) editing past comments and metadata, (b) excising commits, (c) coalescing and splitting commits, (d) removing files and subtrees from repo history, (e) merging or grafting two or more repos, and (f) cutting a repo in two by cutting a parent-child link, preserving the branch structure of both child repos.&quot;</p>
<p>Within GitLab, we use <a href="https://github.com/newren/git-filter-repo">git-filter-repo</a> to let admins perform some risky operations on their Git repositories. Unfortunately, until Git 2.50 (released last June), both <code>git-fast-export(1)</code> and <code>git-fast-import(1)</code> didn't handle cryptographic commit signatures at all. So, although <code>git-fast-export(1)</code> had a <code>--signed-tags=&lt;mode&gt;</code> option that allows users to change how cryptographic tag signatures are handled, commit signatures were simply ignored.</p>
<p>Cryptographic signatures are very fragile because they are based on the exact commit or tag data that was signed. When the signed data or any of its preceding history changes, the cryptographic signature becomes invalid. This is a fragile but necessary requirement to make these signatures useful.</p>
<p>But in the context of rewriting history this is a problem:</p>
<ul>
<li>
<p>We may want to keep cryptographic signatures for both commits and tags that are still valid after the rewrite (e.g. because the history leading up to them did not change).</p>
</li>
<li>
<p>We may want to create new cryptographic signatures for commits and tags where the previous signature has become invalid.</p>
</li>
</ul>
<p>Neither <code>git-fast-import(1)</code> nor <code>git-fast-export(1)</code> allow for these use cases though, which limits what tools like <a href="https://github.com/newren/git-filter-repo">git-filter-repo</a> or <a href="http://www.catb.org/esr/reposurgeon/">reposurgeon</a> can achieve.</p>
<p>We have made some significant progress:</p>
<ul>
<li>
<p>In Git 2.50 we added a <code>--signed-commits=&lt;mode&gt;</code> option to <code>git-fast-export(1)</code> for exporting commit signatures, and support in <code>git-fast-import(1)</code> for importing them.</p>
</li>
<li>
<p>In Git 2.51 we improved the format used for exporting and importing commit signatures, and we made it possible for <code>git-fast-import(1)</code> to import both a signature made on the SHA-1 object ID of the commit and one made on its SHA-256 object ID.</p>
</li>
<li>
<p>In Git 2.52 we added the <code>--signed-commits=&lt;mode&gt;</code> and <code>--signed-tags=&lt;mode&gt;</code> options to <code>git-fast-import(1)</code>, so the user has control over how to handle signed data at import time.</p>
</li>
</ul>
<p>There is still more to be done. We need to add the ability to:</p>
<ul>
<li>
<p>Retain only those commit signatures that are still valid to <code>git-fast-import(1)</code>.</p>
</li>
<li>
<p>Re-sign data where the signature became invalid.</p>
</li>
</ul>
<p>We have already started to work on these next steps and expect this to land in Git 2.53. Once done, tools like <code>git-filter-repo(1)</code> will finally start to handle cryptographic signatures more gracefully. We will keep you posted in our next Git release blog post.</p>
<p><em>This project was led by <a href="https://gitlab.com/chriscool">Christian Couder</a>.</em></p>
<h2>New and improved git-maintenance(1) strategies</h2>
<p>Git repositories require regular maintenance to ensure that they perform well. This maintenance performs a bunch of different tasks: references get optimized, objects get compressed, and stale data gets pruned.</p>
<p>Until Git 2.28, these maintenance tasks were performed by <code>git-gc(1)</code>. The problem with this command is that it wasn't built with customizability in mind: While certain parameters can be configured, it is not possible to control which parts of a repository should be optimized. This means that it may not be a good fit for all use cases. Even more importantly, it made it very hard to iterate on how exactly Git performs repository maintenance.</p>
<p>To fix this issue and allow us to iterate again, <a href="https://github.com/derrickstolee">Derrick Stolee</a> introduced <code>git-maintenance(1)</code>. In contrast to <code>git-gc(1),</code> it is built with customizability in mind and allows the user to configure which tasks specifically should be running in a certain repository. This new tool was made the default for Git‚Äôs automated maintenance in Git 2.29, but, by default, it still uses <code>git-gc(1)</code> to perform the maintenance.</p>
<p>While this default maintenance strategy works well in small or even medium-sized repositories, it is problematic in the context of large monorepos. The biggest limiting factor is how <code>git-gc(1)</code> repacks objects: Whenever there are more than 50 packfiles, the tool will merge all of them together into a single packfile. This operation is quite CPU-intensive and causes a lot of I/O operations, so for large monorepos this operation can easily take many minutes or even hours to complete.</p>
<p>Git already knows how to minimize these repacks via ‚Äúgeometric repacking.‚Äù The idea is simple: The packfiles that exist in the repository must follow a geometric progression where every packfile must contain at least twice as many objects as the next smaller one. This allows Git to amortize the number of repacks required while still ensuring that there is only a relatively small number of packfiles overall. This mode was introduced by <a href="https://github.com/ttaylorr">Taylor Blau</a> in Git 2.32, but it was not wired up as part of the automated maintenance.</p>
<p>All the parts exist to make repository maintenance way more scalable for large monorepos: We have the flexible <code>git-maintenance(1)</code> tool that can be extended to have a new maintenance strategy, and we have a better way to repack objects. All that needs to be done is to combine these two.</p>
<p>And that's exactly what we did with Git 2.52! We have introduced a new ‚Äúgeometric‚Äù maintenance strategy that you can configure in your Git repositories. This strategy is intended as a full replacement for the old strategy based on <code>git-gc(1)</code>. Here is the config code you need:</p>
<pre><code class="language-shell">
$ git config set maintenance.strategy geometric

</code></pre>
<p>From hereon, Git will use geometric repacking to optimize your objects. This should lead to less churn while ensuring that your objects are in a better-optimized state, especially in large monorepos.</p>
<p>In Git 2.53, we aim to make this the default strategy. So stay tuned!</p>
<p><em>This project was led by <a href="https://gitlab.com/pks-gitlab">Patrick Steinhardt</a>.</em></p>
<h2>New subcommand for git-repo(1) to display repository metrics</h2>
<p>Performance of Git operations in a repository are often dependent on certain characteristics of its underlying structure. At GitLab, we host some extremely large repositories and having insight into the general structure of a repository is critical to understand performance. While it is possible to compose various Git commands and other tools together to surface certain repository metrics, Git lacks a means to surface info about a repository's shape/structure via a single command. This has led to the development of other external tools, such as <a href="https://github.com/github/git-sizer">git-sizer(1)</a>, to fill this gap.</p>
<p>With the release of Git 2.52, a new ‚Äústructure‚Äù subcommand has been added to git-repo(1) with the aim to surface information about a repository's structure. Currently, it displays info about the number of references and objects in the repository in the following form:</p>
<pre><code class="language-shell">
$ git repo structure


| Repository structure | Value  |
| -------------------- | ------ |
| * References         |        |
|   * Count            |   1772 |
|     * Branches       |      3 |
|     * Tags           |   1025 |
|     * Remotes        |    744 |
|     * Others         |      0 |
|                      |        |
| * Reachable objects  |        |
|   * Count            | 418958 |
|     * Commits        |  87468 |
|     * Trees          | 168866 |
|     * Blobs          | 161632 |
|     * Tags           |    992 |

</code></pre>
<p>In subsequent releases we hope to expand on this and provide other interesting data points like the largest objects in the repository.</p>
<p><em>This project was led by <a href="https://gitlab.com/justintobler">Justin Tobler</a>.</em></p>
<h2>Improvements related to the Google Summer of Code 2025</h2>
<p>We had three successful projects with the Google Summer of Code.</p>
<h3>Refactoring in order to reduce Git's global state</h3>
<p>Git contains several global variables used throughout the codebase. This increases the complexity of the code and reduces the maintainability. As part of this project, <a href="https://ayu-ch.github.io/">Ayush Chandekar</a> worked on reducing the usage of the <code>the_repository</code> global variable via a series of patches.</p>
<p><em>The project was mentored by <a href="https://gitlab.com/chriscool">Christian Couder</a> and <a href="https://in.linkedin.com/in/ghanshyam-thakkar">Ghanshyam Thakkar</a>.</em></p>
<h3>Machine-readable Repository Information Query Tool</h3>
<p>Git lacks a centralized way to retrieve repository information, requiring users to piece it together from various commands. While <code>git-rev-parse(1)</code> has become the de-facto tool for accessing much of this information, doing so falls outside its primary purpose.</p>
<p>As part of this project, <a href="https://lucasoshiro.github.io/en/">Lucas Oshiro</a> introduced a new command, <code>git-repo(1),</code> which will house all repository-level information. Users can now use <code>git repo info</code> to obtain repository information:</p>
<pre><code class="language-shell">
$ git repo info layout.bare layout.shallow object.format references.format

layout.bare=false
layout.shallow=false
object.format=sha1
references.format=reftable

</code></pre>
<p><em>The project was mentored by <a href="https://gitlab.com/pks-gitlab">Patrick Steinhardt</a> and <a href="https://gitlab.com/knayakgl">Karthik Nayak</a>.</em></p>
<h3>Consolidate ref-related functionality into git-refs</h3>
<p>Git offers multiple commands for managing references, namely <code>git-for-each-ref(1)</code>, <code>git show-ref(1)</code>, <code>git-update-ref(1)</code>, and <code>git-pack-refs(1)</code>. This makes them harder to discover and creates overlapping functionality. To address this, we introduced the <code>git-refs(1)</code> command to consolidate these operations under a single interface. As part of this this project, <a href="https://inosmeet.github.io/">Meet Soni</a> extended the command by adding the following subcommands:</p>
<ul>
<li>
<p><code>git refs optimize</code> to optimize the reference backend</p>
</li>
<li>
<p><code>git refs list</code> to list all references</p>
</li>
<li>
<p><code>git refs exists</code> to verify the existence of a reference</p>
</li>
</ul>
<p><em>The project was mentored by <a href="https://gitlab.com/pks-gitlab">Patrick Steinhardt</a> and <a href="https://luolibrary.com/">shejialuo</a>.</em></p>
<h2>What's next?</h2>
<p>Ready to experience these improvements? Update to Git 2.52.0 and start using <code>git last-modified</code>.</p>
<p>At GitLab, we will of course ensure that all of these improvements will eventually land in a GitLab instance near you!</p>
<p>Learn more in the <a href="https://lore.kernel.org/git/xmqqh5usmvsd.fsf@gitster.g/">official Git 2.52.0 release notes</a> and explore our <a href="https://about.gitlab.com/blog/tags/git/">complete archive of Git development coverage</a>.</p>
- [Claude Sonnet 3.7 deprecation notice for GitLab Duo](https://about.gitlab.com/blog/claude-sonnet-3-7-deprecation-notice-for-gitlab-duo/) ‚Äî <p>Anthropic has announced the <a href="https://docs.claude.com/en/docs/about-claude/model-deprecations">deprecation of Claude Sonnet 3.7</a>. To ensure continued service and access to the latest AI capabilities, GitLab will be removing Claude Sonnet 3.7 support from GitLab Duo features in GitLab 18.8 (which is planned for January 15, 2026).</p>
<h2>Timeline</h2>
<ul>
<li><strong>Now:</strong> Claude Sonnet 3.7 is still available in GitLab Duo</li>
<li><strong>GitLab 18.8:</strong> GitLab will remove Claude Sonnet 3.7 support from GitLab Duo features</li>
<li><strong>Recommended action:</strong> Migrate to Claude 4.0+ models immediately</li>
</ul>
<h2>Additional resources</h2>
<ul>
<li><a href="https://docs.claude.com/en/docs/about-claude/model-deprecations">Anthropic Model Deprecations</a></li>
<li><a href="https://docs.gitlab.com/user/gitlab_duo/model_selection/">GitLab Duo model selection</a></li>
</ul>
<p>If you have questions about this change or need assistance with migration, please reach out to <a href="https://support.gitlab.com/">GitLab Support</a>.</p>
- [Ace your planning without the context-switching](https://about.gitlab.com/blog/ace-your-planning-without-the-context-switching/) ‚Äî <p>Software development teams face a challenging balancing act: dozens of tasks, limited time, and constant pressure to pick the right thing to work on next.</p>
<p>The planning overhead of structuring requirements, managing backlogs, tracking delivery, and writing status updates steals hours from strategic thinking.</p>
<p>The result? Less time for the high-value decisions that actually drive products forward.</p>
<p>That‚Äôs why we developed <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/">GitLab Duo Planner</a>, an AI agent built on <a href="https://about.gitlab.com/gitlab-duo/agent-platform/">GitLab Duo Agent Platform</a> to support product managers directly within GitLab.</p>
<p>GitLab Duo Planner isn't another generic AI assistant. GitLab's product and engineering teams, who live these challenges daily like many of our customers, purpose-built GitLab Duo Planner to orchestrate planning workflows and reduce overhead while improving alignment and predictability.</p>
<h2>Your new planning teammate</h2>
<p>Today‚Äôs planning workflows face three major problems:</p>
<ol>
<li>Prone to drift -  Unplanned and orphaned work reduce trust in the plan.</li>
<li>Disruptive to developers - Constant interruptions for status updates break flow.</li>
<li>Opaque - Hidden risks surface too late to course-correct.</li>
</ol>
<p>Transforming the way teams work, GitLab Duo Planner turns manual overhead like vague ideas into structured requirements in minutes. Surface hidden backlog problems before they derail sprints. Apply RICE and MoSCoW frameworks instantly to make confident prioritization decisions. With awareness of GitLab context across the platform, every interaction with GitLab Duo Planner saves time and improves decision quality. This is possible because of the foundational agent architecture, bringing deep domain expertise and context awareness specific to GitLab.</p>
<h2>Built for teams</h2>
<p>GitLab Duo Planner leverages work items (epics, issues, tasks) and understands the nuances of work breakdown structures, dependency analysis, and effort estimation, making it well positioned to improve visibility, alignment, and confidence in delivery.</p>
<ul>
<li>
<p>Platform approach - Unlike point solutions, Duo Planner orchestrates across your entire GitLab platform, from planning through development and testing, driving visibility across teams and workflows.</p>
</li>
<li>
<p>Embedded in the flow - No more context-switching between tools or diving deep into GitLab to retrieve information. Duo Planner enables contributions, collaboration, and transparency from users across the software development lifecycle.</p>
</li>
<li>
<p>Saves time and effort - Use Duo Planner to free your teams from repetitive coordination work, improving delivery predictability, reducing missed commitments while bringing in focus on what actually moves the needle.</p>
</li>
</ul>
<h2>From chaos to clarity</h2>
<p>GitLab Duo Planner can help at different stages of software planning and delivery while operating within the planning scope, providing a safe, bounded environment with project visibility.</p>
<p>The agent can help with six flows:</p>
<ul>
<li>
<p>Prioritization - Apply frameworks like RICE, MoSCoW, or WSJF to rank work items intelligently</p>
</li>
<li>
<p>Work breakdown - Decompose initiatives into epics, features, and user stories to structure requirements</p>
</li>
<li>
<p>Dependency analysis - Identify blocked work and understand relationships between items to maintain velocity</p>
</li>
<li>
<p>Planning -  Organize sprints, milestones, or quarterly planning</p>
</li>
<li>
<p>Status reporting -  Generate summaries of project progress, risks, and blockers to track delivery</p>
</li>
<li>
<p>Backlog management -  Identify stale issues, duplicates, or items needing refinement to improve data hygiene</p>
</li>
</ul>
<p>Here is an example how GitLab Duo Planner can check the status of an initiative:</p>
<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1131065078?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;GitLab Duo Planner Agent&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>Duo Planner is available as a custom agent in the Duo Chat side panel, with the current page context.</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p><img alt="Duo Planner as a custom agent in the Duo Chat side panel" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/ener1mkyj9shg6zvtp4f.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>Let‚Äôs ask Duo Planner about the status of an initiative by providing the epic link:</p>
<p><img alt="Asking Duo Planner about the status of an initiative by providing the epic link" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/gzv2xudegtjhtesz1oaz.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>We receive a structured summary with an overview, current status of milestones, in-progress items, dependencies, and blockers, along with actionable recommendations.</p>
<p><img alt="Structured summary" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323690/guoyqe1b9bstmbjzunez.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>Next, let‚Äôs ask for an executive summary to share with stakeholders:
GitLab Duo Planner eliminates hours of manual analysis and reporting effort, helping to make decisions faster and keep all stakeholders updated.</p>
<p><img alt="Ask for executive summary" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/xs9zxawqrytfu54ejx2b.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p><img alt="Output of executive summary" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323690/bsbpvjaqnymobzg4knhu.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>Here are a few more prompts you can try with GitLab Duo Planner:</p>
<ul>
<li>‚ÄúWhich of the bugs with a ‚Äúboards‚Äù label should we fix first, considering user impact?‚Äù</li>
<li>‚ÄúRank these epics by strategic value for Q1.‚Äù</li>
<li>‚ÄúHelp me prioritize technical debt against new features.‚Äù</li>
<li>‚ÄúWhat tasks are needed to implement this user story?‚Äù</li>
<li>‚ÄúSuggest a phased approach for this project: (insert URL).‚Äù</li>
</ul>
<h2>What's next</h2>
<p>GitLab Duo Planner focuses intentionally on product managers and engineering managers working in Agile environments. Why? Because specificity drives performance. By training Duo Planner deeply on GitLab's planning workflows and Agile frameworks, we deliver reliable, actionable insights rather than generic suggestions.</p>
<p>As we evolve the platform, we envision a family of specialized agents, each optimized for specific workflows while contributing to a unified intelligence layer. Today's planner for software teams is just the beginning of how AI will transform work prioritization across all teams.</p>
<blockquote>
<p>If you‚Äôre an existing GitLab customer and would like to try GitLab Duo Planner with a prompt of your own, visit our <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/">documentation</a> where we cover prerequisites, use cases, and more.</p>
</blockquote>
- [Claude Haiku 4.5 now available in GitLab Duo Agentic Chat](https://about.gitlab.com/blog/claude-haiku-4-5-now-available-in-gitlab-duo-agentic-chat/) ‚Äî <p>GitLab now offers Claude Haiku 4.5, Anthropic's fastest model combining high intelligence with exceptional speed, directly in the GitLab Duo model selector.</p>
<p>Users have the flexibility to choose Claude Haiku 4.5 alongside other leading models, enhancing their GitLab Duo experience with near-frontier performance at remarkable speed. With strong performance on <a href="https://www.anthropic.com/news/claude-haiku-4-5">SWE-bench Verified (73.3%)</a> and more than 2x the speed of Claude Sonnet 4.5, GitLab users can apply Claude Haiku 4.5 to accelerate their development workflows with rapid, intelligent responses.</p>
<h2>GitLab Duo Agent Platform + Claude Haiku 4.5</h2>
<p><a href="https://about.gitlab.com/gitlab-duo/agent-platform/">GitLab Duo Agent Platform</a> extends the value of Claude Haiku 4.5 by enabling multi-agent orchestration, where Claude Haiku 4.5 can serve as a fast sub-agent executing parallel tasks while more powerful models handle high-level planning. This combination creates efficient agentic workflows, where speed meets intelligence across the software development lifecycle. The result is faster iterations, cost-effective AI assistance, and responsive experiences, all delivered inside the GitLab workflow developers already use every day.</p>
<h2>Where you can use Claude Haiku 4.5</h2>
<p>Claude Haiku 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Haiku 4.5 from the model selection dropdown to leverage its speed and coding capabilities for your development tasks.</p>
<p><strong>Note:</strong> Ability to select Claude Haiku 4.5 in supported IDEs will be available soon.</p>
<p>Key capabilities:</p>
<ul>
<li><strong>Superior coding performance:</strong> Achieves 73% on SWE-bench Verified, matching the intelligence level of models that were cutting-edge just months ago.</li>
<li><strong>Lightning-fast responses:</strong> More than 2x faster than Sonnet 4.5, perfect for real-time pair programming.</li>
<li><strong>Enhanced computer use:</strong> Outperforms Claude Sonnet 4 at autonomous task execution.</li>
<li><strong>Context awareness:</strong> First Haiku model with native context window tracking for better task persistence.</li>
<li><strong>Extended thinking:</strong> Pause and reason through complex problems before generating responses.</li>
</ul>
<h2>Get started today</h2>
<p>GitLab Duo Pro and Enterprise customers can access Claude Haiku 4.5 today. Visit our <a href="https://docs.gitlab.com/user/gitlab_duo/">documentation</a> to learn more about GitLab Duo capabilities and models.</p>
<p>Questions or feedback? Share your experience with us through the GitLab community.</p>
<blockquote>
<p>Want to try GitLab Ultimate with Duo Enterprise? <a href="https://about.gitlab.com/gitlab-duo/">Sign up for a free trial today.</a></p>
</blockquote>
<h2>Read more</h2>
<ul>
<li><a href="https://about.gitlab.com/blog/greater-ai-choice-in-gitlab-duo-claude-sonnet-4-5-arrives/">Greater AI choice in GitLab Duo: Claude Sonnet 4.5 arrives</a></li>
<li><a href="https://about.gitlab.com/blog/gitlab-18-4-ai-native-development-with-automation-and-insight/">GitLab 18.4: AI-native development with automation and insight</a></li>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-chat-gets-agentic-ai-makeover/">GitLab Duo Chat gets agentic AI makeover</a></li>
</ul>
- [Optimize GitLab object storage for scale and performance](https://about.gitlab.com/blog/optimize-gitlab-object-storage-for-scale-and-performance/) ‚Äî <p>Managing GitLab at scale requires strategic object storage configuration.
Here's how to configure object storage for maximum performance, security, and reliability across your GitLab components.</p>
<h2>Use consolidated form for GitLab components</h2>
<p>For artifacts, LFS, uploads, packages, and other GitLab data, eliminate credential duplication with the consolidated form:</p>
<pre><code class="language-gitlab_rails['object_store']['enabled']">gitlab_rails['object_store']['connection'] = {
  'provider' =&gt; 'AWS',
  'region' =&gt; 'us-east-1',
  'use_iam_profile' =&gt; true
}
gitlab_rails['object_store']['objects']['artifacts']['bucket'] = 'gitlab-artifacts'
gitlab_rails['object_store']['objects']['lfs']['bucket'] = 'gitlab-lfs'
# ... additional buckets for each object type
``` This reduces complexity while enabling encrypted S3 buckets and proper Content-MD5 headers.
## Configure container registry separately
The container registry requires its own configuration since it doesn't support the consolidated form:
``` registry['storage'] = {
  's3_v2' =&gt; {  # Use the new v2 driver
    'bucket' =&gt; 'gitlab-registry',
    'region' =&gt; 'us-east-1',
    # Omit access keys to use IAM roles
  }
}
</code></pre>
<p><strong>Note:</strong> The s3_v1 driver is deprecated and will be removed in GitLab 19.0. Migrate to s3_v2 for better performance and reliability.</p>
<h2>Disable proxy download for performance</h2>
<p>Set <code>proxy_download</code> to <strong>false</strong> (default) for direct downloads:</p>
<pre><code class="language-#">gitlab_rails['object_store']['proxy_download'] = false
# Or configure per bucket for granular control
gitlab_rails['object_store']['objects']['artifacts']['proxy_download'] = false
gitlab_rails['object_store']['objects']['lfs']['proxy_download'] = false
gitlab_rails['object_store']['objects']['uploads']['proxy_download'] = true  # Example: keep proxy for uploads
# Container registry defaults to redirect mode (direct downloads)
# Only disable if your environment requires it:
registry['storage']['redirect']['disable'] = false  # Keep as false
</code></pre>
<p><strong>Important:</strong> The <code>proxy_download</code> option can be configured globally at the object-store level or individually per bucket. This gives you flexibility to optimize based on your specific use case ‚Äî for example, you might want direct downloads for large artifacts and LFS files, but proxy smaller uploads through GitLab for additional security controls.
This dramatically reduces server load and egress costs by letting clients download directly from object storage.</p>
<h2>Choose identity-based authentication</h2>
<p><strong>AWS:</strong> Use IAM roles instead of access keys:</p>
<pre><code class="language-#">gitlab_rails['object_store']['connection'] = {
  'provider' =&gt; 'AWS',
  'use_iam_profile' =&gt; true
}
# Container registry
registry['storage'] = {
  's3_v2' =&gt; {
    'bucket' =&gt; 'gitlab-registry',
    'region' =&gt; 'us-east-1'
    # No access keys = IAM role authentication
  }
}
</code></pre>
<p><strong>Google Cloud Platform:</strong> Enable application default credentials:</p>
<pre><code>gitlab_rails['object_store']['connection'] = {
  'provider' =&gt; 'Google',
  'google_application_default' =&gt; true
}
</code></pre>
<p><strong>Azure:</strong> Use workload identities by omitting storage access keys.</p>
<h2>Add encryption layers</h2>
<p>Enable server-side encryption for additional security:</p>
<pre><code class="language-#">gitlab_rails['object_store']['storage_options'] = {
  'server_side_encryption' =&gt; 'AES256'
}
# Container registry
registry['storage'] = {
  's3_v2' =&gt; {
    'bucket' =&gt; 'gitlab-registry',
    'encrypt' =&gt; true
  }
}
</code></pre>
<p>For AWS KMS encryption, specify the key ARN in <code>server_side_encryption_kms_key_id</code>.</p>
<h2>Use separate buckets for organization</h2>
<p>Create dedicated buckets for each component:</p>
<ul>
<li><strong>gitlab-artifacts</strong> - CI/CD job artifacts</li>
<li><strong>gitlab-lfs</strong> - Git LFS objects</li>
<li><strong>gitlab-uploads</strong> - User uploads</li>
<li><strong>gitlab-packages</strong> - Package registry</li>
<li><strong>gitlab-registry</strong> - Container images
This isolation improves security, enables granular access controls, and simplifies cost tracking.</li>
</ul>
<h2>Key configuration differences</h2>
<p>| Component | Consolidated Form | Identity Auth | Encryption | Direct Downloads | | --- | --- | --- | --- | ---| | Artifacts, LFS, Packages | ‚úÖ Supported | ‚úÖ use_iam_profile | ‚úÖ storage_options | ‚úÖ proxy_download: false | | Container Registry | ‚ùå Separate config | ‚úÖ Omit access keys | ‚úÖ encrypt: true | ‚úÖ redirect enabled by default |</p>
<h2>Migration path</h2>
<ol>
<li><strong>Start with GitLab objects:</strong> Use the consolidated form for immediate complexity reduction.</li>
<li><strong>Configure registry separately:</strong> Use s3_v2 driver with IAM authentication.</li>
<li><strong>Enable encryption:</strong> Add server-side encryption for both components.</li>
<li><strong>Optimize performance:</strong> Ensure direct downloads are enabled with appropriate <code>proxy_download</code> settings.</li>
<li><strong>Set up lifecycle policies:</strong> Configure S3 lifecycle rules to clean up incomplete multipart uploads.</li>
</ol>
<h2>Additional resources</h2>
<p>For a complete AWS S3 configuration example, see the <a href="https://docs.gitlab.com/administration/object_storage/#aws-s3">GitLab documentation on AWS S3 object storage setup</a>.
For more details on configuring proxy_download parameters per bucket, refer to the <a href="https://docs.gitlab.com/administration/object_storage/#configure-the-parameters-of-each-object">GitLab object storage configuration documentation</a>.
<em>These configurations will scale with your growth while maintaining security and performance. The separation between GitLab object storage and container registry configurations reflects their different underlying architectures, but both benefit from the same optimization principles.</em></p>
- [How GitLab transforms embedded systems testing cycles](https://about.gitlab.com/blog/how-gitlab-transforms-embedded-systems-testing-cycles/) ‚Äî <p>Embedded developers know this cycle well: write code, wait days or weeks to test on a hardware test bench, discover bugs, fix them, then wait again. Virtual testing environments promise faster feedback, but most implementations create new problems such as environment sprawl and escalating costs.</p>
<p>GitLab's managed lifecycle environments solve these virtual testing challenges. Through virtual environment automation, GitLab accelerates embedded development cycles without the configuration complexity and cost overruns.</p>
<h2>Virtual testing challenges</h2>
<p>Virtual testing environments ‚Äî simulated hardware setups that replicate embedded system behavior and real-world conditions ‚Äî offer the potential to reduce hardware bottlenecks. Teams can test firmware on simulated processors, run model-in-the-loop (MIL) tests in MATLAB/Simulink, or verify software on virtual embedded systems without waiting for physical hardware access.</p>
<p>However, teams often implement virtual environments using one of two common approaches, both of which create unsustainable challenges.</p>
<h3>Flawed approach 1: Pipeline lifecycle environments</h3>
<p><strong>Pipeline lifecycle environments re-create the entire testing setup for every CI/CD run.</strong> When code changes trigger your CI/CD pipeline, the system provisions infrastructure, installs software simulations, and configures everything from scratch before running tests.</p>
<p>This approach works for simple scenarios but becomes inefficient as complexity rises. Consider software-in-the-loop (SIL) testing in a complex virtual environment, for example. Each pipeline run requires complete environment re-creation, including virtual processor provisioning, toolchain installations, and target configurations. <strong>These processes can eat up considerable time.</strong></p>
<p>Moreover, as embedded systems require more sophisticated virtual hardware configurations, the provisioning <strong>costs quickly add up.</strong></p>
<p>To avoid these rebuild costs and delays, many teams turn to long-lived environments that persist between test runs. But they come with downsides.</p>
<h3>Flawed approach 2: Long-lived environments</h3>
<p><strong>Long-lived environments persist indefinitely</strong> to avoid constant rebuilding. Developers request these environments from IT or DevOps teams, wait for approval, then need someone to manually provision the infrastructure. These environments are then tied to individual developers/teams rather than specific code changes, and they support ongoing development work across multiple projects.</p>
<p>While this eliminates rebuild overhead, <strong>it creates environment sprawl.</strong> Environments accumulate without a clear termination date. Infrastructure costs climb as environments consume resources indefinitely.</p>
<p>Long-lived environments also suffer from <strong>&quot;config rot&quot;</strong> ‚Äî environments retain settings, cached data, or software versions from previous tests that can affect subsequent results. A test that should fail ends up passing due to the residue of previous testing.</p>
<p>Ultimately, managing long-lived environments is a manual process that slows development velocity and increases operational overhead.</p>
<p><strong>GitLab offers a third approach</strong> through ‚Äúmanaged lifecycle environments.‚Äù This approach captures the benefits of both long-lived and pipeline lifecycle environments while avoiding the drawbacks.</p>
<h2>Solution: Managed lifecycle environments</h2>
<p>GitLab's managed lifecycle environments tie virtual testing setups to merge requests (<a href="https://docs.gitlab.com/user/project/merge_requests/">MRs</a>) rather than pipeline runs or individual developers. You can also think of them as ‚Äúmanaged MR test environments.‚Äù When you create an MR for a new feature, GitLab automatically orchestrates the provisioning of necessary virtual testing environments. These environments persist throughout the entire feature development process.</p>
<h3>Key benefits</h3>
<ul>
<li>
<p><strong>Persistent environments without rebuilding:</strong> The same virtual environment handles multiple pipeline runs as you iterate on your feature. Whether you're running MIL tests in MATLAB/Simulink or SIL tests on specialized embedded processors, the environment remains configured and ready.</p>
</li>
<li>
<p><strong>Automatic cleanup:</strong> When you merge your feature and delete the branch, GitLab automatically triggers environment cleanup, eliminating environment sprawl.</p>
</li>
<li>
<p><strong>Single source of truth:</strong> The MR records all build results, test outcomes, and environment metadata in one location. Team members can track progress and collaborate without shuffling between different tools or spreadsheets.</p>
</li>
</ul>
<p>Watch this overview video to see how managed lifecycle environments work in practice:</p>
<p>&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/9tfyVPK5DuI?si=Kj_xXNo02bnFBDhy&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;</p>
<p>GitLab automates the entire testing workflow. Each time you run firmware tests, GitLab orchestrates testing in the appropriate virtual environment, records results, and provides full visibility into every pipeline run. This approach transforms complex virtual testing from a manual, error-prone process into automated, reliable workflows.</p>
<p><strong>The result:</strong> Teams get reusable environments without runaway costs. And they increase efficiency while maintaining clean, isolated testing setups for each feature.</p>
<p>See a demonstration of managed lifecycle environments for testing firmware on virtual hardware:</p>
<p>&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/iWdY-kTlpH4?si=D6rpoulr9sv6Sl6E&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;</p>
<h2>Business impact</h2>
<p>GitLab's managed lifecycle environments deliver measurable improvements across embedded development workflows. Teams running MIL testing in MATLAB/Simulink and SIL testing on specialized processors like Infineon AURIX or BlackBerry QNX systems no longer face the tradeoff between constant environment rebuilds or uncontrolled environment sprawl. Instead, these complex virtual testing setups persist throughout feature development while automatically cleaning up when complete, enabling:</p>
<ul>
<li>Faster product development cycles</li>
<li>Shorter time-to-market</li>
<li>Lower infrastructure costs</li>
<li>Higher quality assurance</li>
</ul>
<h2>Start transforming virtual testing today</h2>
<p><a href="https://learn.gitlab.com/embedded-en/whitepaper-unlocking-agility-embedded-development"><strong>Download ‚ÄúUnlocking agility and avoiding runaway costs in embedded development‚Äù</strong></a> for a deeper exploration of managed lifecycle environments and learn how to accelerate embedded development workflows dramatically.</p>
- [Greater AI choice in GitLab Duo: Claude Sonnet 4.5 arrives](https://about.gitlab.com/blog/greater-ai-choice-in-gitlab-duo-claude-sonnet-4-5-arrives/) ‚Äî <p>GitLab now offers Claude Sonnet 4.5, Anthropic‚Äôs most advanced model for coding and real-world agents, directly in the GitLab Duo model selector.</p>
<p>Users now have the flexibility to choose Claude Sonnet 4.5 alongside other leading models, enhancing their <a href="https://about.gitlab.com/gitlab-duo/">GitLab Duo</a> experience with upgrades in tool orchestration, context editing, and domain-specific capabilities. With top performance on <a href="https://www.anthropic.com/news/claude-sonnet-4-5">SWE-bench Verified (77.2%</a>) and strengths in cybersecurity, finance, and research-heavy workflows, GitLab users can apply Claude Sonnet 4.5 to bring sharper insights and deeper context to their development work.</p>
<p>&quot;Having Claude Sonnet 4.5 in GitLab is a big win for developers. It‚Äôs a really capable coding model, and, when you use it with the GitLab Duo Agent Platform, you get smarter help right in your workflows. It‚Äôs the kind of step that makes development easier,&quot; said Taylor McCaslin, Principal, Strategy and Operations for AI Partnerships at GitLab.</p>
<h2>GitLab Duo Agent Platform + Claude Sonnet 4.5</h2>
<p><a href="https://about.gitlab.com/gitlab-duo/agent-platform/">GitLab Duo Agent Platform</a> extends the value of Claude Sonnet 4.5 by orchestrating agents, connecting them to internal systems, and integrating them throughout the software lifecycle. This combination creates a uniquely GitLab experience ‚Äî where advanced reasoning and problem-solving meet platform-wide context and security. The result is faster development, more accurate outcomes, and stronger organizational coverage, all delivered inside the GitLab workflow developers already use every day.</p>
<h2>Where you can use Claude Sonnet 4.5</h2>
<p>Claude Sonnet 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Sonnet 4.5 from the model selection dropdown to leverage its advanced coding capabilities for your development tasks.</p>
<p><img alt="Dropdown selection for Claude Sonnet 4.5 in GitLab Duo" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1759180378/sopuv0msxrmhzt2dyxdi.png" /></p>
<p><strong>Note:</strong> Ability to select Claude Sonnet 4.5 in supported IDEs will be available soon.</p>
<h2>Get started</h2>
<p>GitLab Duo Pro and Enterprise customers can access Claude Sonnet 4.5 today. Visit our <a href="https://docs.gitlab.com/user/gitlab_duo/">documentation</a> to learn more about GitLab Duo capabilities and models.</p>
<p>Questions or feedback? Share your experience with us through the GitLab community.</p>
<blockquote>
<p>Want to try GitLab Ultimate with Duo Enterprise? <a href="https://about.gitlab.com/gitlab-duo/">Sign up for a free trial today.</a></p>
</blockquote>
- [Harness FME - Fast and Furious | Blog](https://www.harness.io/blog/harness-fme-fast-and-furious) ‚Äî Read about all the updates we have made to Harness FME | Blog
- [Harness in Seattle at PASS Data Community Summit 2025 | Blog](https://www.harness.io/blog/harness-in-seattle-at-pass-data-community-summit-2025) ‚Äî Discover AI-powered schema changes, Flyway support, and modern DB governance with Harness at PASS Summit 2025. | Blog
- [Harness Database DevOps Adds Flyway Support | Blog](https://www.harness.io/blog/harness-database-devops-adds-flyway-support) ‚Äî Harness Database DevOps now supports Flyway migrations alongside Liquibase, giving teams more flexibility, automation, and governance in CI/CD. | Blog
- [The AI Visibility Problem: When Speed Outruns Security | Blog](https://www.harness.io/blog/the-ai-visibility-problem-when-speed-outruns-security) ‚Äî Harness surveyed 500 security practitioners and decision makers responsible for securing AI-native applications from the United States, UK, Germany, and France to share findings on global security practices. The State of AI-Native Application Security 2025 dives deep into AI visibility and the changing landscape of security vulnerabilities.¬† | Blog
- [Harness Commitment Orchestrator: A Modernized FinOps Experience | Blog](https://www.harness.io/blog/harness-commitment-orchestrator-a-modernized-finops-experience) ‚Äî Harness launches modernized Commitment Orchestrator with unified RI/SP visibility, AI insights, and streamlined FinOps workflows for smarter cloud cost management. | Blog
- [Harness AI October 2025 Updates: Smarter Pipelines, and more | Blog](https://www.harness.io/blog/harness-ai-october-2025-updates) ‚Äî Discover how Harness AI‚Äôs October 2025 updates solve DevOps bottlenecks with smarter pipeline understanding, instant troubleshooting, and persistent AI memory for faster, safer software delivery. | Blog
- [AI at KubeCon 2025 Atlanta: Faster, Safer Code Delivery | Blog](https://www.harness.io/blog/road-to-kubecon-2025) ‚Äî Get a first look at KubeCon 2025 Atlanta! Discover how Harness AI and Google Cloud are redefining CI/CD with practical AI and platform innovation. | Blog
- [You‚Äôre Late to the OpenTofu Party. Here‚Äôs Why That‚Äôs a Problem | Blog](https://www.harness.io/blog/late-to-opentofu) ‚Äî OpenTofu is revolutionizing Infrastructure as Code. Join us and contribute to the future of open-source automation today! | Blog
- [Harness recognized as a top partner in software delivery success | Blog](https://www.harness.io/blog/we-made-inc-s-2025-power-partners-list-again---heres-why-it-matters) ‚Äî Harness has been named to Inc.‚Äôs 2025 Power Partners List for the second year in a row ‚Äî a recognition based directly on customer testimonials. The award honors companies that deliver exceptional customer impact and partnership. Based on direct client feedback, this recognition underscores Harness‚Äôs commitment to innovation, reliability, and customer success. | Blog
- [Intent-Driven AI Testing: Redefining End-to-End Test Automation  | Blog](https://www.harness.io/blog/intent-driven-assertions-are-redefining-tests) ‚Äî Harness AI Test Automation introduces intent-driven, natural language assertions that focus on user intent instead of brittle UI selectors. By combining AI understanding with context-aware validation, it reduces test flakiness, lowers maintenance, and helps teams deliver reliable software faster. | Blog
- [Validating chaos experiments with GCP Cloud Monitoring probes | Blog](https://www.harness.io/blog/monitoring-chaos-experiments-with-gcp-cloud-monitoring-probe-in-harness) ‚Äî Learn how to use GCP Cloud Monitoring probes in Harness to automatically validate infrastructure metrics against SLOs during chaos experiments. | Blog
- [Streamline feature management with Harness MCP and Claude Code | Blog](https://www.harness.io/blog/ai-powered-feature-management-with-harness-mcp-server-and-claude-code) ‚Äî Harness FME MCP brings feature flag management to your AI coding tools like Claude Code. | Blog
- [Automate CockroachDB Schema Changes with Harness Database DevOps | Blog](https://www.harness.io/blog/cockroachdb-now-supported-in-harness-database-devops) ‚Äî Automate CockroachDB schema changes with Harness Database DevOps using Git, Liquibase, and CI/CD for secure, scalable DB delivery. | Blog
- [introducing-ai-powered-database-migration-authoring | Blog](https://www.harness.io/blog/introducing-ai-powered-database-migration-authoring) ‚Äî AI-powered database migrations let developers describe database changes in plain language‚Äîsafe, compliant, and CI/CD ready. | Blog
- [Automated Database DevOps: Harness vs Liquibase vs Flyway | Blog](https://www.harness.io/blog/harness-database-devops-vs-liquibase-vs-flyway) ‚Äî Compare Harness, Liquibase, and Flyway for Database DevOps. Learn key differences in rollback, automation, visibility, and enterprise scalability. | Blog
- [Protecting Production from a VP | Blog](https://www.harness.io/blog/protecting-production-from-a-vp) ‚Äî Prepare delivery and monitoring systems to manage applications written by citizen developers.  | Blog
- [IaC is Great, But Have You Met IaCM? | Blog](https://www.harness.io/blog/meet-iacm) ‚Äî Discover the key benefits of Infrastructure as Code Management (IaCM) and how Harness IaCM enhances infrastructure security, scalability, and efficiency. Learn why IaCM is essential for modern infrastructure management. | Blog
- [Enhance developer workflow with the new Harness Code experience | Blog](https://www.harness.io/blog/new-harness-code-experience) ‚Äî Discover the new Harness Code experience ‚Äî re-engineered for speed, scale, and developer flow. Smarter PR reviews, faster repo browsing, seamless navigation, and a unified design system already trusted by 500+ Harness engineers. | Blog
- [DevOps enhances AI's role in improving software delivery | Blog](https://www.harness.io/blog/industry-reports-agree-devops-is-the-key-to-unlocking-ais-potential) ‚Äî AI coding assistants create an "AI Velocity Paradox"‚Äîmore code, new bottlenecks. Discover what the data shows about platforms and pipelines.  | Blog
- [A 5-step enterprise guide to IaCM | Blog](https://www.harness.io/blog/5-step-enterprise-guide-to-iacm) ‚Äî Infrastructure as Code (IaC) brings consistency and speed to infrastructure provisioning, but at enterprise scale, managing that code is a whole new challenge. This blog explores why Infrastructure as Code Management (IaCM) is critical for scaling infrastructure operations across teams, clouds, and compliance boundaries, and provides a practical 5-step guide to help your organization operationalize IaCM effectively. | Blog
- [Bridging the Gap Between Finance & Engineering: The Harness Playbook | Blog](https://www.harness.io/blog/bridging-the-gap-between-finance-engineering-the-harness-playbook) ‚Äî Eliminate cloud waste by uniting finance and engineering around shared FinOps outcomes. | Blog
- [Identifying a memory leak in a high-load Go application | Blog](https://www.harness.io/blog/the-silent-leak) ‚Äî Discover how a single Go context reassignment created massive memory leaks across thousands of goroutines in Harness's delegate service. Learn the fix and avoid this common anti-pattern. | Blog
- [Harness Celebrates Hacktoberfest with LitmusChaos | Blog](https://www.harness.io/blog/harness-celebrates-hacktoberfest-with-litmuschaos) ‚Äî Join Harness & LitmusChaos this Hacktoberfest, contribute to open source and shape the Litmus 4.0 roadmap. | Blog
- [Harness strengthens application security with Qwiet AI acquisition | Blog](https://www.harness.io/blog/harness-acquires-qwiet-ai) ‚Äî Harness acquires Qwiet AI to power application security in the AI era, embedding reachability analysis to cut noise and prioritize real risks. | Blog
- [Enhance CI/CD efficiency with Amazon Kiro and Harness integration | Blog](https://www.harness.io/blog/amazon-kiro-and-harness) ‚Äî Discover how Amazon Kiro and Harness‚Äôs MCP server revolutionize software delivery by bringing intelligent CI/CD automation, troubleshooting, and DevOps tools directly into your IDE. Learn how this integration accelerates pipeline management, reduces debugging time, and empowers developers to resolve issues without leaving their coding environment. | Blog
- [Harness recognized as a leader in 2025 Gartner Magic Quadrant | Blog](https://www.harness.io/blog/harness-named-a-leader-in-the-2025-gartner-magic-quadrant-for-devops-platforms-for-the-second-consecutive-year) ‚Äî Harness is named a Leader in the 2025 Gartner¬Æ Magic Quadrant‚Ñ¢ for DevOps Platforms for the second year in a row. Read why we believe organizations choose Harness. | Blog
- [Harness AI: Transforming software delivery beyond coding | Blog](https://www.harness.io/blog/unscripted-2025-announcements) ‚Äî Discover how Harness AI moves beyond coding assistants to automate the entire software delivery lifecycle‚Äîintent-driven maintenance, self-healing CI, and AI-powered rollback‚Äîreducing toil and risk so teams can deliver faster and more reliably. | Blog
- [Automatically Test Database Undo Migrations with Harness | Blog](https://www.harness.io/blog/automatically-testing-your-undo-migrations-with-harness-database-devops) ‚Äî Learn how to automate testing of database undo migrations using Harness Database DevOps and Liquibase for safer, reliable rollbacks. | Blog
- [Understanding Comet: The automation browser's strengths and limits | Blog](https://www.harness.io/blog/reverse-engineering-comet) ‚Äî Explore how Perplexity‚Äôs Comet browser automates web interactions using only DOM and accessibility trees‚Äîno screenshots‚Äîand discover where this approach shines versus its real-world limitations, especially compared to Harness AI Test‚Äôs multi-modal automation. | Blog
- [Trunk vs Feature vs Env: Database Deployment Strategies | Blog](https://www.harness.io/blog/trunk-vs-feature-vs-environment-database-deployment) ‚Äî Explore trunk, feature, and environment-based database deployment strategies with Liquibase and Harness Database DevOps. | Blog
- [Harness AI: AI for Every Stage of the SDLC After Code Generation | Blog](https://www.harness.io/blog/announcing-harness-ai) ‚Äî Harness AI brings enterprise-grade speed, security, and stability by unifying intelligence across testing, security, deployment, and optimization, i.e., for all stages after code generation, delivering a single, adaptive platform for the future of software delivery. | Blog
- [Transform Jenkins pipelines with Harness AI DevOps solutions | Blog](https://www.harness.io/blog/modernize-to-an-ai-devops-platform-in-one-day) ‚Äî Jenkins has served you well, but it‚Äôs time for a change. With patented migration technology from Harness, you can transform your Jenkins nightmare into streamlined AI DevOps in days, not months, cutting maintenance by 80%, accelerating deployments, and bringing AI-driven automation to every release. Migrate painlessly, boost reliability, and future-proof your CI/CD. | Blog
- [Transform database DevOps with AI-driven migration solutions | Blog](https://www.harness.io/blog/ai-in-database-devops-from-manual-bottlenecks-to-autonomous-change) ‚Äî Harness AI-powered migration authoring automates database changes, enhances governance, and accelerates safe, compliant deployments. | Blog
- [Rego 101: Policy Driven DevOps | Blog](https://www.harness.io/blog/rego-101-policy-driven-devops) ‚Äî Learn how to use OPA to ensure that your DevOps team follows best practices when releasing software to production. Then, dive deep in how to write Rego policies in Harness.  | Blog
- [Mastering Windows Command Probes in Harness Chaos | Blog](https://www.harness.io/blog/mastering-windows-command-probes-in-harness-chaos) ‚Äî Learn to set up Windows chaos infrastructure and create command probes in Harness Chaos. Complete guide with step-by-step instructions and examples. | Blog
- [Confidently Ship Reusable OpenTofu and Terraform Modules | Blog](https://www.harness.io/blog/confidently-ship-reusable-opentofu-and-terraform-modules) ‚Äî Learn how to confidently ship reusable OpenTofu and Terraform modules without risking registry-wide outages. This guide covers real-world failure examples, the benefits of integration testing, and practical CI/CD workflows to ensure every shared module is secure, stable, and ready for scale. | Blog
- [Top Open Source Software Deployment Tools | Blog](https://www.harness.io/blog/top-open-source-software-deployment-tools-in-2025) ‚Äî Open source software deployment tools are compared. This guide reviews leading options like Argo CD, Flux, and Jenkins to help you automate deployments and find the best fit for your DevOps team. | Blog
- [Improving Liquibase Developer Experience with Harness Database | Blog](https://www.harness.io/blog/improving-liquibase-developer-experience-with-harness-database-devops-automated-change-generation) ‚Äî Boost your Liquibase development workflow with Harness Database DevOps. Learn how automated changeset generation streamlines schema changes, improves governance, and enhances developer experience across all environments. | Blog
- [Harness AI achieves top ranking in autonomous code fixes | Blog](https://www.harness.io/blog/harness-excels-in-swe-bench-verified) ‚Äî Discover how Harness AI‚Äôs code agent achieved the #4 spot on the SWE-Bench Verified leaderboard by solving real-world software issues autonomously. Learn about the innovative Thinking Mode using Claude 4, dynamic bug fixing, and modular tools powering fast, reliable AI-assisted software delivery. | Blog
- [Latest updates in Harness CI and Code for smarter development | Blog](https://www.harness.io/blog/raising-the-bar-harness-code-ci) ‚Äî Explore the latest updates in Harness CI and Code, including tag protection rules, Build Intelligence for Maven, Kotlin Test Intelligence, audit logs, and repo favorites to improve developer velocity and pipeline reliability. | Blog
- [State vs Script Migrations in CI/CD with Harness | Blog](https://www.harness.io/blog/state-vs-script-migrations-in-modern-database-devops) ‚Äî Learn how Harness Database DevOps blends state and script-based migrations to ensure safe, automated, and repeatable database deployments in CI/CD. | Blog
- [Your DevOps Stack is Too Complicated. Fix It. | Blog](https://www.harness.io/blog/your-devops-stack-is-too-complicated-fix-it) ‚Äî Stop letting a complex DevOps toolchain slow you down. Learn why pipelines break and how to fix them with a simpler approach using golden paths and smart automation. | Blog
- [Harness AI enhances DevOps automation for faster software delivery | Blog](https://www.harness.io/blog/introducing-harness-ai-devops-capabilities) ‚Äî Discover how Harness AI is transforming software delivery with powerful new DevOps automation capabilities. From natural language pipeline creation to AI-driven troubleshooting and policy enforcement, Harness AI empowers teams to move faster, stay compliant, and achieve seamless software delivery at scale. | Blog
- [Scaling Expertise - Instructor Led Training - 1000th Student Trained | Blog](https://www.harness.io/blog/scaling-expertise-instructor-led-training-1000th-student-trained) ‚Äî We have just crossed over our 1000th student who has been trained.  | Blog
- [Harness enhances IaC management with new reusability features | Blog](https://www.harness.io/blog/harness-iacm-1-year-reusability-scalability) ‚Äî Discover how Harness IaCM‚Äôs new Module Registry and Workspace Templates enable scalable, reusable, and secure infrastructure automation at enterprise scale | Blog
- [Stop Pipeline Sprawl: Flexible Template Governance with Harness | Blog](https://www.harness.io/blog/flexible-governance-solving-the-all-or-nothing-problem-in-pipeline-templates) ‚Äî Tired of pipeline sprawl? Learn how Harness's unique, patent-pending "Insert Blocks" allow platform teams to create flexible templates, consolidating thousands of pipelines into a manageable few while maintaining strong governance. | Blog
- [Harness patents cloud autostopping for smarter cost management | Blog](https://www.harness.io/blog/harness-receives-patent-for-cloud-autostopping-a-new-standard-for-intelligent-cost-optimization) ‚Äî Harness‚Äôs patented Cloud AutoStopping technology uses intelligent automation to eliminate idle cloud costs, helping organizations save up to 70% without manual effort or disruption. | Blog
- [AI Test Automation for faster and smarter DevOps delivery | Blog](https://www.harness.io/blog/announcing-harness-ai-test-automation) ‚Äî Harness AI Test Automation is now generally available, revolutionizing end-to-end testing with AI-native automation. Eliminate manual bottlenecks, slash test maintenance by up to 70%, and accelerate software delivery with intent-based, self-healing tests that integrate natively into CI/CD. See how Harness empowers teams to build, test, and deploy with unmatched speed, quality, and security. | Blog
- [Engineer-led Experimentation to Optimize Infrastructure | Blog](https://www.harness.io/blog/engineer-led-experimentation-to-optimize-infrastructure) ‚Äî Discover how engineers can utilize feature flags to run experiments, validate infrastructure choices, and build performant applications. | Blog
- [Database DevOps: Lessons Learned from Manual Migration Hell | Blog](https://www.harness.io/blog/database-devops-lessons-learned-from-manual-migration-hell) ‚Äî Discover how Harness DB DevOps enhances Liquibase workflows by automating SQL migrations for faster, safer, and more reliable database deployments. | Blog
- [Getting Continuous Deployment Right: A Practical Guide | Blog](https://www.harness.io/blog/getting-continuous-deployment-right-a-practical-guide) ‚Äî Discover top continuous deployment tools to enhance your DevOps workflow and boost application delivery speed.
 | Blog
- [Harness named a leader in Forrester Wave for DevOps platforms | Blog](https://www.harness.io/blog/forrester-wave-2025) ‚Äî Harness is named a Leader in The Forrester Wave‚Ñ¢: DevOps Platforms, Q2 2025 - recognizing our AI-driven vision to reduce engineering toil in the entire SLDC. | Blog
- [Improve AWS deployments with Blue-Green Traffic Shifting | Blog](https://www.harness.io/blog/groundbreaking-release-harness-blue-green-traffic-shifting-for-aws-ecs-asg-spot-elastigroup) ‚Äî Harness Blue-Green Traffic Shifting introduces a progressive rollout mechanism that incrementally directs traffic between your blue and green environments, minimizing risk and enabling real-time validation. | Blog
- [The Harness MCP Server | Blog](https://www.harness.io/blog/mcp-announcement) ‚Äî Harness MCP Server: Bridging AI systems and DevOps via secure and efficient communication. | Blog
- [Fidelity's OpenTofu Migration: A DevOps Success Story Worth Studying | Blog](https://www.harness.io/blog/fidelitys-opentofu-migration-a-devops-success-story-worth-studying) ‚Äî Enterprise Terraform to OpenTofu migration can be simple. Follow Fidelity's proven 6-step process for transitioning 50,000+ state files with minimal risk. Get the migration blueprint other enterprises are using successfully.
 | Blog
- [How engineering teams leverage insights for business success | Blog](https://www.harness.io/blog/how-top-engineering-teams-use-software-engineering-insights-to-drive-measurable-business-impact) ‚Äî Opinionated, role-based engineering insights connect metrics to business value, optimize workflows, and drive measurable impact with Harness SEI. | Blog
- [Where Developers Spend Time (vs. Where They Should Be) | Blog](https://www.harness.io/blog/where-developers-spend-time-vs-where-they-should-be) ‚Äî A review of Microsoft‚Äôs recent study, ‚ÄúTime Warp: The Gap Between Developers‚Äô Ideal vs Actual Workweeks in an AI-Driven Era‚Äù | Blog
- [Harness Developer Hub: Celebrating 10,000 contributions to docs | Blog](https://www.harness.io/blog/10000-pr-hdh) ‚Äî The Harness Developer Hub (HDH) has reached a significant milestone of 10,000 pull requests, underscoring the value of documentation as a living, evolving product. By adopting a developer-first, docs-as-code approach, HDH has transformed documentation into a transparent, collaborative asset that evolves alongside the product itself. This model not only fosters continuous improvement but also empowers contributions from across the organization, reinforcing the role of documentation as a critical component of the overall product experience. | Blog
- [Automate verification queries for different database environments | Blog](https://www.harness.io/blog/automating-environment-specific-verification-queries-with-liquibase-and-harness-database-devops) ‚Äî Automate environment-aware database verifications using Liquibase and Harness to ensure safe, consistent changes across all environments. | Blog
- [Empowering product teams through effective experimentation practices | Blog](https://www.harness.io/blog/bridging-the-gap-empowering-product-teams-with-seamless-experimentation) ‚Äî Collaboration between Product Managers and Engineers on experimentation yields improved results. | Blog
- [Celebrating 3000 certifications at Harness University | Blog](https://www.harness.io/blog/harness-university-growth-3000th-certification-issued) ‚Äî Harness issued our 3000th certification in the Harness Certified Expert Certifications.  | Blog
- [Elite Engineering Teams Don‚Äôt Guess‚ÄîThey Prove Impact | Blog](https://www.harness.io/blog/elite-engineering-teams-dont-guess-they-prove-impact) ‚Äî Harness SEI gives engineering leaders actionable insights to prove impact, boost efficiency, and protect creativity by eliminating guesswork. | Blog
- [Top Challenges in Database DevOps | Blog](https://www.harness.io/blog/top-challenges-in-database-devops) ‚Äî Modernize database deployments with CI/CD, governance, and performance insights‚Äîeliminate risk and deploy database changes with confidence. | Blog
- [Early 2025 Harness CD Enhancements | Blog](https://www.harness.io/blog/early-2025-harness-cd-enhancements) ‚Äî Harness is continuously innovating to make deployments faster, easier, and more reliable with powerful new features and enhancements. | Blog
- [Communicating engineering metrics for business leaders | Blog](https://www.harness.io/blog/the-executive-playbook-communicating-engineering-metrics-for-maximum-business-impact) ‚Äî Engineering leaders need to communicate metrics that drive business outcomes. This playbook explores how CTOs, VPs of Engineering, and technical leaders can leverage Harness SEI to translate engineering performance into executive-aligned insights, improving efficiency, productivity, and strategic influence. | Blog
- [Introducing AWS CloudFormation Stack Refactoring Console Experience: Reorganize Your Infrastructure Without Disruption](https://aws.amazon.com/blogs/devops/introducing-aws-cloudformation-stack-refactoring-reorganize-your-infrastructure-without-disruption/) ‚Äî AWS CloudFormation models and provisions cloud infrastructure as code, letting you manage entire lifecycle operations through declarative templates. Stack Refactoring console experience, announced today, extends the AWS CLI experience launched earlier. Now, you move resources between stacks, rename logical IDs, and decompose monolithic templates into focused components without touching the underlying infrastructure using the CloudFormation [‚Ä¶]
- [Take fine-grained control of your AWS CloudFormation StackSets Deployment with StackSet Dependencies](https://aws.amazon.com/blogs/devops/take-fine-grained-control-of-your-aws-cloudformation-stacksets-deployment-with-stackset-dependencies/) ‚Äî Introduction AWS CloudFormation StackSets enable you to deploy CloudFormation stacks across multiple AWS accounts and regions with a single operation, providing centralized management of infrastructure at scale through AWS Organizations integration. In enterprise environments, multiple StackSet often need to deploy in a specific order. For example, networking infrastructure must be ready before applications can deploy [‚Ä¶]
- [Announcing CloudFormation IDE Experience: End-to-End Development in Your IDE](https://aws.amazon.com/blogs/devops/announcing-cloudformation-ide-experience-end-to-end-development-in-your-ide/) ‚Äî If you‚Äôve developed AWS CloudFormation templates, you know the drill; write YAML (YAML Ain‚Äôt Markup Language) in your IDE (Integrated Development Environment), switch to the AWS Management Console to validate, jump to documentation to verify property names. Then run CFN Lint (CloudFormation Linter) in your terminal, deploy and wait, then troubleshoot failures back in the [‚Ä¶]
- [Amazon introduces two benchmark datasets for evaluating AI agents‚Äô ability on code migration](https://aws.amazon.com/blogs/devops/amazon-introduces-two-benchmark-datasets-for-evaluating-ai-agents-ability-on-code-migration/) ‚Äî Introduction: Repository-Level Code Migration Code migration is a repository-level transformation process that modernizes entire software projects to run on new platforms, frameworks, or runtime environments while preserving their original functionality and structure. Rather than focusing on isolated files or APIs, it operates across the full repository, spanning source code, dependencies, build systems, and configuration files [‚Ä¶]
- [Safely Handle Configuration Drift with CloudFormation Drift-Aware Change Sets](https://aws.amazon.com/blogs/devops/safely-handle-configuration-drift-with-cloudformation-drift-aware-change-sets/) ‚Äî Introduction Is configuration drift preventing you from accessing the speed, safety, and governance benefits of AWS CloudFormation for infrastructure management? Configuration drift occurs when cloud resources are modified outside of CloudFormation, leading to a mismatch in the actual state and template definition of resources. Drift tends to accumulate from infrastructure changes that engineers make via [‚Ä¶]
- [Accelerate infrastructure development with CloudFormation pre-deployment validation and simplified troubleshooting](https://aws.amazon.com/blogs/devops/accelerate-infrastructure-development-with-cloudformation-pre-deployment-validation-and-simplified-troubleshooting/) ‚Äî AWS CloudFormation makes it easy to model and provision your cloud application infrastructure as code. CloudFormation templates can be written directly in JSON or YAML, or they can be generated by tools like the AWS Cloud Development Kit (CDK). Resources are created and managed by CloudFormation as units called Stacks. Additionally, change set enable you [‚Ä¶]
- [Streamlining Multi-Account Infrastructure with AWS CloudFormation StackSets and AWS CDK](https://aws.amazon.com/blogs/devops/streamlining-multi-account-infrastructure-with-aws-cloudformation-stacksets-and-aws-cdk/) ‚Äî Introduction Organizations operating at scale on AWS often need to manage resources across multiple accounts and regions. Whether it‚Äôs deploying security controls, compliance configurations, or shared services, maintaining consistency can be challenging. AWS CloudFormation StackSets (StackSets) has been helping organizations deploy resources across multiple accounts and regions since its launch. While the service is powerful [‚Ä¶]
- [How to Simplify Multi-Account Deployments Monitoring: Centralized Logs for AWS CloudFormation StackSets](https://aws.amazon.com/blogs/devops/how-to-simplify-multi-account-deployments-monitoring-centralized-logs-for-aws-cloudformation-stacksets/) ‚Äî Introduction As organizations adopt multi-account strategies for&nbsp;improved security features and governance, AWS CloudFormation StackSets enables organizations to deploy infrastructure across multiple accounts and regions. However, monitoring and tracking these distributed deployments across multiple accounts&nbsp;presents operational challenges. When a critical security baseline deployed across 50 accounts suddenly starts failing, teams face the daunting task of logging [‚Ä¶]
- [Infrastructure as Code at Thomson Reuters with AWS CDK](https://aws.amazon.com/blogs/devops/infrastructure-as-code-at-thomson-reuters-with-aws-cdk/) ‚Äî This post is cowritten by Danilo Tommasina and Lalit Kumar B from Thomson Reuters. Large organizations often struggle with infrastructure management challenges including compliance issues, development bottlenecks and errors from inconsistent AWS resource creation across teams. Without standardized naming, tagging and policy enforcement, teams face repeated boilerplate code and difficulty accessing centrally-managed resources. In this [‚Ä¶]
- [Boosting Unit Test Automation at Audible with Amazon Q Developer](https://aws.amazon.com/blogs/devops/boosting-unit-test-automation-at-audible-with-amazon-q-developer/) ‚Äî Audible, an Amazon company, is a leading producer and provider of audio storytelling. With a vast library of over 1,000,000 titles including audiobooks, podcasts, and Audible Originals with specific curated offerings available in each marketplace, Audible makes it easy to transform everyday moments into extraordinary opportunities for learning, imagination, and entertainment through immersive audio experiences. [‚Ä¶]
- [StackSets Deployment Strategies: Balancing Speed, Safety, and Scale to Optimize Deployments for Different Organizational Needs](https://aws.amazon.com/blogs/devops/stacksets-deployment-strategies-balancing-speed-safety-and-scale-to-optimize-deployments-for-different-organizational-needs/) ‚Äî AWS CloudFormation StackSets enables organizations to deploy infrastructure consistently across multiple AWS accounts and regions. However, success depends on choosing the right deployment strategy that balances three critical factors: deployment speed, operational safety, and organizational scale. This guide explores proven StackSets deployment strategies specifically designed for multi-account infrastructure management. Understanding StackSets Deployment Fundamentals What are [‚Ä¶]
- [Moeve: Controlling resource deployment at scale with AWS CloudFormation Guard Hooks](https://aws.amazon.com/blogs/devops/moeve-controlling-resource-deployment-at-scale-with-aws-cloudformation-guard-hooks/) ‚Äî This post is co-written with Rayco Mart√≠nez Hern√°ndez, Head of Cloud Governance at Moeve. Moeve, formerly known as Cepsa, is a global integrated energy company with over 90 years of experience and more than 11,000 employees. Moeve is committed to driving Europe‚Äôs energy transition and accelerating decarbonization efforts. The company has embraced digital transformation to [‚Ä¶]
- [Beyond Bootstrap: Bootstrapless CDK Deployments at GoDaddy](https://aws.amazon.com/blogs/devops/beyond-bootstrap-bootstrapless-cdk-deployments-at-godaddy/) ‚Äî This is a guest post written by Ramanathan Nachiappan from GoDaddy. In the world of infrastructure as code, the AWS Cloud Development Kit (AWS CDK) has revolutionized how teams define and provision cloud resources. Central to its operation is the bootstrapping process, which ensures all required resources and permissions are in place to enable secure [‚Ä¶]
- [Reduce Docker image build time on AWS CodeBuild using Amazon ECR as a remote cache](https://aws.amazon.com/blogs/devops/reduce-docker-image-build-time-on-aws-codebuild-using-amazon-ecr-as-a-remote-cache/) ‚Äî In modern software development, containerization with Docker has revolutionized how we build and deploy applications. While Docker enables packaging applications into portable containers, the continuous need to update these images can be resource intensive. AWS CodeBuild addresses this challenge by providing a managed build service that eliminates infrastructure maintenance overhead. In this blog post, we‚Äôll [‚Ä¶]
- [Multi-Cloud Code Deployments using Amazon Q Developer with Echo3D](https://aws.amazon.com/blogs/devops/multi-cloud-code-deployments-using-amazon-q-developer-with-echo3d/) ‚Äî Overview Founded in 2018, echo3D built a revolutionary 3D digital asset management (DAM) platform to address the surging demand for immersive content across industries. The company‚Äôs platform enables enterprises to seamlessly store, secure, optimize, and share 3D content, serving over 200,000 professionals across energy, healthcare, gaming, retail, and beyond. echo3D‚Äôs platform has become the go-to [‚Ä¶]
- [Accelerating AWS Infrastructure Deployment: A Practical Guide to Console-to-Code](https://aws.amazon.com/blogs/devops/accelerating-aws-infrastructure-deployment-a-practical-guide-to-console-to-code/) ‚Äî In today‚Äôs cloud-first environment, Infrastructure as Code (IaC) has become crucial for managing cloud resources effectively. However, organizations often face significant challenges in adopting IaC practices, including steep learning curves, complex syntax requirements, and difficulty translating manual operations into code.&nbsp;&nbsp;Amazon Q Developer‚Äòs Console-to-Code feature addresses these challenges by providing an intuitive bridge between manual AWS [‚Ä¶]
- [Multi Agent Collaboration with Strands](https://aws.amazon.com/blogs/devops/multi-agent-collaboration-with-strands/) ‚Äî In the evolving landscape of autonomous systems, multi-agent collaboration is becoming not only feasible but necessary. As agents gain more capabilities, like advanced reasoning, adaptation, and tool use, the challenge shifts from individual performance to effective coordination. The question is no longer ‚Äúcan an agent solve a task?‚Äù but ‚Äúhow do we organize execution across [‚Ä¶]
- [AWS named as a Leader in the 2025 Gartner Magic Quadrant for AI Code Assistants](https://aws.amazon.com/blogs/devops/aws-named-as-a-leader-in-the-2025-gartner-magic-quadrant-for-ai-code-assistants/) ‚Äî We are excited to share that AWS has been named a Leader in the 2025 Gartner Magic Quadrant for AI Code Assistants for the second year in row. This recognition highlights for us Amazon Q Developer‚Äôs commitment to innovation and delivering exceptional customer experiences. We believe this Leader placement showcases our rapid pace of innovation, [‚Ä¶]
- [Introducing universal installers for AWS CLI v2 on macOS](https://aws.amazon.com/blogs/devops/introducing-universal-installers-for-aws-cli-v2-on-macos/) ‚Äî Amazon Web Services (AWS) is announcing the availability of universal macOS installers for the AWS Command Line Interface (AWS CLI) v2. What‚Äôs new Starting with AWS CLI v2 version 2.30.0, the AWS CLI installers will provide universal binary support for macOS that works natively on both Apple silicon and Intel processors with a single download. [‚Ä¶]
- [AWS Cloud Development Kit (CDK) Launches Refactor](https://aws.amazon.com/blogs/devops/aws-cloud-development-kit-cdk-launches-refactor/) ‚Äî We are excited to announce a new AWS Cloud Development Kit (CDK) feature that makes it easier and safer to refactor your infrastructure as code. CDK Refactor aims to preserve your AWS resources as you rename constructs, move resources between stacks, and reorganize your CDK applications ‚Äì operations that previously risked resource replacement. When writing [‚Ä¶]
- [November Patches for Azure DevOps Server](https://devblogs.microsoft.com/devops/november-patches-for-azure-devops-server-2/) ‚Äî <p>Today we are releasing patches that impact our self-hosted product, Azure DevOps Server. We strongly encourage and recommend that all customers use the latest, most secure release of Azure DevOps Server. You can download the latest version of the product, Azure DevOps Server 2022.2 from the Azure DevOps Server download page. Azure DevOps Server 2022.2 [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/november-patches-for-azure-devops-server-2/">November Patches for Azure DevOps Server</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Azure Developer CLI: Azure Container Apps Dev-to-Prod Deployment with Layered Infrastructure](https://devblogs.microsoft.com/devops/azure-developer-cli-azure-container-apps-dev-to-prod-deployment-with-layered-infrastructure/) ‚Äî <p>This post walks through how to implement &#8220;build once, deploy everywhere&#8221; patterns using Azure Container Apps with the new azd publish and layered infrastructure features in Azure Developer CLI v1.20.0. You&#8217;ll learn how to deploy the same containerized application across multiple environments with proper separation of concerns. This is the third installment in our Azure [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/azure-developer-cli-azure-container-apps-dev-to-prod-deployment-with-layered-infrastructure/">Azure Developer CLI: Azure Container Apps Dev-to-Prod Deployment with Layered Infrastructure</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Upcoming Updates for Azure Pipelines Agents Images](https://devblogs.microsoft.com/devops/upcoming-updates-for-azure-pipelines-agents-images/) ‚Äî <p>To ensure our hosted agents in Azure Pipelines are operating in the most secure and up-to-date environments, we continuously update the supported images and phase out older ones. In October 2024, we announced support for Ubuntu-24.04. Soon, we plan to update the ubuntu-latest image to map to Ubuntu-24.04. Additionally, MacOS 15 Sequoia and Windows 2025 [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/upcoming-updates-for-azure-pipelines-agents-images/">Upcoming Updates for Azure Pipelines Agents Images</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Modernizing Authentication for Legacy Visual Studio Clients](https://devblogs.microsoft.com/devops/modernizing-authentication-for-legacy-visual-studio-clients/) ‚Äî <p>As part of our ongoing commitment to security and modernization, we‚Äôre updating outdated authentication mechanisms used by older versions of clients reliant on our older Visual Studio client libraries. For full details on all known impacted clients, refer to the official announcement we made in April 2024: End of Support for Microsoft products reliant on [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/modernizing-authentication-for-legacy-visual-studio-clients/">Modernizing Authentication for Legacy Visual Studio Clients</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Azure DevOps local MCP Server is generally available](https://devblogs.microsoft.com/devops/azure-devops-local-mcp-server-generally-available/) ‚Äî <p>Today we are excited to take our local MCP Server for Azure DevOps out of preview ü•≥. Since the initial preview announcement, we&#8217;ve worked closely with early adopters and the community to incorporate feature suggestions and feedback. We‚Äôve improved login and authorization, added and refined tooling, and introduced domains so users can scope active tools [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/azure-devops-local-mcp-server-generally-available/">Azure DevOps local MCP Server is generally available</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Announcing the new Azure DevOps Server RC Release](https://devblogs.microsoft.com/devops/announcing-the-new-azure-devops-server-rc-release/) ‚Äî <p>We‚Äôre excited to announce the release candidate (RC) of Azure DevOps Server, bringing new features previously available in our hosted version. You can‚ÄØ download Azure DevOps Server RC today. A direct upgrade to Azure DevOps Server RC is supported from any version of Team Foundation Server, including Team Foundation Server 2015 and newer. Note: October [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/announcing-the-new-azure-devops-server-rc-release/">Announcing the new Azure DevOps Server RC Release</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [New Test Run Hub in Azure Test Plans](https://devblogs.microsoft.com/devops/new-test-run-hub/) ‚Äî <p>Delivering high-quality software is a necessity and that‚Äôs why Azure Test Plans has introduced the all-new Test Run Hub, an enabler for teams who want to take control of their testing process and drive continuous improvement. What Makes the Test Run Hub a Must-Have? The Test Run Hub is designed to help teams track test [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/new-test-run-hub/">New Test Run Hub in Azure Test Plans</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Azure Developer CLI: From Dev to Prod with Azure DevOps Pipelines](https://devblogs.microsoft.com/devops/azure-developer-cli-from-dev-to-prod-with-azure-devops-pipelines/) ‚Äî <p>Building on our previous post about implementing dev-to-prod promotion with GitHub Actions, this follow-up demonstrates the same &#8220;build once, deploy everywhere&#8221; pattern using Azure DevOps Pipelines. You&#8217;ll learn how to leverage Azure DevOps YAML pipelines with Azure Developer CLI (azd). This approach ensures consistent, reliable deployments across environments. Environment-Specific Infrastructure The infrastructure approach is identical [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/azure-developer-cli-from-dev-to-prod-with-azure-devops-pipelines/">Azure Developer CLI: From Dev to Prod with Azure DevOps Pipelines</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>

---

## üí¨ Social Buzz
- [CD-i](https://en.wikipedia.org/wiki/CD-i) ‚Äî <p>Article URL: <a href="https://en.wikipedia.org/wiki/CD-i">https://en.wikipedia.org/wiki/CD-i</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=46021572">https://news.ycombinator.com/item?id=46021572</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [Show HN: Quick install script for self-hosted Forgejo (Git+CI) server](https://wkoszek.github.io/easyforgejo/) ‚Äî <p>I wanted to get Forgejo on my VM on a local NAS fast, and I realized that getting everything running with Git and CI working was ... harder than I anticipated. After spending more time than I wanted, and bugging lovely people at Forgejo's matrix, I came up with this:<p><a href="https://wkoszek.github.io/easyforgejo/" rel="nofollow">https://wkoszek.github.io/easyforgejo/</a><p>With this script, you should get Forgejo installed on your Linux computer in 2min. I tested this on a VM for now, and it works well enough for beta launch.<p>Repo is here:<p><a href="https://github.com/wkoszek/easyforgejo" rel="nofollow">https://github.com/wkoszek/easyforgejo</a><p>Let me know what you think and submit PRs if you find bugs. I'd not use it in production just yet.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45994118">https://news.ycombinator.com/item?id=45994118</a></p>
<p>Points: 4</p>
<p># Comments: 2</p>
- [Show HN: Haloy ‚Äì an open‚Äësource, lightweight deployment system for Docker apps](https://github.com/haloydev/haloy) ‚Äî <p>Haloy is a lightweight way to deploy Docker apps to your own servers without complex setups. One config file, one deploy command. Handles routing, HTTPS, rollbacks, and multi‚Äëserver environments.<p>Repo: <a href="https://github.com/haloydev/haloy" rel="nofollow">https://github.com/haloydev/haloy</a>
Docs: <a href="https://haloy.dev" rel="nofollow">https://haloy.dev</a></p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45992412">https://news.ycombinator.com/item?id=45992412</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [Emulator Bugs: Sega CD](https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/) ‚Äî <p>Article URL: <a href="https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/">https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45992158">https://news.ycombinator.com/item?id=45992158</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [Emulator Bugs: Sega CD](https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/) ‚Äî <p>Article URL: <a href="https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/">https://jsgroth.dev/blog/posts/emulator-bugs-sega-cd/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45981927">https://news.ycombinator.com/item?id=45981927</a></p>
<p>Points: 5</p>
<p># Comments: 0</p>
- [Show HN: MemBrowse - CI/CD memory footprint tracking for embedded firmware](https://membrowse.com) ‚Äî <p>Built this after working with Intel‚Äôs Bluetooth firmware team - we kept seeing builds suddenly fail because memory crept up a few bytes at a time until it overflowed. Then everyone lost hours/days bisecting symbols and linker maps to figure out which commit pushed it over.<p>So I made a CI tool that tracks firmware memory footprint across commits and flags bloat before it breaks the build.<p>How it works:<p>CLI parses ELF + DWARF, extracts per-section/per-symbol/per-file size info<p>CI uploads reports; the platform stores history and generates diffs<p>Shows exactly what changed between commits (what grew, where, and by how much)<p>Optional memory budgets via commit keywords (acts as a CI gate to block regressions)<p>The report generator is open source: <a href="https://github.com/membrowse/membrowse-action" rel="nofollow">https://github.com/membrowse/membrowse-action</a><p>Works with GitHub Actions or any CI, tested on ARM, ESP32, ARC, x86 (multiple toolchains).<p>There‚Äôs also a live demo analyzing MicroPython firmware builds at membrowse.com.<p>Curious to hear from embedded folks:
* How do you track code size / memory usage today?<p>* What would stop you from adding this type of check to CI?<p>* What‚Äôs missing that would make this genuinely useful for your workflow?</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45979698">https://news.ycombinator.com/item?id=45979698</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Runme: DevOps Notebooks Built with Markdown](https://runme.dev) ‚Äî <p>Article URL: <a href="https://runme.dev">https://runme.dev</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45969961">https://news.ycombinator.com/item?id=45969961</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Caching Playwright on CI [video]](https://www.youtube.com/watch?v=_BcCHW6OgQ4) ‚Äî <p>Article URL: <a href="https://www.youtube.com/watch?v=_BcCHW6OgQ4">https://www.youtube.com/watch?v=_BcCHW6OgQ4</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45959431">https://news.ycombinator.com/item?id=45959431</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [The Deployment Age](https://reactionwheel.net/2015/10/the-deployment-age.html) ‚Äî <p>Article URL: <a href="https://reactionwheel.net/2015/10/the-deployment-age.html">https://reactionwheel.net/2015/10/the-deployment-age.html</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45949585">https://news.ycombinator.com/item?id=45949585</a></p>
<p>Points: 3</p>
<p># Comments: 0</p>
- [ArkA ‚Äì an open video protocol with full CI/CD](https://github.com/baconpantsuppercut/arkA) ‚Äî <p>Article URL: <a href="https://github.com/baconpantsuppercut/arkA">https://github.com/baconpantsuppercut/arkA</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45947981">https://news.ycombinator.com/item?id=45947981</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Production-Grade Container Deployment with Podman Quadlets ‚Äì Larvitz Blog](https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html) ‚Äî <p>Article URL: <a href="https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html">https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45945200">https://news.ycombinator.com/item?id=45945200</a></p>
<p>Points: 62</p>
<p># Comments: 20</p>
- [Ask HN: Where to Migrate as an IT Support/DevOps Engineer for Work?](https://news.ycombinator.com/item?id=45942862) ‚Äî <p>Whatever I do and learn no progress is going to happen in my country(nepal).<p>There is no benefit of having merit in Nepal. Private jobs are already low paying. Remote jobs do not come generally to Nepal.I see remote jobs love India and south east asia. I do not get the point of remote job if they are hiring from specific country.
The only thing I can do in nepal is public service commission (civil services) and crack computer engineer. But the pay is meagre there as well, unless I am lucky enough to enter central bank of Nepal(NRB).<p>Honestly, it feels like I am pushing not just a wall but universe itself in Nepal. Because nothing is going to happen irrespective of my abilities. I am currently preparing for PSC and I do not believe I will be happy as a PSC engineer even if I end up at NRB(central bank).<p>Something feels missing inside me. I have took countless therapies and what not. They helped me a lot to be where I am at now. I feel scared to try opportunities out of my comfort zone (kathmandu is my comfort zone).<p>As an adult, nobody pushes you, you have to push yourself. I am in a serious deadlock internally. I can decrease the effect using yoga and meditation but that does not troubleshoot the cause. Personally, I want to pursue something academic away from nepal. I believe that would provide me the much needed confidence in my life.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45942862">https://news.ycombinator.com/item?id=45942862</a></p>
<p>Points: 3</p>
<p># Comments: 4</p>
- [Docker Compose Continuous Deployment](https://github.com/kimdre/doco-cd) ‚Äî <p>Article URL: <a href="https://github.com/kimdre/doco-cd">https://github.com/kimdre/doco-cd</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45937846">https://news.ycombinator.com/item?id=45937846</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [The short, happy reign of CD-ROM (2024)](https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst) ‚Äî <p>Article URL: <a href="https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst">https://www.fastcompany.com/91128052/history-of-cd-roms-encarta-myst</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45933285">https://news.ycombinator.com/item?id=45933285</a></p>
<p>Points: 3</p>
<p># Comments: 1</p>
- [Three things I've learned about Git while building a CI/CD tool](https://www.ocuroot.com/blog/things-i-learned-about-git/) ‚Äî <p>Article URL: <a href="https://www.ocuroot.com/blog/things-i-learned-about-git/">https://www.ocuroot.com/blog/things-i-learned-about-git/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45927686">https://news.ycombinator.com/item?id=45927686</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [What to do with 5900 blank CD-Rs?](https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/) ‚Äî <p>Article URL: <a href="https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/">https://old.reddit.com/r/DataHoarder/comments/1ovelti/what_to_do_with_5900_blank_cdrs/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45926312">https://news.ycombinator.com/item?id=45926312</a></p>
<p>Points: 4</p>
<p># Comments: 1</p>
- [Docker Compose Continuous Deployment](https://github.com/kimdre/doco-cd) ‚Äî <p>Article URL: <a href="https://github.com/kimdre/doco-cd">https://github.com/kimdre/doco-cd</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45895328">https://news.ycombinator.com/item?id=45895328</a></p>
<p>Points: 2</p>
<p># Comments: 0</p>
- [Cut cloud costs with the CI Arcade (satire. enjoy. smile.)](https://blog.misfit.dev/the-ci-arcade/) ‚Äî <p>Article URL: <a href="https://blog.misfit.dev/the-ci-arcade/">https://blog.misfit.dev/the-ci-arcade/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45891421">https://news.ycombinator.com/item?id=45891421</a></p>
<p>Points: 1</p>
<p># Comments: 0</p>
- [Jenkins won't start after upgrade, complains about plugin versions ...](https://community.jenkins.io/t/jenkins-wont-start-after-upgrade-complains-about-plugin-versions-even-though-i-updated-all-plugins-before-upgrading/30790) ‚Äî I've updated every plugin using the plugins manager in the webUI but after upgrading jenkins it fails to start and complains about a list of plugins that need ...
- [What are some common issues you've faced when upgrading ...](https://www.reddit.com/r/jenkinsci/comments/1ftlaiz/what_are_some_common_issues_youve_faced_when/) ‚Äî Hey everyone, I'm planning to upgrade Jenkins and several of its plugins soon, but I've heard there can be issues during the process.
- [Operations center fails to load plugins or start after upgrade with ...](https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/operations-center/pipeline-plugins-installed-on-operations-center) ‚Äî After upgrading your Operations center, it fails to load plugins, or fails to startup, and you see an error in the logs mentioning Failed Loading plugin.
- [Issues upgrading jenkins - Ask a question](https://community.jenkins.io/t/issues-upgrading-jenkins/17772) ‚Äî I am trying to upgrade jenkins from 2.423 to the latest version and I am running into issues. I stop jenkins, I backup the jenkins war file, and I download the ...
- [A plugin release is not available from Update Center](https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/troubleshooting-guides/a-plugin-release-is-not-available-from-update-center) ‚Äî Check first if your instance is connected to a Custom Update Center (UC) from an Operation Center. ¬∑ A new plugin releases should be published on the official ...
- [Upgrading from CloudBees CI versions older than 2.452.2.3](https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/best-practices/upgrading-from-versions-before-2-401-1-3) ‚Äî CloudBees CI versions 2.401.1.3 through 2.452.1.2 are affected by an HTTP Client leak that can cause severe performance degradation depending on environment ...
- [Issues with Jenkins Upgrade with huge number of plugins](https://stackoverflow.com/questions/75553296/issues-with-jenkins-upgrade-with-huge-number-of-plugins) ‚Äî I will recommend to switch to approach where you will install plugins before you start Jenkins. This will allow you to track and fix all errors in build phase.
- [Build Failures and Plugin Errors - Using Jenkins](https://community.jenkins.io/t/build-failures-and-plugin-errors/16821) ‚Äî After the Jenkins update, these plugins either fail to load correctly or produce errors during the build process. This has led to failed builds ...
- [Update Center best practices for plugin management](https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/best-practices/update-center-best-practices-for-plugin-management) ‚Äî Careful attention must be exercised during upgrades, to avoid such scenario. Not providing plugins for Jenkins Enterprise core plugins via the update-center.
- [Jenkins is not starting after plugin upgrade - Ask a question](https://community.jenkins.io/t/jenkins-is-not-starting-after-plugin-upgrade/17375) ‚Äî Hello Team, I have upgraded Jenkins version from 2.361.1 to 2.387.3 . Once upgraded to this version, unable to upgrade all the plugins.
- [CloudBees vs GitHub Actions: Choosing a CI/CD Platform for Your ...](https://www.cloudbees.com/blog/cloudbees-vs-github-actions-choosing-a-ci-cd-platform-for-your-business) ‚Äî This article compares CloudBees and GitHub Actions, examining their unique features, similarities, and differences.
- [Gitlab CI vs Jenkins vs GitHub Actions : r/devops - Reddit](https://www.reddit.com/r/devops/comments/105a2bn/gitlab_ci_vs_jenkins_vs_github_actions/) ‚Äî As you and others said, Jenkins is on its way out the door. Do not bother. Gitlab CI and Github Actions is the same Gitlab vs. Github argument.
- [GitLab vs. GitHub vs. Harness vs. CloudBees vs Devtron](https://devtron.ai/blog/gitlab-vs-github-vs-harness-vs-cloudbees-vs-devtron-choosing-the-right-devops-platform/) ‚Äî CloudBees builds on the popular Jenkins automation server, enhancing it for enterprise-scale DevOps. Here's why CloudBees might be the right ...
- [Jenkins vs. GitHub Actions vs. GitLab CI - DEV Community](https://dev.to/574n13y/jenkins-vs-github-actions-vs-gitlab-ci-2k35) ‚Äî A detailed breakdown of Jenkins, GitHub Actions, and GitLab CI, with specific use cases to highlight their strengths and weaknesses.
- [GitHub Actions vs. Jenkins: Which one's right for your team? | Buildkite](https://buildkite.com/resources/ci-cd-perspectives/github-actions-vs-jenkins-which-one-s-right-for-your-team/) ‚Äî GitHub Actions uses a cloud-first model that attempts to scale automatically, while Jenkins gives you a self-hosted model requiring more manual ...
- [Comparing GitHub Actions vs Jenkins: CI showdown - Pluralsight](https://www.pluralsight.com/resources/blog/cloud/comparing-github-actions-vs-jenkins-ci-showdown) ‚Äî In this post, we compare and contrast Jenkins and GitHub Actions and chat use cases so you can get a sense of which CI tool is best for your needs.
- [GitLab's Confusion - CloudBees](https://www.cloudbees.com/blog/gitlabs-confusion) ‚Äî Jenkins can and does do CI and CD. CloudBees' customers using CloudBees Core to manage Jenkins do both. Flow and Jenkins X make CloudBees' CD even better.
- [GitHub vs. GitLab and other DevOps tools](https://resources.github.com/devops/tools/compare/) ‚Äî We help you understand GitHub vs GitLab, how GitHub integrates with Azure and Jenkins, and present all the features GitHub has to offer.
- [An Overview of CI/CD Pipeline and tools ‚Äî Jenkins, Cloudbees ...](https://hanwenzhang123.medium.com/an-overview-of-cicd-pipeline-with-tools-jenkins-cloudbees-rancher-af246250dce9) ‚Äî Cloudbees is basically a similar tool to Jenkins, but more stable and powerful. It is more on the enterprise level that enables organizations to ...
- [How to Migrate Off Jenkins: The Road to Modern CI/CD | Blog](https://www.harness.io/blog/how-to-migrate-off-jenkins-the-road-to-modern-ci-cd) ‚Äî The Harness team can provide tooling to automate the migration of Jenkins pipelines. While this will migrate projects quickly, keep in mind ...
- [Convert a Jenkins pipeline to a Harness CD pipeline](https://developer.harness.io/docs/continuous-delivery/cd-onboarding/new-user/convert-jenkins-pipeline-to-harness) ‚Äî When beginning migration from Jenkins to Harness, we recommend targeting migration based on the application teams you want to enable on Harness ...
- [Migrating From Jenkins to Harness CIE: A Journey | Blog](https://www.harness.io/blog/jenkins-to-cie-journey) ‚Äî Migrating from Jenkins to Harness Continuous Integration (CIE) eliminated performance bottlenecks and increased developer productivity by ...
- [Moving from Jenkins to Harness, any advice and experience you ...](https://www.reddit.com/r/devops/comments/1lqynqk/moving_from_jenkins_to_harness_any_advice_and/) ‚Äî I have to learn more about Harness, and our org is moving from Jenkins to Harness. Some pain points I have heard is that it isn't working easily with Terraform.
- [Jenkins to Harness Migration/Modernization Demo - YouTube](https://www.youtube.com/watch?v=0BudbQ7hQWc) ‚Äî In this video, we demonstrate how easy it is to migrate your pipelines from Jenkins to Harness using our patented migration tool.
- [Migrating CD Jenkins Pipelines to Harness Using Helm | Blog](https://www.harness.io/blog/cd-jenkins-pipelines-harness) ‚Äî Migrating from Jenkins to Harness CD streamlined deployment processes, eliminating the need for scripting and enabling advanced deployment ...
- [Strategic Migration and Modernization: A Comprehensive Guide to ...](https://www.staffworx.co.uk/2025/07/23/strategic-migration-and-modernization-a-comprehensive-guide-to-transitioning-from-jenkins-to-the-harness-platform/) ‚Äî This report provides a comprehensive analysis and strategic playbook for migrating from the Jenkins automation server to the Harness ...
- [Break Free From Jenkins - Modernize Pipelines in 1 Day - Harness](https://www.harness.io/butler) ‚Äî With patented migration technology from Harness, you can transform your Jenkins nightmare into streamlined, modern AI DevOps pipelines in days not months.
- [Migrating to a new host and upgrading - Community - Jenkins](https://community.jenkins.io/t/migrating-to-a-new-host-and-upgrading/15526) ‚Äî I have just been tasked with upgrading our Jenkins 1.642.3-1 to the current version and moving the instance to a new VM.
- [Migrate from Jenkins - GitLab Docs](https://docs.gitlab.com/ci/migration/jenkins/) ‚Äî If you're migrating from Jenkins to GitLab CI/CD, you are able to create CI/CD pipelines that replicate and enhance your Jenkins workflows.
- [DORA's software delivery metrics: the four keys](https://dora.dev/guides/dora-metrics-four-keys/) ‚Äî DORA's four keys can be divided into metrics that show the throughput of software changes, and metrics that show stability of software changes.
- [Beyond Flow Metrics: A Holistic Approach to Software Value Delivery](https://uplevelteam.com/blog/beyond-flow-metrics) ‚Äî Flow metrics and DORA look at software delivery through two very different lenses. Use both to optimize processes, identify bottlenecks, ...
- [Best DevOps Platforms Reviews 2025 | Gartner Peer Insights](https://www.gartner.com/reviews/market/devops-platforms) ‚Äî Find the top Value Stream Delivery Platforms with Gartner. Compare and filter by verified product reviews and choose the software that's right for your ...
- [Best Internal Developer Portals Reviews 2025 | Gartner Peer Insights](https://www.gartner.com/reviews/market/internal-developer-portals) ‚Äî It offers comprehensive tooling for backend development and DevOps. The platform has unique features such as
- [A curated list of awesome DevOps platforms, tools ... - GitHub](https://github.com/wmariuss/awesome-devops) ‚Äî A curated list of platforms, tools, practices and resources to create, improve DevOps culture and SRE Team in the organization.

---

## üìà Trends
- [Show HN: OriGen ‚Äì A Deterministic Workflow Compiler (Maps ‚Üí IR ‚Üí CI/K8s)](https://origen-hub.github.io/origen-core-public/) ‚Äî <p>OriGen is a deterministic workflow compiler.<p>It takes declarative Maps (YAML) and produces a backend-neutral
Intermediate Representation (Route). Guides then translate the IR
into native execution artifacts: Kubernetes Jobs, CI configs, or
local container scripts.<p>The key properties:<p>‚Ä¢ Deterministic planning (no execution during compilation)<p>‚Ä¢ Digest-pinned toolchains (Navigators)<p>‚Ä¢ Immutable resource bundles (Backpacks)<p>‚Ä¢ Planning/execution separation<p>‚Ä¢ Backend neutrality (K8s first, others follow)<p>‚Ä¢ Zero-trust emerges as architecture (no hidden behavior)<p>‚Ä¢ Automatic Digital Provenance (ADP) falls out of determinism<p>OriGen does *not* run workflows. It compiles them.<p>The goal is to provide a stable planning layer above CI/CD,
orchestrators, and container engines: one Map ‚Üí many backends.<p>Documentation (Primer, Architecture, Zero-Trust, ADP):
<a href="https://origen-hub.github.io/origen-core-public/" rel="nofollow">https://origen-hub.github.io/origen-core-public/</a><p>This release is documentation-only (v0.0.1). IR and schema
prototypes are next. Early architectural feedback is welcome.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=46005747">https://news.ycombinator.com/item?id=46005747</a></p>
<p>Points: 1</p>
<p># Comments: 2</p>
- [DevOps-ifying my blog: Hugo on K8s with Git-sync and fluxcd](https://blog.prizrak.me/post/hugo-on-k8s/) ‚Äî <p>Article URL: <a href="https://blog.prizrak.me/post/hugo-on-k8s/">https://blog.prizrak.me/post/hugo-on-k8s/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45894000">https://news.ycombinator.com/item?id=45894000</a></p>
<p>Points: 1</p>
<p># Comments: 2</p>
- [Show HN: OriGen ‚Äì A Deterministic Workflow Compiler (Maps ‚Üí IR ‚Üí CI/K8s)](https://origen-hub.github.io/origen-core-public/) ‚Äî <p>OriGen is a deterministic workflow compiler.<p>It takes declarative Maps (YAML) and produces a backend-neutral
Intermediate Representation (Route). Guides then translate the IR
into native execution artifacts: Kubernetes Jobs, CI configs, or
local container scripts.<p>The key properties:<p>‚Ä¢ Deterministic planning (no execution during compilation)<p>‚Ä¢ Digest-pinned toolchains (Navigators)<p>‚Ä¢ Immutable resource bundles (Backpacks)<p>‚Ä¢ Planning/execution separation<p>‚Ä¢ Backend neutrality (K8s first, others follow)<p>‚Ä¢ Zero-trust emerges as architecture (no hidden behavior)<p>‚Ä¢ Automatic Digital Provenance (ADP) falls out of determinism<p>OriGen does *not* run workflows. It compiles them.<p>The goal is to provide a stable planning layer above CI/CD,
orchestrators, and container engines: one Map ‚Üí many backends.<p>Documentation (Primer, Architecture, Zero-Trust, ADP):
<a href="https://origen-hub.github.io/origen-core-public/" rel="nofollow">https://origen-hub.github.io/origen-core-public/</a><p>This release is documentation-only (v0.0.1). IR and schema
prototypes are next. Early architectural feedback is welcome.</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=46005747">https://news.ycombinator.com/item?id=46005747</a></p>
<p>Points: 1</p>
<p># Comments: 2</p>
- [DevOps-ifying my blog: Hugo on K8s with Git-sync and fluxcd](https://blog.prizrak.me/post/hugo-on-k8s/) ‚Äî <p>Article URL: <a href="https://blog.prizrak.me/post/hugo-on-k8s/">https://blog.prizrak.me/post/hugo-on-k8s/</a></p>
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=45894000">https://news.ycombinator.com/item?id=45894000</a></p>
<p>Points: 1</p>
<p># Comments: 2</p>
- [Evolving GitHub Copilot‚Äôs next edit suggestions through custom model training](https://github.blog/ai-and-ml/github-copilot/evolving-github-copilots-next-edit-suggestions-through-custom-model-training/) ‚Äî <p>GitHub Copilot‚Äôs next edit suggestions just got faster, smarter, and more precise thanks to new data pipelines, reinforcement learning, and continuous model updates built for in-editor workflows.</p>
<p>The post <a href="https://github.blog/ai-and-ml/github-copilot/evolving-github-copilots-next-edit-suggestions-through-custom-model-training/">Evolving GitHub Copilot‚Äôs next edit suggestions through custom model training</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [How we‚Äôre making GitHub Copilot smarter with fewer tools](https://github.blog/ai-and-ml/github-copilot/how-were-making-github-copilot-smarter-with-fewer-tools/) ‚Äî <p>We're using embedding-guided tool routing, adaptive clustering, and a streamlined 13-tool core to deliver faster experience in VS Code.</p>
<p>The post <a href="https://github.blog/ai-and-ml/github-copilot/how-were-making-github-copilot-smarter-with-fewer-tools/">How we‚Äôre making GitHub Copilot smarter with fewer tools</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [How to write a great agents.md: Lessons from over 2,500 repositories](https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/) ‚Äî <p>Learn how to write effective agents.md files for GitHub Copilot with practical tips, real examples, and templates from analyzing 2,500+ repositories.</p>
<p>The post <a href="https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/">How to write a great agents.md: Lessons from over 2,500 repositories</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [Unlocking the full power of Copilot code review: Master your instructions files](https://github.blog/ai-and-ml/unlocking-the-full-power-of-copilot-code-review-master-your-instructions-files/) ‚Äî <p>Discover practical tips, examples, and best practices for writing effective instructions files. Whether you‚Äôre new or experienced, you‚Äôll find something to level up your code reviews.</p>
<p>The post <a href="https://github.blog/ai-and-ml/unlocking-the-full-power-of-copilot-code-review-master-your-instructions-files/">Unlocking the full power of Copilot code review: Master your instructions files</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [How Copilot helps build the GitHub platform](https://github.blog/ai-and-ml/github-copilot/how-copilot-helps-build-the-github-platform/) ‚Äî <p>A breakdown of how Copilot coding agent has contributed to a better, more powerful GitHub.</p>
<p>The post <a href="https://github.blog/ai-and-ml/github-copilot/how-copilot-helps-build-the-github-platform/">How Copilot helps build the GitHub platform</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>
- [GitLab 18.6: From configuration to control](https://about.gitlab.com/blog/gitlab-18-6-from-configuration-to-control/) ‚Äî <p><em>Editor‚Äôs note: After this blog was originally published, the default Security Manager role was withdrawn from the release. It will be included in a future update. The content below has been updated for accuracy.</em></p>
<p>With <a href="https://about.gitlab.com/releases/2025/11/20/gitlab-18-6-released/">GitLab 18.6</a>, we‚Äôre continuing to advance how AI integrates into everyday software development with enhancements that give teams greater choice and control. GitLab 18.6 will help plan, build, and secure software more intelligently across the entire software lifecycle.
Teams now have greater flexibility to select the right models for their workflows, extend AI into secure and self-managed environments, and strengthen visibility and governance across every stage of development.</p>
<h2>AI that adapts to you</h2>
<p>With 18.6, GitLab‚Äôs AI becomes more adaptable to real-world workflows. GitLab Duo Agents now plan with greater context, work seamlessly across IDEs and self-managed instances, and offer new open-source model options ‚Äî helping teams accelerate delivery without compromising compliance or control.</p>
<p><strong>GitLab Duo Planner and Security Analyst agent enhancements</strong></p>
<p>In 18.6, <a href="https://about.gitlab.com/blog/ace-your-planning-without-the-context-switching/">GitLab Duo Planner</a> and <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/security_analyst_agent/">GitLab Duo Security Analyst</a> are now available by default in the Agentic Chat dropdown ‚Äî no configuration or setup required. Both agents can be used immediately across projects and groups, giving teams built-in assistance for planning, issue refinement, and security analysis.</p>
<p>GitLab Duo Planner agent now works at the group level with awareness of the epic being viewed and supports milestone and iteration workflows. Security Analyst agent provides automated vulnerability review, context interpretation, and guided remediation suggestions. Both agents are also available to self-managed customers.</p>
<p>For a full list of what these agents can do, see the <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/">documentation</a>.</p>
<p><strong>gpt-oss-120b model support for GitLab Duo Agent Platform</strong></p>
<p>GitLab Duo Self-Hosted customers can now deploy the <a href="https://platform.openai.com/docs/models/gpt-oss-120b"><strong>gpt-oss-120b</strong></a> model within the GitLab Duo Agent Platform ‚Äî a high-performance, fully open-source model optimized for agentic workflows. This addition enables teams to execute complex tasks and reasoning-driven processes while maintaining control over model transparency and infrastructure. For organizations that require open, auditable models to address compliance or data sovereignty requirements, gpt-oss-120b provides a reliable alternative to proprietary models without sacrificing performance.</p>
<p>For more information on supported models, please see our <a href="https://docs.gitlab.com/administration/gitlab_duo_self_hosted/supported_models_and_hardware_requirements/#supported-models">documentation</a>.</p>
<p><strong>End-user model selection for cloud-connected self-managed instances (GA)</strong></p>
<p>Cloud-connected self-managed end users can now choose which AI model powers their GitLab Duo Agentic Chat experience directly from the GitLab UI. This gives administrators and end users more control over how conversations perform and how costs and governance requirements are managed.</p>
<p>No matter the deployment environment ‚Äî on-premises, private cloud, or public cloud ‚Äî  teams can select regionally compliant or in-house models to help satisfy data residency needs and compare model quality for speed or accuracy. This flexibility ensures that every organization can tailor Agentic Chat to its operational priorities.</p>
<p>For full details on how to select a model in Agentic Chat, see the model selection section of the GitLab <a href="https://docs.gitlab.com/user/gitlab_duo_chat/agentic_chat/#select-a-model">documentation</a>.</p>
<p><strong>Web IDE support for air-gapped deployments</strong></p>
<p>Air-gapped or tightly controlled environments ‚Äî such as public sector organizations, defense agencies, and regulated enterprises ‚Äî can now run the Web IDE with full functionality even without internet access. By allowing administrators to configure their own Web IDE extension host domain, GitLab enables markdown preview, code editing, and GitLab Duo Chat capabilities in isolated or offline systems. This makes it possible for development teams in secure or restricted networks to benefit from modern IDE workflows without sacrificing security and compliance.</p>
<p><strong>Modern interface now default for self-managed instances</strong></p>
<p>Self-managed GitLab instances now default to the modern interface in 18.6, bringing the same streamlined experience already available on GitLab.com to on-premises deployments. The updated layout improves navigation consistency and makes core workflows more intuitive across the platform. Administrators maintain full flexibility with opt-out controls via feature flag or user-level toggling if needed. This update ensures self-managed customers benefit from GitLab's latest interface improvements while maintaining the control and customization options enterprise environments require.</p>
<h2>Platform security with awareness and authority</h2>
<p>GitLab 18.6 strengthens platform security with deeper context and clearer control, helping security teams focus on the risks that matter most while maintaining governance across every project.</p>
<p><strong>Security attributes and context filtering</strong></p>
<p>Security teams can now apply custom business context labels to projects and groups, transforming raw scan results into prioritized, risk-based insights. Instead of viewing vulnerabilities in isolation, teams can tag projects by business unit, application type, or criticality ‚Äî then filter and sort security data by impact. This allows organizations to focus remediation on the areas of greatest business risk, helping to accelerate time to resolution for the issues that matter most.</p>
<h2>AI that adapts to your workflow</h2>
<p>This release represents more than new capabilities ‚Äî it's about how GitLab Duo Agent Platform is becoming an embedded part of everyday software development workflows. Watch a walkthrough video that shows how a member of your software development team can start on a new project using GitLab Duo Agent Platform:</p>
<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1138657697?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.6 Demo (TO BE UPDATED)&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<h2>Get started today</h2>
<p>GitLab Premium and Ultimate users can start using these capabilities today on <a href="https://GitLab.com">GitLab.com</a> and self-managed environments, with availability for GitLab Dedicated customers planned for next month.</p>
<p>New to GitLab? <a href="https://about.gitlab.com/free-trial/devsecops/">Start your free trial</a> and see why the future of development is AI-powered, secure, and orchestrated through the world‚Äôs most comprehensive DevSecOps platform.</p>
<p><em><strong>Note:</strong> GitLab Duo Agent Platform is currently in beta. Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they are planned to be made available with a paid add-on option for GitLab Duo Agent Platform.</em></p>
<h3>Stay up to date with GitLab</h3>
<p>To make sure you‚Äôre getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:</p>
<ul>
<li>
<p><a href="https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/">Upgrade Path Tool</a> ‚Äî enter your current version and see the exact upgrade steps for your instance</p>
</li>
<li>
<p><a href="https://docs.gitlab.com/update/upgrade_paths/">Upgrade Documentation</a> ‚Äî detailed guides for each supported version, including requirements, step-by-step instructions, and best practices</p>
</li>
</ul>
<p>By upgrading regularly, you‚Äôll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.</p>
<p>For organizations that want a hands-off approach, consider <a href="https://content.gitlab.com/viewer/d1fe944dddb06394e6187f0028f010ad#1">GitLab‚Äôs Managed Maintenance service</a>. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.</p>
<p><em>This blog post contains &quot;forward‚Äëlooking statements&quot; within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption &quot;Risk Factors&quot; in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.</em></p>
- [Achieve CMMC Level 2 with GitLab Dedicated for Government](https://about.gitlab.com/blog/achieve-cmmc-level-2-fast-with-gitlab-dedicated-for-government/) ‚Äî <p>For Defense Industrial Base (DIB) companies, the U.S. Department of Defense's release of the Cybersecurity Maturity Model Certification (CMMC) <a href="https://www.federalregister.gov/documents/2025/09/10/2025-17359/defense-federal-acquisition-regulation-supplement-assessing-contractor-implementation-of_">Final Rule</a> and new guidance on ‚ÄúFedRAMP equivalency‚Äù has dramatically increased the cost of compliance and fundamentally changed the way in which they drive their risk management programs. Gone is the era of ‚Äúself-attestation‚Äù of security programs; DIB companies are required to strictly apply NIST 800-171 to their environments that handle Controlled Unclassified Information (CUI), and have their security controls audited by a Third-Party Assessment Organization (3PAO) every three years.</p>
<p>DIB companies are engineering focused, not compliance driven, and formal audits get pricey quickly. These changes add significant complications for companies focused on supporting the warfighter. The good news? <a href="https://about.gitlab.com/press/releases/2025-05-19-gitlab-announces-gitlab-achieves-fedramp-moderate-authorization/">GitLab Dedicated for Government's FedRAMP Moderate Authorization</a> means DIB companies can directly use GitLab Dedicated for Government with no additional audits or authorizations, which reduces the impact and cost of compliance.</p>
<h2>The foundational rule: FedRAMP Moderate Equivalency</h2>
<p>The protection of Controlled Unclassified Information (CUI) within the DIB is driven by a foundational legal and contractual mandate: the Defense Federal Acquisition Regulation Supplement (DFARS) <a href="https://www.acquisition.gov/dfars/252.204-7012-safeguarding-covered-defense-information-and-cyber-incident-reporting.">Clause 252.204-7012</a>. This clause specifically states that if a contractor uses an external cloud service provider to &quot;store, process, or transmit any covered defense information,&quot; that provider must meet security requirements &quot;equivalent to those established by the Government for the FedRAMP Moderate baseline.&quot;</p>
<p>The DOD's January 2, 2024, memorandum, &quot;<a href="https://dodcio.defense.gov/Portals/0/Documents/Library/FEDRAMP-EquivalencyCloudServiceProviders.pdf">Federal Risk and Authorization Management Program (FedRAMP) Moderate Equivalency for Cloud Service Provider's (CSPs) Cloud Service Offerings</a>&quot; defines ‚ÄúFedRAMP Moderate Equivalency,‚Äù and also directly specifies that FedRAMP Moderate Cloud Service Offerings (CSOs) can be used without any additional assessment, such as individual CMMC assessment, to meet equivalency requirements:</p>
<p>‚ÄúThis memorandum does not apply to CSOs that are FedRAMP Moderate Authorized under the existing FedRAMP process. <strong>FedRAMP Moderate Authorized CSOs identified in the FedRAMP Marketplace</strong> provide the required security to store, process or transmit CDI in accordance with Defense Federal Acquisition Regulations Supplement (DFARS) Clause 252.204-7012, &quot;Safeguarding Covered Defense Information and Cyber Incident Reporting&quot; and <strong>can be leveraged without further assessment to meet the equivalency requirements</strong>.‚Äù</p>
<h2>The GitLab platform: A proven path to compliance</h2>
<p>GitLab's GovCloud Offering, GitLab Dedicated for Government, <a href="https://marketplace.fedramp.gov/products/FR2411959145">has achieved FedRAMP Moderate Authorization</a>. This means that DIB companies can leverage GitLab Dedicated for Government as their DevSecOps platform immediately and without any additional audits or compliance checks. DIB companies leveraging GitLab Dedicated for Government inherit all of our security controls and our Body of Evidence, shifting the risk and cost of compliance away from themselves and allowing them to focus on their mission.</p>
<h2>The Shared Responsibility Matrix: Your role as a DIB contractor</h2>
<p>While a FedRAMP-authorized solution significantly reduces your compliance burden, compliance is a joint effort. You are responsible for the security controls that fall under your purview. This is where the Shared Responsibility Matrix (SRM), also called the Customer Responsibility Matrix (CRM), comes in.</p>
<p>When you adopt GitLab Dedicated for Government, you will receive a comprehensive SRM that clearly delineates which security controls are managed by GitLab and which are your responsibility as the customer. Your CMMC C3PAO will use this document to ensure you have implemented the necessary controls on your end. By leveraging GitLab's FedRAMP-authorized platform, you can confidently address your CMMC Level 2 compliance requirements, focusing on your mission while trusting that GitLab has you covered.</p>
<blockquote>
<p>To learn more about GitLab Dedicated for Government, visit our <a href="https://about.gitlab.com/solutions/public-sector/">GitLab for Public Sector</a> page. Interested in a demo? Contact Sales for more information at <a href="mailto:sales-pubsec@gitlab.com">sales-pubsec@gitlab.com</a>.</p>
</blockquote>
<h2>References</h2>
<ul>
<li><a href="https://www.federalregister.gov/documents/2025/09/10/2025-17359/defense-federal-acquisition-regulation-supplement-assessing-contractor-implementation-of">CMMC ‚ÄúFinal Rule‚Äù DFARS Supplement</a></li>
<li><a href="https://dodcio.defense.gov/Portals/0/Documents/Library/FEDRAMP-EquivalencyCloudServiceProviders.pdf">DOD-CIO ‚ÄúFedRAMP Moderate Equivalency‚Äù Memo</a></li>
<li><a href="https://marketplace.fedramp.gov/products/FR2411959145">GitLab Dedicated for Government FedRAMP Marketplace Listing</a></li>
</ul>
- [Secure AI agent deployment to GKE](https://about.gitlab.com/blog/secure-ai-agent-deployment-to-gke/) ‚Äî <p>Building <a href="https://about.gitlab.com/gitlab-duo/agent-platform/">AI agents</a> is</p>
<p>exciting, but deploying them securely to production shouldn't be</p>
<p>complicated. In this tutorial, you will learn how GitLab's <a href="https://cloud.google.com/blog/topics/partners/understand-the-google-cloud-gitlab-integration">native Google Cloud integration</a> makes it straightforward to deploy AI agents to Google Kubernetes Engine (GKE) ‚Äî with built-in scanning and zero service account keys.</p>
<h2>Why choose GKE to deploy your AI agents?</h2>
<p>GKE provides enterprise-grade orchestration that connects seamlessly with GitLab CI/CD pipelines through OIDC authentication. Your development team can deploy AI agents while maintaining complete visibility, compliance, and control over your cloud infrastructure. This guide uses Google's Agent Development Kit (<a href="https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/">ADK</a>) to build the app, so you can expect increased seamlessness as this is deployed using GitLab.</p>
<p>Three key advantages to this approach:</p>
<p><strong>Full infrastructure control</strong> - Your data, your rules, your environment. You maintain complete control over where your AI agents run and how they're configured.</p>
<p><strong>Native GitLab integration</strong> - No complex workarounds. Your existing pipelines work right out of the box thanks to GitLab's native integration with Google Cloud.</p>
<p><strong>Production-grade scaling</strong> - GKE automatically handles the heavy lifting of scaling and internal orchestration as your AI workloads grow.</p>
<p>The key point is that GitLab with GKE provides the enterprise reliability your AI deployments demand without sacrificing the developer experience your teams expect.</p>
<h2>Prerequisites</h2>
<p>Before you start, make sure you have these APIs enabled:</p>
<ul>
<li>
<p>GKE API</p>
</li>
<li>
<p>Artifact Registry API</p>
</li>
<li>
<p>Vertex AI API</p>
</li>
</ul>
<p>Also make sure you have:</p>
<ul>
<li>
<p>GitLab project created</p>
</li>
<li>
<p>GKE cluster provisioned</p>
</li>
<li>
<p>Artifact Registry repository created</p>
</li>
</ul>
<h2>The deployment process</h2>
<h3>1. Set up IAM and permissions on GitLab</h3>
<p>Navigate to your GitLab integrations to configure Google Cloud authentication (IAM).</p>
<p>Go to <strong>Settings &gt; Integrations</strong> and configure the Google Cloud integration. If you're using a group-level integration, notice that default settings are already inherited by projects. This means you configure once at the group level, and all projects benefit and inherit this setting.</p>
<p>To set this up from scratch, provide:</p>
<ul>
<li>
<p>Project ID</p>
</li>
<li>
<p>Project Number</p>
</li>
<li>
<p>Workload Identity Pool ID</p>
</li>
<li>
<p>Provider ID</p>
</li>
</ul>
<p>Once configured, GitLab provides a script to run in Google Cloud Console, via Cloud Shell. The outcome of running this script is a Workload Identity Federation pool with the necessary service principal to enable the proper access.</p>
<h3>2. Configure Artifact Registry integration</h3>
<p>Still in GitLab's integration settings, configure Artifact Management:</p>
<ol>
<li>
<p>Click <strong>Artifact Management</strong>.</p>
</li>
<li>
<p>Select <strong>Google Artifact Registry</strong>.</p>
</li>
<li>
<p>Provide:</p>
<ul>
<li>Project ID</li>
<li>Repository Name (created beforehand)</li>
<li>Repository Location</li>
</ul>
</li>
</ol>
<p>GitLab provides another script to run in Google Cloud Console.</p>
<p><strong>Important:</strong> Before proceeding, add these extra roles to the Workload Identity Federation pool:</p>
<ul>
<li>
<p>Service Account User</p>
</li>
<li>
<p>Kubernetes Developer</p>
</li>
<li>
<p>Kubernetes Cluster Viewer</p>
</li>
</ul>
<p>These permissions allow GitLab to deploy to GKE in subsequent steps.</p>
<h3>3. Create the CI/CD pipeline</h3>
<p>Now for the key part ‚Äî creating the CI/CD pipeline for deployment.</p>
<p>Head to <strong>Build &gt; Pipeline Editor</strong> and define your pipeline with four stages:</p>
<ul>
<li>
<p><strong>Build</strong> - Docker creates the container image.</p>
</li>
<li>
<p><strong>Test</strong> - GitLab Auto DevOps provides built-in security scans to ensure there are no vulnerabilities.</p>
</li>
<li>
<p><strong>Upload</strong> - Uses GitLab's built-in CI/CD component to push to Google Artifact Registry.</p>
</li>
<li>
<p><strong>Deploy</strong> - Uses Kubernetes configuration to deploy to GKE.</p>
</li>
</ul>
<p>Here's the complete <code>.gitlab-ci.yml</code>:</p>
<pre><code class="language-yaml">

default:
  tags: [ saas-linux-2xlarge-amd64 ]

stages:
  - build
  - test
  - upload
  - deploy

variables:
  GITLAB_IMAGE: $CI_REGISTRY_IMAGE/main:$CI_COMMIT_SHORT_SHA
  AR_IMAGE: $GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_LOCATION-docker.pkg.dev/$GOOGLE_ARTIFACT_REGISTRY_PROJECT_ID/$GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_NAME/main:$CI_COMMIT_SHORT_SHA
  GCP_PROJECT_ID: &quot;your-project-id&quot;
  GKE_CLUSTER: &quot;your-cluster&quot;
  GKE_REGION: &quot;us-central1&quot;
  KSA_NAME: &quot;ai-agent-ksa&quot;

build:
  image: docker:24.0.5
  stage: build
  services:
    - docker:24.0.5-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $GITLAB_IMAGE .
    - docker push $GITLAB_IMAGE

include:
  - template: Jobs/Dependency-Scanning.gitlab-ci.yml
  - template: Jobs/Container-Scanning.gitlab-ci.yml
  - template: Jobs/Secret-Detection.gitlab-ci.yml
  - component: gitlab.com/google-gitlab-components/artifact-registry/upload-artifact-registry@main
    inputs:
      stage: upload
      source: $GITLAB_IMAGE
      target: $AR_IMAGE

deploy:
  stage: deploy
  image: google/cloud-sdk:slim
  identity: google_cloud
  before_script:
    - apt-get update &amp;&amp; apt-get install -y kubectl google-cloud-sdk-gke-gcloud-auth-plugin
    - gcloud container clusters get-credentials $GKE_CLUSTER --region $GKE_REGION --project $GCP_PROJECT_ID
  script:
    - |
      kubectl apply -f - &lt;&lt;EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ai-agent
        namespace: default
      spec:
        replicas: 2
        selector:
          matchLabels:
            app: ai-agent
        template:
          metadata:
            labels:
              app: ai-agent
          spec:
            serviceAccountName: $KSA_NAME
            containers:
            - name: ai-agent
              image: $AR_IMAGE
              ports:
              - containerPort: 8080
              resources:
                requests: {cpu: 500m, memory: 1Gi}
                limits: {cpu: 2000m, memory: 4Gi}
              livenessProbe:
                httpGet: {path: /health, port: 8080}
                initialDelaySeconds: 60
              readinessProbe:
                httpGet: {path: /health, port: 8080}
                initialDelaySeconds: 30
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: ai-agent-service
        namespace: default
      spec:
        type: LoadBalancer
        ports:
        - port: 80
          targetPort: 8080
        selector:
          app: ai-agent
      ---
      apiVersion: autoscaling/v2
      kind: HorizontalPodAutoscaler
      metadata:
        name: ai-agent-hpa
        namespace: default
      spec:
        scaleTargetRef:
          apiVersion: apps/v1
          kind: Deployment
          name: ai-agent
        minReplicas: 2
        maxReplicas: 10
        metrics:
        - type: Resource
          resource:
            name: cpu
            target: {type: Utilization, averageUtilization: 70}
      EOF
      
      kubectl rollout status deployment/ai-agent -n default --timeout=5m
      EXTERNAL_IP=$(kubectl get service ai-agent-service -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
      echo &quot;Deployed at: http://$EXTERNAL_IP&quot;
  only:
    - main
</code></pre>
<h4>The critical configuration for GKE</h4>
<p>What makes this work ‚Äî and why we need this extra configuration for GKE‚Äî is that we must have a Kubernetes Service Account in the cluster that can work with Vertex AI. We need that service account to be permitted to access the AI capabilities of Google Cloud.</p>
<p>Without this, we can deploy the application, but the AI agent won't work. We need to create a Kubernetes Service Account that can access Vertex AI.</p>
<p>Run this one-time setup:</p>
<pre><code class="language-bash">

#!/bin/bash


PROJECT_ID=&quot;your-project-id&quot;


GSA_NAME=&quot;ai-agent-vertex&quot;


GSA_EMAIL=&quot;${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&quot;


KSA_NAME=&quot;ai-agent-ksa&quot;


CLUSTER_NAME=&quot;your-cluster&quot;


REGION=&quot;us-central1&quot;



# Create GCP Service Account


gcloud iam service-accounts create $GSA_NAME \
    --display-name=&quot;AI Agent Vertex AI&quot; \
    --project=$PROJECT_ID

# Grant Vertex AI permissions


gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member=&quot;serviceAccount:${GSA_EMAIL}&quot; \
    --role=&quot;roles/aiplatform.user&quot;

# Get cluster credentials


gcloud container clusters get-credentials $CLUSTER_NAME \
    --region $REGION --project $PROJECT_ID

# Create Kubernetes Service Account


kubectl create serviceaccount $KSA_NAME -n default



# Link accounts


kubectl annotate serviceaccount $KSA_NAME -n default \
    iam.gke.io/gcp-service-account=${GSA_EMAIL}

gcloud iam service-accounts add-iam-policy-binding ${GSA_EMAIL} \
    --role=roles/iam.workloadIdentityUser \
    --member=&quot;serviceAccount:${PROJECT_ID}.svc.id.goog[default/${KSA_NAME}]&quot; \
    --project=$PROJECT_ID
</code></pre>
<h3>4. Deploy to GKE</h3>
<p>Once you're done, push this change to the pipeline and you're good to go.</p>
<p>You can see the pipeline has just deployed. Go to <strong>CI/CD &gt; Pipelines</strong> and you'll see the four stages:</p>
<ul>
<li>
<p>Build</p>
</li>
<li>
<p>Test (with all defined security scans)</p>
</li>
<li>
<p>Upload to Artifact Registry (successful)</p>
</li>
<li>
<p>Deploy to Kubernetes in GKE (success)</p>
</li>
</ul>
<h2>Summary</h2>
<p>With GitLab and Google Cloud together, you're able to deploy your AI agent to GKE with ease and security. We didn't have to go through a lot of steps ‚Äî we were able to do that thanks to GitLab's native integration with Google Cloud.</p>
<p>Watch this demo:</p>
<p>&lt;!-- blank line --&gt;</p>
<p>&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/mc2pCL5Qjus?si=QoH02lvz5KH5Ku9O&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
&lt;/figure&gt;</p>
<p>&lt;!-- blank line --&gt;</p>
<blockquote>
<p>Use this tutorial's <a href="https://gitlab.com/gitlab-partners-public/google-cloud/demos/gke-ai-agent">complete code example</a> to get started now. Not a GitLab customer yet? Explore the DevSecOps platform with <a href="https://about.gitlab.com/free-trial/">a free trial</a>. Startups hosted on Google Cloud have a <a href="https://about.gitlab.com/solutions/startups/google-cloud/">special perk to try and use GitLab</a>.</p>
</blockquote>
- [Migrate from pipeline variables to pipeline inputs for better security](https://about.gitlab.com/blog/migrate-from-pipeline-variables-to-pipeline-inputs-for-better-security/) ‚Äî <p><a href="https://docs.gitlab.com/ci/variables/#use-pipeline-variables">Pipeline
variables</a>
have long been a convenient way to customize GitLab CI/CD pipelines at
runtime. However, as CI/CD security best practices have evolved, we've
recognized the need for stronger controls around pipeline customization.
Unrestricted pipeline variables allow any users with pipeline trigger
permissions to override values without validation or type checking.</p>
<p>Beyond security considerations, pipeline variables lack proper documentation and explicit declaration, making it difficult to understand what inputs are expected and how they're used throughout your pipeline. This can lead to maintenance challenges and make it harder to establish proper governance over your <a href="https://about.gitlab.com/topics/ci-cd/">CI/CD</a> processes.</p>
<h2>Enter pipeline inputs</h2>
<p>Instead of relying on pipeline variables, we strongly recommend using GitLab's <a href="https://docs.gitlab.com/ci/inputs/#for-a-pipeline">pipeline inputs</a> feature. Pipeline inputs provide:</p>
<ul>
<li>
<p><strong>Explicit declaration</strong>: Inputs must be explicitly declared in your <code>.gitlab-ci.yml</code> and are self-documented.</p>
</li>
<li>
<p><strong>Type safety</strong>: Support for different input types (string, boolean, number, array)</p>
</li>
<li>
<p><strong>Built-in validation</strong>: Automatic validation of input values</p>
</li>
<li>
<p><strong>Better security</strong>: No risk of variable injection attacks ‚Äî only the declared inputs can be passed from the outside</p>
</li>
</ul>
<h3>Basic example</h3>
<pre><code>
spec:
  inputs:
    deployment_env:
      description: &quot;Target deployment environment&quot;
      type: string
      options: [&quot;staging&quot;, &quot;production&quot;]
      default: &quot;staging&quot;
    enable_tests:
      description: &quot;Run test suite&quot;
      type: boolean
      default: true

test:
  script:
    - echo &quot;Running tests&quot;
  rules:
    - if: $[[ inputs.enable_tests ]] == true

deploy:
  script:
    - echo &quot;Deploying to $[[ inputs.deployment_env ]]&quot;
</code></pre>
<p>Learn more about how CI/CD inputs provide type-safe parameter passing with validation in this <a href="https://about.gitlab.com/blog/ci-cd-inputs-secure-and-preferred-method-to-pass-parameters-to-a-pipeline/">tutorial</a>.</p>
<h2>Restrict pipeline variables</h2>
<p>To effectively move to pipeline inputs and away from pipeline variables, you should configure the <a href="https://docs.gitlab.com/ci/variables/#restrict-pipeline-variables">&quot;Minimum role to use pipeline variables&quot;</a> setting. This setting provides fine-grained control over which role can use pipeline variables when triggering pipelines.</p>
<p><strong>At the project level:</strong> Navigate to your project's <strong>Settings &gt; CI/CD &gt; Variables &gt; Minimum role to use pipeline variables</strong> to configure the setting.</p>
<p>Available options are:</p>
<ul>
<li>
<p><strong>No one allowed</strong> (<code>no_one_allowed</code>) - Recommended and most secure option. Prevents all variable overrides.</p>
</li>
<li>
<p><strong>Developer</strong> (<code>developer</code>) - Allows developers and above to override variables</p>
</li>
<li>
<p><strong>Maintainer</strong> (<code>maintainer</code>) - Requires maintainer role or higher</p>
</li>
<li>
<p><strong>Owner</strong> (<code>owner</code>) - Only project owners can override variables</p>
</li>
</ul>
<p><strong>At the group level:</strong> Group maintainers can go to <strong>Settings &gt; CI/CD &gt; Variables &gt; Default role to use pipeline variables</strong> to establish secure defaults for all new projects within their group, ensuring consistent security policies across your organization. Here we recommend again to use <strong>No one allowed</strong> as default value ‚Äî this way, new projects in this group are created with a secure default setting. Note that this still allows project owners to change the setting.</p>
<p>When pipeline variables are restricted completely (with ‚ÄúNo one allowed‚Äù), the <a href="https://docs.gitlab.com/ci/pipelines/#prefill-variables-in-manual-pipelines">prefilled variables</a> won‚Äôt appear in the &quot;New Pipeline UI&quot; form.</p>
<h2>How to migrate from pipeline variables</h2>
<h3>Close the gaps</h3>
<p>Your group may have projects that have pipeline variables enabled by default despite never having used them when triggering a pipeline. These projects can be migrated to the more secure setting without a risk of interruption. GitLab <a href="https://docs.gitlab.com/ci/variables/#enable-pipeline-variable-restriction-for-multiple-projects">provides migration functionality</a> via group settings:</p>
<ul>
<li>
<p>Go to <strong>Settings &gt; CI/CD &gt; Variables</strong></p>
</li>
<li>
<p>In <strong>Disable pipeline variables in projects that don‚Äôt use them,</strong> select <strong>Start migration</strong>.</p>
</li>
</ul>
<p>This migration is a background job that safely disables pipeline variables via project settings for all projects that historically have not used them.</p>
<h3>Convert pipeline variables to inputs</h3>
<p>For each identified pipeline variable, create a corresponding pipeline input.</p>
<p><strong>Before (using pipeline variables)</strong></p>
<pre><code>
variables:
  DEPLOY_ENV:
    description: &quot;Deployment environment&quot;
    value: &quot;staging&quot;
  ENABLE_CACHE:
    description: &quot;Enable deployment cache&quot;
    value: &quot;true&quot;
  VERSION:
    description: &quot;Application version&quot;
    value: &quot;1.0.0&quot;

deploy:
  script:
    - echo &quot;Deploying version $VERSION to $DEPLOY_ENV&quot;
    - |
      if [ &quot;$ENABLE_CACHE&quot; = &quot;true&quot; ]; then
        echo &quot;Cache enabled&quot;
      fi
</code></pre>
<p><strong>After (using pipeline inputs)</strong></p>
<pre><code>
spec:
  inputs:
    deploy_env:
      description: &quot;Deployment environment&quot;
      type: string
      default: &quot;staging&quot;
      options: [&quot;dev&quot;, &quot;staging&quot;, &quot;production&quot;]

    enable_cache:
      description: &quot;Enable deployment cache&quot;
      type: boolean
      default: true
    
    version:
      description: &quot;Application version&quot;
      type: string
      default: &quot;1.0.0&quot;
      regex: '^[0-9]+\.[0-9]+\.[0-9]+$'

deploy:
  script:
    - echo &quot;Deploying version $[[ inputs.version ]] to $[[ inputs.deploy_env ]]&quot;
    - |
      if [ &quot;$[[ inputs.enable_cache ]]&quot; = &quot;true&quot; ]; then
        echo &quot;Cache enabled&quot;
      fi
</code></pre>
<h3>Migrate trigger jobs</h3>
<p>If you use trigger jobs with the <code>trigger</code> keyword, ensure they don't define job-level <code>variables</code> or disable inheriting variables from top-level <code>variables</code>, <code>extends</code>, or <code>include</code>, because variables could implicitly be passed downstream as pipeline variables. If pipeline variables are restricted on the downstream project, pipeline creation will fail.</p>
<p>Consider updating your CI configuration to use pipeline inputs instead of pipeline variables.</p>
<pre><code>
variables:
  FOO: bar

deploy-staging:
  inherit:
    variables: false # otherwise FOO would be sent downstream as a pipeline variable
  trigger:
    project: myorg/deployer
    inputs:
      deployment_env: staging
      enable_tests: true
</code></pre>
<h2>Summary</h2>
<p>Migrating from pipeline variables to pipeline inputs is a security enhancement that protects your CI/CD infrastructure from variable injection while providing better documentation, type safety, and validation. By implementing these restrictions and adopting pipeline inputs, you're not just improving security, you're also making your pipelines more maintainable, self-documenting, and resilient.</p>
<p>The transition requires some initial effort, but the long-term benefits far outweigh the migration costs. Start by restricting pipeline variables at the group level for new projects, then systematically migrate existing pipelines using the step-by-step approach outlined above.</p>
<p>Security is not a destination but a journey. Pipeline inputs are one important step in creating a more secure CI/CD environment, complementing other GitLab security features like protected branches, job token allowlists, and container registry protections.</p>
<blockquote>
<p>To get started with pipeline inputs, <a href="https://about.gitlab.com/free-trial/devsecops/">sign up for a free trial of GitLab Ultimate today</a>.</p>
</blockquote>
- [Modernize Java applications quickly with GitLab Duo with Amazon Q](https://about.gitlab.com/blog/modernize-java-applications-quickly-with-gitlab-duo-with-amazon-q/) ‚Äî <p>Upgrading applications to newer, supported versions of Java has traditionally been a tedious and time-consuming process. Development teams must spend countless hours learning about deprecated APIs, updated libraries, and new language features. In many cases, significant code rewrites are necessary, turning what should be a straightforward upgrade into a multi-week project that diverts resources from building new features.</p>
<p><a href="https://about.gitlab.com/gitlab-duo/duo-amazon-q/">GitLab Duo with Amazon Q</a> changes this paradigm entirely with AI-powered automation. What once took weeks can now be accomplished in minutes, with full traceability and ready-to-review merge requests that maintain your application's functionality while leveraging modern Java features.</p>
<h2>How it works: Upgrade your Java application</h2>
<p>Let's walk through how you can modernize a Java 8 application to Java 17.</p>
<p><strong>Start with an issue</strong></p>
<p>First, create an issue in your GitLab project describing your modernization goal. You don't need to specify version details - GitLab Duo with Amazon Q is able to detect that your application is currently built with Java 8 and needs to be upgraded. Simply describe that you want to refactor your code to Java 17 in the issue title and description.</p>
<p><strong>Trigger the transformation</strong></p>
<p>Once your issue is created, invoke GitLab Duo with Amazon Q using the <code>/q transform</code> command in a comment on the issue. This simple command sets in motion an automated process that will analyze your entire codebase, create a comprehensive upgrade plan, and generate all necessary code changes.</p>
<p><strong>Automated analysis and implementation</strong></p>
<p>Behind the scenes, Amazon Q analyzes your Java 8 codebase to understand your application's structure, dependencies, and implementation patterns. It identifies deprecated features, determines which Java 17 constructs can replace existing code, and creates a merge request with all the necessary updates. The transformation updates not just your source code files ‚Äî including CLI, GUI, and model classes ‚Äî but also your build configuration files like <code>pom.xml</code> with Java 17 settings and dependencies.</p>
<p><strong>Review and verification</strong></p>
<p>The generated merge request provides a complete view of all changes. You can review how your code has been modernized with Java 17 language features and verify that all tests still pass. The beauty of this approach is that all functionality is preserved and your application works exactly the same way, just with improved, more modern code.</p>
<h2>Why use GitLab Duo with Amazon Q</h2>
<p>Leveraging GitLab Duo with Amazon Q for application modernization has a number of advantages for development teams:</p>
<p><strong>Time reduction</strong>: What traditionally takes weeks of developer effort is reduced to hours or minutes, freeing your team to focus on building new features rather than managing technical debt.</p>
<p><strong>Minimized risk</strong>: The automated analysis and transformation process reduces the risk of human error that often accompanies manual code migrations. Every change is traceable and reviewable through GitLab's merge request workflow.</p>
<p><strong>Complete audit trail</strong>: Every transformation is documented through GitLab's version control, providing a clear record of what changed and why, which is essential for compliance and troubleshooting.</p>
<p><strong>Enterprise-grade security</strong>: The integration leverages GitLab's end-to-end security features and AWS's robust cloud infrastructure, helping to ensure your code and data remain protected throughout the modernization process.</p>
<p>Are you ready to see GitLab Duo with Amazon Q in action? Watch our complete walkthrough video demonstrating the Java modernization process from start to finish:</p>
<p>&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/qGyzG9wTsEo?si=47JnSb6flOgZAJcR&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;</p>
<blockquote>
<p>To learn more about GitLab Duo with Amazon Q visit our <a href="https://about.gitlab.com/gitlab-duo/duo-amazon-q/">web site</a> or reach out to your GitLab representative.</p>
</blockquote>
<h2>Read more</h2>
<ul>
<li><a href="https://about.gitlab.com/blog/agentic-ai-guides-and-resources/">Agentic AI guides and resources</a></li>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-with-amazon-q-devsecops-meets-agentic-ai/">GitLab Duo with Amazon Q: DevSecOps meets agentic AI</a></li>
<li><a href="https://about.gitlab.com/blog/agentic-ai-guides-and-resources/#gitlab-duo-with-amazon-q-tutorials">More GitLab Duo with Amazon Q tutorials</a></li>
</ul>
- [Delivering faster and smarter scans with Advanced SAST](https://about.gitlab.com/blog/delivering-faster-and-smarter-scans-with-advanced-sast/) ‚Äî <p>Static application security testing (SAST) is critical to building secure software, helping teams identify vulnerabilities in code before they can be exploited. Last year, with GitLab 17.4, we <a href="https://about.gitlab.com/blog/gitlab-advanced-sast-is-now-generally-available/">launched Advanced SAST</a> to deliver higher-quality scan results directly in developer workflows. Since then, Advanced SAST has powered millions of scans across over a hundred thousand codebases, reducing risk and helping customers build more secure applications from the start.</p>
<p>We‚Äôre building on that foundation with a set of performance enhancements designed to improve accuracy and speed, so developers get results they can trust, without losing their flow. <a href="https://about.gitlab.com/blog/gitlab-18-5-intelligence-that-moves-software-development-forward/">New capabilities</a> include better out-of-the-box precision, the ability to add custom detection rules, and a trio of improvements to accelerate scan times through multi-core scanning, algorithmic optimizations, and diff-based scanning. Together, these improvements make <a href="https://docs.gitlab.com/user/application_security/sast/gitlab_advanced_sast/">Advanced SAST</a> smarter and faster, delivering security that‚Äôs developer-friendly by design.</p>
<h2>SAST adoption hinges on both accuracy and speed</h2>
<p>Most SAST programs rarely fail due to inaccurate vulnerability detection; they fail because developers don‚Äôt adopt security tooling. Too often, AppSec solutions like SAST deliver accuracy at the expense of the developer experience, or developer experience at the expense of accuracy. In reality, both are necessary. Without accuracy, developers don‚Äôt trust the results; without speed and usability, adoption lags.</p>
<p>When both come together, security fits naturally into the development process ‚Äî and that‚Äôs the only way security teams successfully drive SAST adoption at scale. This philosophy guides the GitLab roadmap for Advanced SAST.</p>
<h2>Add custom detection rules for greater accuracy</h2>
<p>The built-in Advanced SAST rules are informed by our in-house security research team, designed to maximize accuracy out of the box. Until now, you could <a href="https://docs.gitlab.com/user/application_security/sast/customize_rulesets/">disable rules</a> or adjust their name, description, or severity, but you couldn‚Äôt add new detection logic. With GitLab 18.5, teams can now define their own custom, pattern-based rules to catch organization-specific issues ‚Äî like flagging banned function calls ‚Äî while still using GitLab‚Äôs curated ruleset as the baseline. Any violations of custom rules are reported in the same place as built-in GitLab rules, so developers can glean information from a single dashboard.</p>
<p>Custom rules are effective at catching straightforward issues that matter to your organization, but they don‚Äôt influence the taint analysis that Advanced SAST uses to catch injections and similar flaws. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. The result is higher-quality scan results tuned to your context, giving security teams more control and developers clearer, more actionable findings.</p>
<h2>Faster scans to get developers in the flow</h2>
<p>Speed matters. If a SAST scan takes too long, developers often switch to another task, so adoption suffers.</p>
<p>That‚Äôs why we‚Äôve invested in several performance-based enhancements to dramatically reduce scan times without compromising on accuracy, including:</p>
<ul>
<li><strong>Multi-core scanning</strong>: Leverages multiple CPU cores on GitLab Runners</li>
<li><strong>Diff-based scanning</strong>: Scans only the changed code in a merge request</li>
<li><strong>Ongoing optimizations</strong>: Smarter algorithms and engine enhancements</li>
</ul>
<p>These improvements build on each other, delivering faster scans with significant impact:</p>
<ul>
<li>Multi-core scanning typically reduces scan runtime by up to <strong>50%.</strong></li>
<li>Diff-based scanning helps the most in large repositories, where less code is modified in each change. It‚Äôs specifically designed to give faster feedback in the code review process by delivering faster scans in merge requests. In our testing, many large repositories now take less than <strong>10 minutes to return results in MRs, where previously scans took more than 20 minutes.</strong></li>
<li>In recent internal testing, algorithmic optimizations <strong>cut scan times by up to 71%</strong> on large open-source codebases, with Apache Lucene (Java) showing the biggest improvement. Other projects, including Django (Python), Kafka, and Zulip, also saw <strong>performance boosts of over 50% in single-core mode</strong>. You can see the results for yourself below.</li>
</ul>
<p>For developers, these improvements mean quicker feedback in merge requests, less waiting on security results, and a smoother path to adoption. And with multi-core scanning and diff-based analysis layered on top, the gains will be even greater.</p>
<p><img alt="chart showing Python scan times" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760714805/rxl2zzo58j7y0k2ldxeq.png" />
&lt;p&gt;&lt;/p&gt;</p>
<p><img alt="chart showing Java scan times" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760714805/hz9bsrir6nrqthkjddvi.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<blockquote>
<p>These performance gains reflect GitLab‚Äôs broader focus on improving the developer experience across our platform. For example, one of our customers recently transitioned to GitLab‚Äôs <a href="https://docs.gitlab.com/user/application_security/policies/pipeline_execution_policies/">Pipeline Execution Policies</a> (PEP) to gain greater control and flexibility over how security scans run within their pipelines. By standardizing templates, adding caching, and optimizing pipeline logic, their teams cut dependency scan runtimes from <strong>15‚Äì60 minutes down to just 1‚Äì2 minutes per job ‚Äî saving roughly 100,000 compute minutes every day across 15,000 scans</strong>. It‚Äôs a clear example of how more customizable and efficient pipeline execution policies lead to faster feedback loops, higher productivity, and broader adoption.</p>
</blockquote>
<p>With these latest enhancements, Advanced SAST gives security and development teams the accuracy, speed, and flexibility they need to keep up with modern software development. By reducing false positives, enabling custom detection, and accelerating scan times, we‚Äôre making security an enabler ‚Äî not a blocker ‚Äî for developers.</p>
<p>Like all of <a href="https://about.gitlab.com/solutions/application-security-testing/">GitLab‚Äôs application security capabilities</a>, Advanced SAST is built directly into our DevSecOps platform, making security a natural part of how developers build, test, deploy, and secure software.</p>
<p>The result: faster adoption, fewer bottlenecks, and more secure applications delivered from the start.</p>
<blockquote>
<p>Get started with Advanced SAST today! Sign up for a <a href="https://about.gitlab.com/free-trial/">free trial of GitLab Ultimate</a>.</p>
</blockquote>
<h2>Learn more</h2>
<ul>
<li><a href="https://about.gitlab.com/blog/gitlab-advanced-sast-is-now-generally-available/">GitLab Advanced SAST is now generally available</a></li>
<li><a href="https://about.gitlab.com/blog/comprehensive-guide-to-gitlab-dast/">A comprehensive guide to GitLab DAST</a></li>
<li><a href="https://about.gitlab.com/solutions/application-security-testing/">GitLab Security Testing solutions</a></li>
</ul>
- [GitLab 18.5: Intelligence that moves software development forward](https://about.gitlab.com/blog/gitlab-18-5-intelligence-that-moves-software-development-forward/) ‚Äî <p>Software development teams are drowning in noise. Thousands of vulnerabilities flood security dashboards, but only a fraction pose real risk. Developers context-switch between planning backlogs, triaging security findings, reviewing code, and responding to CI/CD failures ‚Äî losing hours to manual work. <a href="https://about.gitlab.com/releases/2025/10/16/gitlab-18-5-released/">GitLab 18.5</a> calms this chaos.</p>
<p>At the heart of this release is a valuable improvement in overall usability of GitLab and how AI integrates into your user experience. A new panel-based UI makes it easier to see data in context, and allows GitLab Duo Chat to be persistently visible across the platform, wherever it is needed. Purpose-built agents tackle vulnerability triage and backlog management, and popular AI tools integrate with agentic workflows even more seamlessly than before. We‚Äôve also extended our market-leading security capabilities to help you better identify exploitable vulnerabilities versus theoretical ones, distinguish active credentials from expired ones, and scan only changed code to keep developers in flow.</p>
<h2>What‚Äôs new in 18.5</h2>
<p>18.5 represents our biggest release so far this year ‚Äî watch our introduction to the release, and read more details below.
&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128975773?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;GitLab_18.5 Release_101925_MP_v2&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<h3>Modern user experience with quick access to GitLab Duo everywhere</h3>
<p>GitLab 18.5 delivers a modernized user experience with a more intuitive interface driven by a new panel-based layout.</p>
<p>With panels, key information appears side by side so that you can work contextually, without losing your place. For example, when you click on an issue in the issues list, its details automatically open in a side panel. You can also launch the GitLab Duo panel on the right, bringing Duo wherever you are in GitLab. This lets you ask contextual questions or give instructions, right alongside your work.</p>
<p>Several usability improvements make navigation easier. The global search box now appears at the top center for improved accessibility. Global navigation elements, including Issues, Merge Requests, To-Dos, and your avatar have moved to the top right. Additionally, the left sidebar is now collapsible and expandable, giving you more control over your workspace.</p>
<p>Teams using experimental and GitLab Duo beta features will be the first to receive the new interface, followed by all GitLab.com users who will be able to turn this experience on using the toggle located under your user icon. To learn more about this feature, reference our documentation <a href="https://docs.gitlab.com/user/interface_redesign/#turn-new-navigation-on-or-off">here</a>. Please share your feedback or report any issues <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/577554">here</a>, you're helping us shape a better GitLab!</p>
<h3>Updates to GitLab Duo Agent Platform</h3>
<p><strong>Security Analyst Agent: Transform manual vulnerability triage into intelligent automation</strong></p>
<p>GitLab Duo <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/security_analyst_agent/">Security Analyst Agent</a> automates vulnerability management workflows through AI-powered analysis, helping transform hours of manual triage into intelligent automation. Building on the Vulnerability Management Tools available through GitLab Duo Agentic Chat, Security Analyst Agent orchestrates multiple tools, applying security policies, and creating custom flows for recurring workflows automatically.</p>
<p>Security teams can access enriched vulnerability data, including CVE details, static reachability analysis, and code flow information, while executing operations like dismissing false positives, confirming threats, adjusting severity levels, and creating linked issues for remediation ‚Äî all through conversational AI. The agent reduces repetitive clicking through vulnerability dashboards and replaces custom scripts with simple natural language commands.</p>
<p>For example, when a security scan reveals dozens of vulnerabilities, simply prompt: &quot;Dismiss vulnerabilities with reachable=FALSE and create issues for critical findings.&quot; Security Analyst Agent analyzes reachability data, applies security policies, and completes bulk operations in moments ‚Äî helping decrease work that would otherwise take hours.</p>
<p>While individual Vulnerability Management Tools can be accessed directly through Agentic Chat for specific tasks, Security Analyst Agent orchestrates these tools intelligently and automates complex multi-step workflows. Note that Vulnerability Management Tools are available through Agentic Chat on GitLab Self-managed and GitLab.com instances, and Security Analyst Agent is available on GitLab.com only for 18.5, while availability in Self-managed and Dedicated environments will come with our next release.
Watch this demo:</p>
<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128975984?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.5 Security Demo&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p><strong>GitLab Duo Planner: Turn backlog chaos into strategic clarity</strong></p>
<p>Managing complex software delivery requires constant context-switching between planning tasks. <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/">GitLab Duo Planner</a> addresses the real-world planning challenges we see teams face every day. Duo Planner acts as your teammate with awareness of your project context, including how you manage issues, epics, and merge requests. Unlike generic AI assistants, it's purpose-built with deep knowledge of GitLab's planning workflows coupled with Agile and prioritization frameworks to help you balance effort, risk, and strategic alignment.</p>
<p>GitLab Duo Planner can turn vague ideas into structured planning hierarchies, identify stale backlog items, and draft executive updates. For example, when refining your backlog with hundreds of issues accumulated over months, simply prompt: &quot;Identify stale backlog items and suggest priorities.&quot; Within seconds, you'll receive a structured summary showing issues without recent activity, items missing key details, duplicate work, and recommended priorities based on labels and milestones, complete with actionable recommendations.</p>
<p>For teams managing complex roadmaps, the Planner aims to eliminate hours of manual analysis and context-switching, helping Product Managers and engineering leads make faster, more informed decisions. As of 18.5, GitLab Duo Planner is currently ‚Äúread-only,‚Äù meaning that it can analyze, plan, and suggest, but cannot yet take direct action to modify anything. Please see our <a href="https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/">documentation</a> for more information.</p>
<p><strong>Extensible Agent Catalog: Popular AI tools as native GitLab agents</strong></p>
<p>GitLab 18.5 introduces popular AI agents directly into the <a href="https://docs.gitlab.com/user/duo_agent_platform/ai_catalog/">AI Catalog</a>, making external tools like Claude, OpenAI Codex, Google Gemini CLI, Amazon Q Developer, and OpenCode available as native GitLab agents. Users can now discover, configure, and deploy these agents through the same unified catalog interface used for GitLab's built-in agents, with automatic syncing of foundational agents across organization catalogs.</p>
<p>This eliminates the complexity of manual agent setup by providing a point-and-click catalog experience while maintaining enterprise-grade security through GitLab's authentication and audit systems. GitLab Duo Enterprise subscriptions now include built-in usage of Claude and Codex within GitLab, allowing you to use your existing GitLab subscription for these tools without requiring separate API keys or additional billing setup. Other agents may still require separate subscriptions and configuration while we finalize our integration plans.</p>
<p><strong>Self-hosted GitLab Duo Agent Platform (Beta): Address data sovereignty requirements without sacrificing AI power</strong></p>
<p>GitLab 18.5 moves GitLab Duo Agent Platform's self-hosted capabilities from experimental to beta, enabling organizations to execute AI agents and flows entirely within their own infrastructure ‚Äî critical for regulated industries and data sovereignty requirements. The beta release includes improved timeout configurations and AI Gateway settings, allowing teams to use AI agents for code reviews, bug fixes, and feature implementations, while providing enterprise-grade security for sensitive code.</p>
<h2>Smarter, faster security: Prioritize real risks and keep developers in the flow</h2>
<p>GitLab 18.5 introduces new application security capabilities that help teams focus on exploitable risk, reduce noise, and strengthen software supply chain security. These updates continue our commitment to building security directly into the development process ‚Äî delivering precision, speed, and insight without disrupting developer flow.</p>
<p><strong>Static Reachability Analysis</strong></p>
<p>With over <a href="https://www.cvedetails.com/">37,000 new CVEs</a> issued this year, security teams face an overwhelming volume of vulnerabilities and struggle to understand which ones are truly exploitable. Static Reachability Analysis, now in limited availability, brings library-level precision by helping to identify whether vulnerable code is actually invoked in your application, not just present in dependencies.</p>
<p>Paired with our <a href="https://docs.gitlab.com/user/application_security/vulnerabilities/risk_assessment_data/">recently released</a> Exploit Prediction Scoring System (EPSS) and Known Exploited Vulnerability (KEV) data, security teams can more effectively accelerate vulnerability triage and prioritize real risks to help strengthen overall supply chain security. In 18.5, we‚Äôre adding support for Java, alongside existing support for Python, JavaScript, and TypeScript.</p>
<p><strong>Secret Validity Checks</strong></p>
<p>Just as Static Reachability Analysis helps teams prioritize exploitable vulnerabilities from open source dependencies, Secret Validity Checks bring the same insight to exposed secrets ‚Äî currently available in beta on GitLab.com and GitLab Self-Managed. For GitLab-issued security tokens, instead of manually verifying whether a leaked credential or API key is active, GitLab automatically distinguishes active secrets from expired ones directly in the <a href="https://docs.gitlab.com/user/application_security/vulnerability_report/">Vulnerability Report</a>. This helps enable security and development teams to focus remediation efforts on genuine risks. Support for AWS- and GCP-issued secrets is planned for future releases.</p>
<p><strong>Custom rules for Advanced SAST</strong></p>
<p>Advanced SAST runs on rules informed by our in-house security research team, designed to maximize accuracy out of the box. However, some teams required additional flexibility to tune the SAST engine for their specific organization. With Custom Rules for Advanced SAST, AppSec teams can define atomic, pattern-based detection logic to help capture security issues specific to their organization ‚Äî like flagging banned function calls ‚Äî while still using GitLab‚Äôs curated ruleset as the baseline. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. While these rules will not support taint analysis, they do give organizations greater flexibility in achieving accurate SAST results.</p>
<p><strong>Advanced SAST C and C++ language support</strong></p>
<p>We‚Äôre expanding our language coverage for Advanced SAST to include C and C++, which are widely used languages in embedded systems software development. To enable scanning, projects must generate a compilation database that captures compiler commands and includes paths used during builds. This works to ensure the scanner can accurately parse and analyze source files, delivering precise, context-aware results that help security teams identify real vulnerabilities in the development process. The implementation requirements for C and C++ require specific configurations, which can be found in our <a href="https://docs.gitlab.com/user/application_security/sast/cpp_advanced_sast/">documentation</a>. Advanced SAST C and C++ support are currently available in beta.</p>
<p><strong>Diff-based SAST scanning</strong></p>
<p>Traditional SAST scans re-analyze entire codebases with every commit, slowing pipelines and disrupting developer flow. The developer experience is a critical consideration that can make or break the adoption of application security testing. Diff-based SAST scanning aims to speed up scan times by focusing only on the code changed in a merge request, reducing redundant analysis and surfacing relevant results tied to the developer‚Äôs work. By aligning scans with actual code changes, GitLab delivers faster, more focused feedback that helps keep developers in flow while maintaining strong security coverage.</p>
<h2>Simplify API configurations</h2>
<p>API-driven workflows offer power and flexibility, but they can also create unnecessary complexity for tasks that teams need to perform regularly. The new Maven Virtual Registry interface brings a UI layer to these operations.</p>
<h3>Maven Virtual Registry interface</h3>
<p>The new web-based interface for managing Maven Virtual Registries turns complex API configurations into visual simplicity, providing a more intuitive experience for package administrators and platform engineers.</p>
<p>Previously, teams configured and maintained virtual registries only through API calls, which made routine maintenance time-consuming and required specialized platform knowledge. The new interface removes that barrier, helping to make everyday tasks faster and easier.</p>
<p>With this update, you can now:</p>
<ul>
<li>Create virtual registries to simplify dependency configuration</li>
<li>Create and order upstreams to help improve performance and compliance</li>
<li>Browse and clear stale cache entries directly in the UI</li>
</ul>
<p>This visual experience helps reduce operational overhead and provides development teams with clearer insight into how dependencies are resolved, enabling them to make better decisions about build performance and security policies.</p>
<p>Watch a demo:</p>
<p>&lt;!-- blank line --&gt;
&lt;figure class=&quot;video_container&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/CiOZJPhAvaI?si=cYaoR_OIgqFKbyM2&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;
&lt;/figure&gt;
&lt;!-- blank line --&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>We invite enterprise customers to join the <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/543045">Maven Virtual Registry Beta program</a> and share feedback to help shape the final release.</p>
<h2>AI that adapts to your workflow</h2>
<p>This release represents more than new capabilities ‚Äî it's about choice and control. Watch the walkthrough video here:</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128992281?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.5-tech-demo&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>GitLab Premium and Ultimate users can start using these capabilities today on <a href="https://GitLab.com">GitLab.com</a> and self-managed environments, with availability for GitLab Dedicated customers planned for next month.</p>
<p>GitLab Duo Agent Platform is currently in <strong>beta</strong> ‚Äî enable beta and experimental features to experience how full-context AI can transform the way your teams build software. New to GitLab? <a href="https://about.gitlab.com/free-trial/devsecops/">Start your free trial</a> and see why the future of development is AI-powered, secure, and orchestrated through the world‚Äôs most comprehensive DevSecOps platform.</p>
<p><em><strong>Note:</strong> Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they will be made available with a paid add-on option for GitLab Duo Agent Platform.</em></p>
<h3>Stay up to date with GitLab</h3>
<p>To make sure you‚Äôre getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:</p>
<ul>
<li><a href="https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/">Upgrade Path Tool</a> ‚Äì enter your current version and see the exact upgrade steps for your instance</li>
<li><a href="https://docs.gitlab.com/update/upgrade_paths/">Upgrade Documentation</a> ‚Äì detailed guides for each supported version, including requirements, step-by-step instructions, and best practices</li>
</ul>
<p>By upgrading regularly, you‚Äôll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.</p>
<p>For organizations that want a hands-off approach, consider <a href="https://content.gitlab.com/viewer/d1fe944dddb06394e6187f0028f010ad#1">GitLab‚Äôs Managed Maintenance service</a>. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.</p>
<p><em>This blog post contains &quot;forward‚Äëlooking statements&quot; within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption &quot;Risk Factors&quot; in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.</em></p>
- [Variable and artifact sharing in GitLab parent-child pipelines](https://about.gitlab.com/blog/variable-and-artifact-sharing-in-gitlab-parent-child-pipelines/) ‚Äî <p>Software projects have different evolving needs and requirements. Some have
said that <em>software is never finished, merely abandoned</em>. Some software
projects are small and others are large with complex integrations. Some have
dependencies on external projects, while others are self-contained.
Regardless of the size and complexity, the need to validate and ensure
functionality remains paramount.</p>
<p>CI/CD pipelines can help with the challenge of building and validating software projects consistently, but, much like the software itself, these pipelines can become complex with many dependencies. This is where ideas like <a href="https://docs.gitlab.com/ci/pipelines/downstream_pipelines/#parent-child-pipelines">parent-child pipelines</a> and data exchange in CI/CD setups become incredibly important.</p>
<p>In this article, we will cover common CI/CD data exchange challenges users may encounter with parent-child pipelines in GitLab ‚Äî and how to solve them. You'll learn how to turn complex CI/CD processes into more manageable setups.</p>
<h2>Using parent-child pipelines</h2>
<p>The pipeline setup in the image below illustrates a scenario where a project could require a large, complex pipeline. The whole project resides in one repository and contains different modules. Each module requires its own set of build and test automation steps.</p>
<p>One approach to address the CI/CD configuration in a scenario like this is to break down the larger pipeline into smaller ones (i.e., child pipelines) and keep a common CI/CD process that is shared across all modules in charge of the whole orchestration (i.e., parent pipeline).</p>
<p><img alt="CI/CD configuration" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617772/hizwvhmgxn6exbmvsnrv.png" /></p>
<p>The parent-child pipeline pattern allows a single pipeline to orchestrate one or many downstream pipelines. Similar to how a single pipeline coordinates the execution of multiple <a href="https://docs.gitlab.com/ci/jobs/">jobs</a>, the parent pipeline coordinates the running of full pipelines with one or more jobs.</p>
<p>This pattern has been shown to be helpful in a variety of use cases:</p>
<ul>
<li>
<p>Breaking down large, complex pipelines into smaller, manageable pieces</p>
</li>
<li>
<p>Conditionally executing certain pipelines as part of a larger CI/CD process</p>
</li>
<li>
<p>Executing pipelines in parallel</p>
</li>
<li>
<p>Helping manage user permissions to access and run certain pipelines</p>
</li>
</ul>
<p>GitLab‚Äôs current CI/CD structure supports this pattern and makes it simple to implement parent-child pipelines. While there are many benefits when using the parent-child pipeline pattern with GitLab, one question we often get is how to share data between the parent and child pipelines. In the next sections, we‚Äôll go over how to make use of GitLab variables and artifacts to address this concern.</p>
<h3>Sharing variables</h3>
<p>There are cases where it is necessary to pass the output from a parent pipeline job to a child pipeline. These outputs can be shared as variables, <a href="https://docs.gitlab.com/ci/jobs/job_artifacts/">artifacts</a>, and <a href="https://docs.gitlab.com/ci/inputs/">inputs</a>.</p>
<p>Consider a case where we create a custom variable <code>var_1</code> during the runtime of a job:</p>
<pre><code>
stages:
  - build
  - triggers

# This job only creates a variable 

create_var_job:
  stage: build
  script:
    - var_1=&quot;Hi, I'm a Parent pipeline variable&quot;
    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env
  artifacts:
    reports:
      dotenv: var.env
</code></pre>
<p>Notice that the variable is created as part of the script steps in the job (during runtime). In this example, we are using a simple string <code>&quot;Hi, I'm a Parent pipeline variable&quot;</code> to illustrate the main syntax required to later share this variable with a child pipeline. Let's break down the <code>create_var_job</code>  and analyze the main steps from this GitLab job</p>
<p>First, we need to save <code>var_1</code> as <code>dotenv</code>:</p>
<pre><code>  script:
    - var_1=&quot;Hi, I'm a pipeline variable&quot;
    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env
</code></pre>
<p>After saving <code>var_1</code> as <code>var.env</code>, the next important step is to make this variable available as an artifact produced by the <code>create_var_job</code>. To do that, we use the following syntax:</p>
<pre><code>
artifacts:
    reports:
      dotenv: var.env
</code></pre>
<p>Up to this point, we have created a variable during runtime and saved it as a <code>dotenv</code> report. Now let's add the job that should trigger the child pipeline:</p>
<pre><code>
telco_service_a:
  stage: triggers
  trigger:
    include: service_a/.gitlab-ci.yml
  rules:
    - changes:
        - service_a/*
</code></pre>
<p>The goal of <code>telco_service_a</code>  job is to find the <code>.gitlab-ci.yml</code> configuration of the child pipeline,  which is defined in this case as <code>service_a,</code> and trigger its execution. Let's examine this job:</p>
<pre><code>
telco_service_a:
  stage: triggers
  trigger:
    include: service_a/.gitlab-ci.yml
</code></pre>
<p>We see it belongs to another <code>stage</code> of the pipeline named <code>triggers.</code>This job will run only after <code>create_var_job</code> from the first stage successfully finishes and where the variable  <code>var_1</code> we want to pass is created.</p>
<p>After defining the stage, we use the reserved words <code>trigger</code> and <code>include</code> to tell GitLab where to search for the child pipeline configuration, as illustrated in the YAML below:</p>
<pre><code>  trigger:
    include: service_a/.gitlab-ci.yml
</code></pre>
<p>Our child-pipeline YAML configuration is under <code>service_a/.gitlab-ci.yml</code> folder in the GitLab repository, for this example.</p>
<p><img alt="child-pipeline YAML configuration" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617772/ujkirpbifthpuujkcm6f.png" /></p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>&lt;center&gt;&lt;i&gt;Child pipelines folders with configurations&lt;/i&gt;&lt;/center&gt;</p>
<p>&lt;p&gt;&lt;/p&gt;</p>
<p>Take into consideration that the repository structure depicted above can vary. What matters is properly pointing the  <code>triggers: include</code> properties at the location of your child-pipeline configuration in your repository.</p>
<p>Finally, we use <code>rules: changes</code> to indicate to GitLab that this child pipeline should be triggered only if there is any change in any file in the <code>service_a/.gitlab-ci.yml</code> directory, as illustrated in the following code snippet:</p>
<pre><code>
rules:
    - changes:
        - service_a/*
</code></pre>
<p>Using this rule helps to optimize cost by triggering the child pipeline job only when necessary. This approach is particularly valuable in a monorepo architecture where specific modules contain numerous components, allowing us to avoid running their dedicated pipelines when no changes have been made to their respective codebases.</p>
<h4>Configuring the parent pipeline</h4>
<p>Up to this point, we have put together our parent pipeline. Here's the full code snippet for this segment:</p>
<pre><code>
# Parent Pipeline Configuration

# This pipeline creates a custom variable and triggers a child pipeline


stages:
  - build
  - trigger

create_var_job:
  stage: build
  script:
    - var_1=&quot;Hi, I'm a Parent pipeline variable&quot;
    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env
  artifacts:
    reports:
      dotenv: var.env

telco_service_a:
  stage: triggers
  trigger:
    include: service_a/.gitlab-ci.yml
  rules:
    - changes:
        - service_a/*
</code></pre>
<p>When GitLab executes the YAML configuration in the GitLab UI, the parent pipeline gets rendered as follows:</p>
<p><img alt="parent pipeline rendering" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617771/e1azkkr0rnzd42dzkw1x.png" /></p>
<p>Notice the label &quot;trigger job,&quot; which indicates this job will start the execution of another pipeline configuration.</p>
<h4>Configuring the child pipeline</h4>
<p>Moving forward, let's now focus on the child pipeline configuration, where we expect to inherit and print the value of the <code>var_1</code> created in the parent pipeline.</p>
<p>The pipeline configuration in <code>service_a/.gitlab_ci.yml</code> has the following definition:</p>
<pre><code>
stages:
  - build

build_a:
  stage: build
  script:
    - echo &quot;this job inherits the variable from the Parent pipeline:&quot;
    - echo $var_1
  needs:
    - project: gitlab-da/use-cases/7-4-parent-child-pipeline
      job: create_var_job
      ref: main
      artifacts: true
</code></pre>
<p>Like before, let's break down this pipeline and highlight the main parts to achieve our goal. This pipeline only contains one stage (i.e., <code>build)</code> and one job (i.e., <code>build_a)</code>. The script in the job contains two steps:</p>
<pre><code>
build_a:
  stage: build
  script:
    - echo &quot;this job inherits the variable from the Parent pipeline:&quot;
    - echo $var_1
</code></pre>
<p>These two steps print output during the execution. The most interesting one is the second step, <code>echo $var_1</code>, where we expect to print the variable value inherited from the parent pipeline. Remember, this was a simple string with value: <code>&quot;Hi, I'm a Parent pipeline variable.&quot;</code></p>
<h4>Inheriting variables using needs</h4>
<p>To set and link this job to inherit variables from the parent pipeline, we use the reserved GitLab CI properties <code>needs</code> as depicted in the following snippet:</p>
<pre><code>
needs:
    - project: gitlab-da/use-cases/7-4-parent-child-pipeline
      job: create_var_job
      ref: main
      artifacts: true
</code></pre>
<p>Using the &quot;needs&quot; keyword, we define dependencies that must be completed before running this job. In this case, we pass four different values. Let's walk through each one  of them:</p>
<ul>
<li>
<p><strong>Project:</strong> The complete namespace of the project where the main <code>gitlab-ci.yml</code> containing the parent pipeline YAML is located. Make sure to include the absolute path.</p>
</li>
<li>
<p><strong>Job:</strong> The specific job name in the parent pipeline from where we want to inherit the variable.</p>
</li>
<li>
<p><strong>Ref:</strong> The name of the branch where the main <code>gitlab-ci.yml</code> containing the parent pipeline YAML is located.</p>
</li>
<li>
<p><strong>Artifacts:</strong> Where we set a boolean value, indicating that artifacts from the parent pipeline job should be downloaded and made available to this child pipeline job.</p>
</li>
</ul>
<p><strong>Note:</strong> This specific approach using the needs property is only available to GitLab Premium and Ultimate users. We will cover another example for GitLab community users later on.</p>
<h4>Putting it all together</h4>
<p>Now let's assume we make a change to any of the files under <code>service_a</code> folder and commit the changes to the repository. When GitLab detects the change, the rule we set up will trigger the child job pipeline execution. This gets displayed in the GitLab UI as follows:</p>
<p><img alt="Rule triggering the child job pipeline execution" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617771/e1azkkr0rnzd42dzkw1x.png" /></p>
<p>Clicking on the <code>telco_service_a</code>  will take us to the jobs in the child pipeline:</p>
<p><img alt="Jobs in pipeline" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617773/vftjkg7ct2wqmew1e3yk.png" /></p>
<p>We can see the parent-child relationship, and finally, by clicking on the <code>build_a job</code>, we can visually verify the variable inheritance in the job execution log:</p>
<p><img alt="Verifying the variable inheritance in the job execution log" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617758/hxfkfmev9hebbqhgcvoh.png" /></p>
<p>This output confirms the behavior we expected. The custom runtime variable <code>var_1</code> created in the parent job is inherited in the child job, unpacked from the <code>dotenv</code> report, and its value accessible as can be confirmed in Line 26 above.</p>
<p>This use case illustrates how to share custom variables that can contain any value between pipelines. This example is intentionally simple and can be extrapolated to more realistic scenarios. Take, for instance, the following CI/CD configuration, where the custom variable we need to share is the tag of a Docker image:</p>
<pre><code>
# Pipeline 


build-prod-image:
  tags: [ saas-linux-large-amd64 ]
  image: docker:20.10.16
  stage: build
  services:
    - docker:20.10.16-dind
  
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -t $PRODUCTION_IMAGE .
    - docker push $PRODUCTION_IMAGE
    - echo &quot;UPSTREAM_CONTAINER_IMAGE=$PRODUCTION_IMAGE&quot; &gt;&gt; prodimage.env

  artifacts:
    reports:
      dotenv: prodimage.env

  rules:
      - if: '$CI_COMMIT_BRANCH == &quot;main&quot;'
        when: always
      - when: never
</code></pre>
<p>And use the variable with the Docker image tag, in another job that updates a Helm manifest file:</p>
<pre><code>
update-helm-values:
    stage: update-manifests
    image:
        name: alpine:3.16
        entrypoint: [&quot;&quot;]
  
    before_script:
          - apk add --no-cache git curl bash yq
          - git remote set-url origin https://${CI_USERNAME}:${GITOPS_USER}@${SERVER_PATH}/${PROJECT_PATH}
          - git config --global user.email &quot;gitlab@gitlab.com&quot;
          - git config --global user.name &quot;GitLab GitOps&quot;
          - git pull origin main
    script:
          - cd src
          - echo $UPSTREAM_CONTAINER_IMAGE
          - yq eval -i &quot;.spec.template.spec.containers[0].image |= \&quot;$UPSTREAM_CONTAINER_IMAGE\&quot;&quot; store-deployment.yaml
          - cat store-deployment.yaml
          - git pull origin main
          - git checkout -B main
          - git commit -am '[skip ci] prod image update'
          - git push origin main
    needs:
      - project: gitlab-da/use-cases/devsecops-platform/simply-find/simply-find-front-end
        job: build-prod-image
        ref: main
        artifacts: true
</code></pre>
<p>Mastering how to share variables between pipelines while maintaining the relationship between them enables us to create more sophisticated workflow orchestration that can meet our software building needs.</p>
<h3>Using GitLab Package Registry to share artifacts</h3>
<p>While the needs feature mentioned above works great for Premium and Ultimate users, GitLab also has features to help achieve similar results for Community Edition users. One suggested approach is to store artifacts in the <a href="https://docs.gitlab.com/user/packages/package_registry/">GitLab Package Registry</a>.</p>
<p>Using a combination of the variables provided in GitLab CI/CD jobs and the GitLab API, you can upload artifacts to the GitLab Package Registry from a parent pipeline. In the child pipeline, you can then access the uploaded artifact from the package registry using the same variables and API to access the artifact. Let‚Äôs take a look at the example pipeline and some supplementary scripts that illustrate this:</p>
<p><strong>gitlab-ci.yml (parent pipeline)</strong></p>
<pre><code>
# Parent Pipeline Configuration

# This pipeline creates an artifact, uploads it to Package Registry, and triggers a child pipeline


stages:
  - create-upload
  - trigger

variables:
  PACKAGE_NAME: &quot;pipeline-artifacts&quot;
  PACKAGE_VERSION: &quot;$CI_PIPELINE_ID&quot;
  ARTIFACT_FILE: &quot;artifact.txt&quot;

# Job 1: Create and upload artifact to Package Registry

create-and-upload-artifact:
  stage: create-upload
  image: alpine:latest
  before_script:
    - apk add --no-cache curl bash
  script:
    - bash scripts/create-artifact.sh
    - bash scripts/upload-to-registry.sh
  rules:
    - if: $CI_PIPELINE_SOURCE == &quot;push&quot;

# Job 2: Trigger child pipeline

trigger-child:
  stage: trigger
  trigger:
    include: child-pipeline.yml
    strategy: depend
  variables:
    PARENT_PIPELINE_ID: $CI_PIPELINE_ID
    PACKAGE_NAME: $PACKAGE_NAME
    PACKAGE_VERSION: $PACKAGE_VERSION
    ARTIFACT_FILE: $ARTIFACT_FILE
  rules:
    - if: $CI_PIPELINE_SOURCE == &quot;push&quot;
</code></pre>
<p><strong>child-pipeline.yml</strong></p>
<pre><code>
# Child Pipeline Configuration

# This pipeline downloads the artifact from Package Registry and processes it


stages:
  - download-process

variables:
  # These variables are passed from the parent pipeline
  PACKAGE_NAME: &quot;pipeline-artifacts&quot;
  PACKAGE_VERSION: &quot;$PARENT_PIPELINE_ID&quot;
  ARTIFACT_FILE: &quot;artifact.txt&quot;

# Job 1: Download and process artifact from Package Registry

download-and-process-artifact:
  stage: download-process
  image: alpine:latest
  before_script:
    - apk add --no-cache curl bash
  script:
    - bash scripts/download-from-registry.sh
    - echo &quot;Processing downloaded artifact...&quot;
    - cat $ARTIFACT_FILE
    - echo &quot;Artifact processed successfully!&quot;
</code></pre>
<p><strong>upload-to-registry.sh</strong></p>
<pre><code>
#!/bin/bash


set -e


# Configuration

PACKAGE_NAME=&quot;${PACKAGE_NAME:-pipeline-artifacts}&quot;

PACKAGE_VERSION=&quot;${PACKAGE_VERSION:-$CI_PIPELINE_ID}&quot;

ARTIFACT_FILE=&quot;${ARTIFACT_FILE:-artifact.txt}&quot;


# Validate required variables

if [ -z &quot;$CI_PROJECT_ID&quot; ]; then
    echo &quot;Error: CI_PROJECT_ID is not set&quot;
    exit 1
fi


if [ -z &quot;$CI_JOB_TOKEN&quot; ]; then
    echo &quot;Error: CI_JOB_TOKEN is not set&quot;
    exit 1
fi


if [ -z &quot;$CI_API_V4_URL&quot; ]; then
    echo &quot;Error: CI_API_V4_URL is not set&quot;
    exit 1
fi


if [ ! -f &quot;$ARTIFACT_FILE&quot; ]; then
    echo &quot;Error: Artifact file '$ARTIFACT_FILE' not found&quot;
    exit 1
fi


# Construct the upload URL

UPLOAD_URL=&quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}&quot;


# Upload the file using curl

response=$(curl -w &quot;%{http_code}&quot; -o /tmp/upload_response.json \
    --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; \
    --upload-file &quot;$ARTIFACT_FILE&quot; \
    &quot;$UPLOAD_URL&quot;)

if [ &quot;$response&quot; -eq 201 ]; then
    echo &quot;Upload successful!&quot;
else
    echo &quot;Upload failed with HTTP code: $response&quot;
    exit 1
fi

</code></pre>
<p><strong>download-from-regsitry.sh</strong></p>
<pre><code>
#!/bin/bash


set -e


# Configuration

PACKAGE_NAME=&quot;${PACKAGE_NAME:-pipeline-artifacts}&quot;

PACKAGE_VERSION=&quot;${PACKAGE_VERSION:-$PARENT_PIPELINE_ID}&quot;

ARTIFACT_FILE=&quot;${ARTIFACT_FILE:-artifact.txt}&quot;


# Validate required variables

if [ -z &quot;$CI_PROJECT_ID&quot; ]; then
    echo &quot;Error: CI_PROJECT_ID is not set&quot;
    exit 1
fi


if [ -z &quot;$CI_JOB_TOKEN&quot; ]; then
    echo &quot;Error: CI_JOB_TOKEN is not set&quot;
    exit 1
fi


if [ -z &quot;$CI_API_V4_URL&quot; ]; then
    echo &quot;Error: CI_API_V4_URL is not set&quot;
    exit 1
fi


if [ -z &quot;$PACKAGE_VERSION&quot; ]; then
    echo &quot;Error: PACKAGE_VERSION is not set&quot;
    exit 1
fi


# Construct the download URL

DOWNLOAD_URL=&quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}&quot;


# Download the file using curl

response=$(curl -w &quot;%{http_code}&quot; -o &quot;$ARTIFACT_FILE&quot; \
    --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; \
    --fail-with-body \
    &quot;$DOWNLOAD_URL&quot;)

if [ &quot;$response&quot; -eq 200 ]; then
    echo &quot;Download successful!&quot;
else
    echo &quot;Download failed with HTTP code: $response&quot;
    exit 1
fi

</code></pre>
<p>In this example, the parent pipeline uploads a file to the GitLab Package Registry by calling a script named <code>upload-to-registry.sh</code>. The script gives the artifact a name and version and constructs the API call to upload the file to the package registry. The parent pipeline is able to authenticate using a <code>$CI_JOB_TOKEN</code> to push the artifact.txt file to the registry.</p>
<p>The child pipeline operates the same as the parent pipeline by using a script to construct the API call to download the artifact.txt file from the package registry. It also is able to authenticate to the registry using the <code>$CI_JOB_TOKEN</code>.</p>
<p>Since the GitLab Package Registry is available to all GitLab users, it helps to serve as a central location for storing and versioning artifacts. It is a great option for users working with many kinds of artifacts and needing to version artifacts for workflows even beyond CI/CD.</p>
<h3>Using inputs to pass variables to a child pipeline</h3>
<p>If you made it this far in this tutorial, and you have plans to start creating new pipeline configurations, you might want to start by evaluating if your use case can benefit from using <strong>inputs</strong> to pass variables to other pipelines.</p>
<p>Using inputs is a recommended way to pass variables when you need to define specific values in a CI/CD job and have those values remain fixed during the pipeline run. Inputs might offer certain advantages over the method we implemented before. For example, with inputs, you can include data validation through options (i.e., values must be one of these: [‚Äòstaging', ‚Äòprod‚Äô]), variable descriptions, type checking, and assign default values before the pipeline run.</p>
<h4>Configuring CI/CD inputs</h4>
<p>Consider the following parent pipeline configuration:</p>
<pre><code>
# .gitlab-ci.yml (main file)

stages:
  - trigger

trigger-staging:
  stage: trigger
  trigger:
    include:
      - local: service_a/.gitlab-ci.yml
        inputs:
          environment: staging
          version: &quot;1.0.0&quot;
</code></pre>
<p>Let's zoom in at the main difference between the code snippet above and the previous parent pipeline examples in this tutorial:</p>
<pre><code>
trigger:
    include:
      - local: service_a/.gitlab-ci.yml
        inputs:
          environment: staging
          version: &quot;1.0.0&quot;
</code></pre>
<p>The main difference is using the reserved word &quot;inputs&quot;. This part of the YAML configuration can be read in natural language as: ‚Äútrigger the child pipeline defined in <code>service_a.gitlab-ci.yml</code> and make sure to pass ‚Äòenvironment: staging‚Äô and ‚Äòversion:1.0.0‚Äô as input variables that the child pipeline will know how to use.</p>
<h4>Reading CI/CD inputs in child pipelines</h4>
<p>Moving to the child pipeline, it must contain in its declaration a spec that defines the inputs it can take. For each input, it is possible to add a little description, a set of predefined options the input value can take, and the type of value it will take. This is illustrated as follows:</p>
<pre><code>
# target pipeline or child-pipeline in this case


spec:
  inputs:
    environment:
      description: &quot;Deployment environment&quot;
      options: [staging, production]
    version:
      type: string
      description: &quot;Application version&quot;


---


stages:
  - deploy
# Jobs that will use the inputs

deploy:
  stage: deploy
  script:
      -  echo &quot;Deploying version $[[ inputs.version ]] to $[[ inputs.environment ]]&quot;

</code></pre>
<p>Notice from the code snippet that after defining the spec, there is a YAML document separator &quot;---&quot;  followed by the actual child pipeline definition where we access the variables <code>$[[ inputs.version ]]</code> and <code>$[[ inputs.environment ]]&quot;</code> from the defined inputs using input interpolation.</p>
<h2>Get hands-on with parent-child pipelines, artifacts, and more</h2>
<p>We hope this article has helped with navigating the challenge of sharing variables and artifacts in parent-child pipeline setups.</p>
<p>To try these examples for yourself, feel free to view or fork the <a href="https://gitlab.com/gitlab-da/use-cases/devsecops-platform/devops-platform-wave/scenarios/scenario7-deep-dive-into-build-automation-and-ci/7-4-parent-child-pipeline/-/tree/main">Premium/Ultimate</a> and the <a href="https://gitlab.com/gitlab-da/playground/dhelfand/parent-child-pipeline-with-package-registry-artifacts">GitLab Package Registry</a> examples of sharing artifacts.</p>
<p>You can also sign up for a <a href="https://about.gitlab.com/free-trial/">30-day free trial of GitLab Ultimate</a> to experience all the features GitLab has to offer. Thanks for reading!</p>
- [How we built a structured Streamlit Application Framework in Snowflake](https://about.gitlab.com/blog/how-we-built-a-structured-streamlit-application-framework-in-snowflake/) ‚Äî <p>Recently, the GitLab Data team transformed scattered <a href="https://streamlit.io/">Streamlit</a> applications into a unified, secure, and scalable solution for our Snowflake environment. To accomplish this, we packed Python, Snowflake, and Streamlit together with GitLab. Follow along on this journey and discover the results we achieved, and learn how you can, too.</p>
<h2>The challenge</h2>
<p>Imagine this scenario: Your organization has dozens of Streamlit applications across different environments, running various Python versions, connecting to sensitive data with inconsistent security practices. Some apps work, others break mysteriously, and nobody knows who built what or how to maintain them.</p>
<p>This was exactly the challenge our data team faced. Applications were being created in isolation, with no standardization, no security oversight, and no clear deployment process. The result? A compliance nightmare and a maintenance burden that was growing exponentially.</p>
<p><img alt="Functional architectural design (high level)" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/i50lpkrwy9bok056rdak.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Functional architectural design (high level)&lt;/i&gt;&lt;/center&gt;</p>
<h2>How we started</h2>
<p>We leveraged our unique position as customer zero by building this entire framework on GitLab's own CI/CD infrastructure and project management tools. Here are the ingredients we started with:</p>
<ol>
<li><a href="https://about.gitlab.com/platform/">GitLab</a> (product)</li>
<li><a href="https://about.gitlab.com/platform/">Snowflake</a> - our single source of truth (SSOT) for the data warehouse activities (and more than that)</li>
<li><a href="https://streamlit.io/">Streamlit</a> - an open-source tool for visual applications that has pure Python code under the hood
This provided us with immediate access to enterprise-grade DevSecOps capabilities, enabling us to implement automated testing, code review processes, and deployment pipelines from the outset. By utilizing GitLab's built-in features for issue tracking, merge requests, and automated deployments (CI/CD pipelines), we can iterate rapidly and validate the framework against real-world enterprise requirements. This internal-first approach ensured our solution was battle-tested on GitLab's own infrastructure before any external implementation.</li>
</ol>
<h3>The lessons we learned</h3>
<p>The most critical lesson we learned from building the Streamlit Application Framework in Snowflake is that <strong>structure beats chaos every time</strong> ‚Äî implement governance early rather than retrofitting it later when maintenance becomes exponential.
You also need to clearly define roles and responsibilities, separating infrastructure concerns from application development, so that each team can focus on its strengths.
Security and compliance cannot be afterthoughts; they must be built into templates and automated processes from day one, as it's far easier to enforce consistent standards upfront than to force them after the fact. Invest heavily in automation and CI/CD pipelines, as manual processes don't scale and introduce human error.
<img alt="Architecture of the framework (general overview)" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035998/qt9gfemxjnj8kjumkuh7.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Architecture of the framework (general overview)&lt;/i&gt;&lt;/center&gt;</p>
<h2>How the Streamlit Application Framework changes everything</h2>
<p>The Streamlit Application Framework turns a scattered approach into a structure. It gives developers freedom within secure guardrails, while automating deployment and eliminating maintenance complexity.</p>
<h3>Three clear roles, one unified process</h3>
<p>The framework introduces a structured approach with three distinct roles:</p>
<ol>
<li><strong>Maintainers</strong> (Data team members and contributors) handle the infrastructure, including CI/CD pipelines, security templates, and compliance rules. They ensure the framework runs smoothly and stays secure.</li>
<li><strong>Creators</strong> (those who need to build applications) can focus on what they do best: creating visualizations, connecting to Snowflake data, and building user experiences. They have full flexibility to create new applications from scratch, add new pages to existing apps, integrate additional Python libraries, and build complex data visualisations ‚Äî all without worrying about deployment pipelines or security configurations.</li>
<li><strong>Viewers</strong> (end users) access polished, secure applications without any technical overhead. All they need is Snowflake access.
<img alt="Roles overview and their functionality" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/oatqyx3ug7vsgzishpma.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Overview of roles and their functions&lt;/i&gt;&lt;/center&gt;</li>
</ol>
<h2>Automate everything</h2>
<p>By implementing CI/CD, days of manual deployments and configuration headaches are gone. The framework provides:</p>
<ul>
<li><strong>One-click environment preparation:</strong> With a set of <code>make</code> commands, the environment is installed and ready in a few seconds.</li>
</ul>
<pre><code class="language-yaml">================================================================================
‚úÖ Snowflake CLI successfully installed and configured!
Connection: gitlab_streamlit
User: YOU@GITLAB.COM
Account: gitlab
================================================================================
Using virtualenv: /Users/YOU/repos/streamlit/.venv
üìö Installing project dependencies...
Installing dependencies from lock file
No dependencies to install or update
‚úÖ Streamlit environment prepared!
</code></pre>
<ul>
<li><strong>Automated CI/CD pipelines:</strong> Handle testing, code review, and deployment from development to production.</li>
<li><strong>Secure sandbox environments:</strong> Provide for safe development and testing before production deployment.</li>
</ul>
<pre><code class="language-yaml">‚ï∞‚îÄ$ make streamlit-rules
üîç Running Streamlit compliance check...
================================================================================
CODE COMPLIANCE REPORT
================================================================================
Generated: 2025-07-09 14:01:16
Files checked: 1

SUMMARY:
‚úÖ Passed: 1
‚ùå Failed: 0
Success Rate: 100.0%

APPLICATION COMPLIANCE SUMMARY:
üì± Total Applications Checked: 1
‚ö†Ô∏è Applications with Issues: 0
üìä File Compliance Rate: 100.0%

DETAILED RESULTS BY APPLICATION:
...
</code></pre>
<ul>
<li><strong>Template-based application creation:</strong> Ensures consistency across all applications and pages.</li>
</ul>
<pre><code class="language-yaml">‚ï∞‚îÄ$ make streamlit-new-page STREAMLIT_APP=sales_dashboard STREAMLIT_PAGE_NAME=analytics
üìù Generating new Streamlit page: analytics for app: sales_dashboard
üìÉ Create new page from template:
Page name: analytics
App directory: sales_dashboard
Template path: page_template.py
‚úÖ Successfully created 'analytics.py' in 'sales_dashboard' directory from template
</code></pre>
<ul>
<li><strong>Poetry-based dependency management:</strong> Prevents version conflicts and maintains clean environments.</li>
<li><strong>Organized project structure:</strong> Has dedicated folders for applications, templates, compliance rules, and configuration management.</li>
</ul>
<pre><code class="language-yaml">‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ applications/     # Folder for Streamlit applications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main_app/     # Main dashboard application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/   # Shared components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ &lt;your_apps&gt;/  # Your custom application
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ &lt;your_apps2&gt;/ # Your 2nd custom application
‚îÇ   ‚îú‚îÄ‚îÄ templates/        # Application and page templates
‚îÇ   ‚îú‚îÄ‚îÄ compliance/       # Compliance rules and checks
‚îÇ   ‚îî‚îÄ‚îÄ setup/            # Setup and configuration utilities
‚îú‚îÄ‚îÄ tests/                # Test files
‚îú‚îÄ‚îÄ config.yml            # Environment configuration
‚îú‚îÄ‚îÄ Makefile              # Build and deployment automation
‚îî‚îÄ‚îÄ README.md             # Main README.md file
</code></pre>
<ul>
<li><strong>Streamlined workflow:</strong> Takes local development through testing schema to production, all automated through GitLab CI/CD pipelines.</li>
</ul>
<p><img alt="GitLab CI/CD pipelines for full automation of the process" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035998/usyma2jkgiazu9iay1au.png" />
&lt;p&gt;&lt;/p&gt; &lt;center&gt;&lt;i&gt;GitLab CI/CD pipelines for full automation of the process&lt;/i&gt;&lt;/center&gt;</p>
<h2>Security and compliance by design</h2>
<p>Instead of bolting on security as an afterthought, the structured Streamlit Application Framework builds it in from the ground up. Every application adheres to the same security standards, and compliance requirements are automatically enforced. Audit trails are maintained throughout the development lifecycle.
We introduce our compliance rules and verify them with a single command. For instance, we can list which classes and methods are mandatory to use, which files you should have, and which roles are allowed and which are forbidden to share the application with. The rules are flexible and descriptive; all you need to do is define them in a YAML file:</p>
<pre><code class="language-yaml">class_rules:
  - name: &quot;Inherit code for the page from GitLabDataStreamlitInit&quot;
    description: &quot;All Streamlit apps must inherit from GitLabDataStreamlitInit&quot;
    severity: &quot;error&quot;
    required: true
    class_name: &quot;*&quot;
    required_base_classes:
      - &quot;GitLabDataStreamlitInit&quot;
    required_methods:
      - &quot;__init__&quot;
      - &quot;set_page_layout&quot;
      - &quot;setup_ui&quot;
      - &quot;run&quot;

function_rules:
  - name: &quot;Main function required&quot;
    description: &quot;Must have a main() function&quot;
    severity: &quot;error&quot;
    required: true
    function_name: &quot;main&quot;

import_rules:
  - name: &quot;Import GitLabDataStreamlitInit&quot;
    description: &quot;Must import the mandatory base class&quot;
    severity: &quot;error&quot;
    required: true
    module_name: &quot;gitlab_data_streamlit_init&quot;
    required_items:
      - &quot;GitLabDataStreamlitInit&quot;
  - name: &quot;Import streamlit&quot;
    description: &quot;Must import streamlit library&quot;
    severity: &quot;error&quot;
    required: true
    module_name: &quot;streamlit&quot;

file_rules:
  - name: &quot;Snowflake configuration required (snowflake.yml)&quot;
    description: &quot;Each application must have a snowflake.yml configuration file&quot;
    severity: &quot;error&quot;
    required: true
    file_pattern: &quot;**/applications/**/snowflake.yml&quot;
    base_path: &quot;&quot;
  - name: &quot;Snowflake environment required (environment.yml)&quot;
    description: &quot;Each application must have a environment.yml configuration file&quot;
    severity: &quot;error&quot;
    required: true
    file_pattern: &quot;**/applications/**/environment.yml&quot;
    base_path: &quot;&quot;
  - name: &quot;Share specification required (share.yml)&quot;
    description: &quot;Each application must have a share.yml file&quot;
    severity: &quot;warning&quot;
    required: true
    file_pattern: &quot;**/applications/**/share.yml&quot;
    base_path: &quot;&quot;
  - name: &quot;README.md required (README.md)&quot;
    description: &quot;Each application should have a README.md file with a proper documentation&quot;
    severity: &quot;error&quot;
    required: true
    file_pattern: &quot;**/applications/**/README.md&quot;
    base_path: &quot;&quot;
  - name: &quot;Starting point recommended (dashboard.py)&quot;
    description: &quot;Each application must have a dashboard.py as a starting point&quot;
    severity: &quot;warning&quot;
    required: true
    file_pattern: &quot;**/applications/**/dashboard.py&quot;
    base_path: &quot;&quot;

sql_rules:
  - name: &quot;SQL files must contain only SELECT statements&quot;
    description: &quot;SQL files and SQL code in other files should only contain SELECT statements for data safety&quot;
    severity: &quot;error&quot;
    required: true
    file_extensions: [&quot;.sql&quot;, &quot;.py&quot;]
    select_only: true
    forbidden_statements:
      - ....
    case_sensitive: false
  - name: &quot;SQL queries should include proper SELECT statements&quot;
    description: &quot;When SQL is present, it should contain proper SELECT statements&quot;
    severity: &quot;warning&quot;
    required: false
    file_extensions: [&quot;.sql&quot;, &quot;.py&quot;]
    required_statements:
      - &quot;SELECT&quot;
    case_sensitive: false

share_rules:
  - name: &quot;Valid functional roles in share.yml&quot;
    description: &quot;Share.yml files must contain only valid functional roles from the approved list&quot;
    severity: &quot;error&quot;
    required: true
    file_pattern: &quot;**/applications/**/share.yml&quot;
    valid_roles:
      - ...
    safe_data_roles:
      - ...
  - name: &quot;Share.yml file format validation&quot;
    description: &quot;Share.yml files must follow the correct YAML format structure&quot;
    severity: &quot;error&quot;
    required: true
    file_pattern: &quot;**/applications/**/share.yml&quot;
    required_keys:
      - &quot;share&quot;
    min_roles: 1
    max_roles: 10

</code></pre>
<p>With one command running:</p>
<pre><code class="language-bash">‚ï∞‚îÄ$ make streamlit-rules
</code></pre>
<p>We can verify all the rules we have created and validate that the developers (who are building a Streamlit application) are following the policy specified by the creators (who determine the policies and building blocks of the framework), and that all the building blocks are in the right place. This ensures consistent behavior across all Streamlit applications.</p>
<pre><code class="language-yaml">üîç Running Streamlit compliance check...
================================================================================
CODE COMPLIANCE REPORT
================================================================================
Generated: 2025-08-18 17:05:12
Files checked: 4

SUMMARY:
‚úÖ Passed: 4
‚ùå Failed: 0
Success Rate: 100.0%

APPLICATION COMPLIANCE SUMMARY:
üì± Total Applications Checked: 1
‚ö†Ô∏è Applications with Issues: 0
üìä File Compliance Rate: 100.0%

DETAILED RESULTS BY APPLICATION:
================================================================================
‚úÖ PASS APPLICATION: main_app
------------------------------------------------------------
üìÅ FILES ANALYZED (4):
‚úÖ dashboard.py
üì¶ Classes: SnowflakeConnectionTester
üîß Functions: main
üì• Imports: os, pwd, gitlab_data_streamlit_init, snowflake.snowpark.exceptions, streamlit

‚úÖ show_streamlit_apps.py
üì¶ Classes: ShowStreamlitApps
üîß Functions: main
üì• Imports: pandas, gitlab_data_streamlit_init, snowflake_session, streamlit

‚úÖ available_packages.py
üì¶ Classes: AvailablePackages
üîß Functions: main
üì• Imports: pandas, gitlab_data_streamlit_init, streamlit

‚úÖ share.yml
üë• Share Roles: snowflake_analyst_safe

üìÑ FILE COMPLIANCE FOR MAIN_APP:
‚úÖ Required files found:
‚úì snowflake.yml
‚úì environment.yml
‚úì share.yml
‚úì README.md
‚úì dashboard.py

RULES CHECKED:
----------------------------------------
Class Rules (1):
- Inherit code for the page from GitLabDataStreamlitInit (error)

Function Rules (1):
- Main function required (error)

Import Rules (2):
- Import GitLabDataStreamlitInit (error)
- Import streamlit (error)

File Rules (5):
- Snowflake configuration required (snowflake.yml) (error)
- Snowflake environment required (environment.yml) (error)
- Share specification required (share.yml) (warning)
- README.md required (README.md) (error)
- Starting point recommended (dashboard.py) (warning)

SQL Rules (2):
- SQL files must contain only SELECT statements (error)
üóÑ SELECT-only mode enabled
üö® Forbidden: INSERT, UPDATE, DELETE, DROP, ALTER...
- SQL queries should include proper SELECT statements (warning)

Share Rules (2):
- Valid functional roles in share.yml (error)
üë• Valid roles: 15 roles defined
üîí Safe data roles: 11 roles
- Share.yml file format validation (error)
------------------------------------------------------------
‚úÖ Compliance check passed
-----------------------------------------------------------
</code></pre>
<h2>Developer experience that works</h2>
<p>Whether you prefer your favorite IDE, a web-based development environment, or Snowflake Snowsight, the experience remains consistent. The framework provides:</p>
<ul>
<li><strong>Template-driven development:</strong> New applications and pages are created through standardized templates, ensuring consistency and best practices from day one. No more scattered design and elements.</li>
</ul>
<pre><code class="language-yaml">‚ï∞‚îÄ$ make streamlit-new-app NAME=sales_dashboard
üîß Configuration Environment: TEST
üìù Configuration File: config.yml
üìú Config Loader Script: ./setup/get_config.sh
üêç Python Version: 3.12
üìÅ Applications Directory: ./src/applications
üóÑ Database: ...
üìä Schema: ...
üèó Stage: ...
üè≠ Warehouse: ...
üÜï Creating new Streamlit app: sales_dashboard
Initialized the new project in ./src/applications/sales_dashboard
</code></pre>
<ul>
<li><strong>Poetry package management:</strong> All dependencies are managed through Poetry, creating isolated environments that won't disrupt your existing Python setup.</li>
</ul>
<pre><code class="language-toml">[tool.poetry]
name = &quot;GitLab Data Streamlit&quot;
version = &quot;0.1.1&quot;
description = &quot;GitLab Data Team Streamlit project&quot;
authors = [&quot;GitLab Data Team &lt;*****@gitlab.com&gt;&quot;]
readme = &quot;README.md&quot;

[tool.poetry.dependencies]
python = &quot;&lt;3.13,&gt;=3.12&quot;
snowflake-snowpark-python = &quot;==1.32.0&quot;
snowflake-connector-python = {extras = [&quot;development&quot;, &quot;pandas&quot;, &quot;secure-local-storage&quot;], version = &quot;^3.15.0&quot;}
streamlit = &quot;==1.22.0&quot;
watchdog = &quot;^6.0.0&quot;
types-toml = &quot;^0.10.8.20240310&quot;
pytest = &quot;==7.0.0&quot;
black = &quot;==25.1.0&quot;
importlib-metadata = &quot;==4.13.0&quot;
pyyaml = &quot;==6.0.2&quot;
python-qualiter = &quot;*&quot;
ruff = &quot;^0.1.0&quot;
types-pyyaml = &quot;^6.0.12.20250516&quot;
jinja2 = &quot;==3.1.6&quot;

[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<ul>
<li><strong>Multi-page application support:</strong> Creators can easily build complex applications with multiple pages and add new libraries as needed. Multi-page applications are part of the framework and a developer is focusing on the logic, not the design and structuring.</li>
</ul>
<p><img alt="Multipage application example (in Snowflake)" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/at1q2xgmjthkrgju4okm.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Multipage application example (in Snowflake)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;</p>
<ul>
<li><strong>Seamless Snowflake integration:</strong> Built-in connectors and authentication handling for secure data access provide the same experience, whether in local development or directly in Snowflake.</li>
</ul>
<pre><code class="language-yaml">make streamlit-push-test APPLICATION_NAME=sales_dashboard
üì§ Deploying Streamlit app to test environment: sales_dashboard
...
------------------------------------------------------------------------------------------------------------
üîó Running share command for application: sales_dashboard
Running commands to grant shares
üöÄ Executing: snow streamlit share sales_dashboard with SOME_NICE_ROLE
‚úÖ Command executed successfully
üìä Execution Summary: 1/1 commands succeeded
</code></pre>
<ul>
<li>
<p><strong>Comprehensive Makefile:</strong> All common commands are wrapped in simple Makefile commands, from local development to testing and deployment, including CI/CD pipelines.</p>
</li>
<li>
<p><strong>Safe local development:</strong> Everything runs in isolated Poetry environments, protecting your system while providing production-like experiences.</p>
</li>
</ul>
<p><img alt="Same experience despite the environment (example of the local development)" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/phmubsb34hn2mfefjvqh.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;Same experience despite the environment (example of the local development)&lt;/i&gt;&lt;/center&gt;
&lt;p&gt;&lt;/p&gt;</p>
<ul>
<li><strong>Collaboration via code:</strong> All applications and components are wrapped up in one repository, which allows the entire organization to collaborate on the same resources and avoid double work and redundant setup.</li>
</ul>
<h2>How you can get started</h2>
<p>If you're facing similar challenges with scattered Streamlit applications, here's how to begin and move quickly:</p>
<ol>
<li><strong>Assess your current state:</strong> Inventory your existing applications and identify pain points.</li>
<li><strong>Define your roles:</strong> Separate maintainer responsibilities from creator and end users' needs.</li>
<li><strong>Start with templates:</strong> Create standardized application templates that enforce your security and compliance requirements.</li>
<li><strong>Implement CI/CD:</strong> Automate your deployment pipeline to reduce manual errors and ensure consistency.</li>
</ol>
<p><img alt="Deploy the application in Snowflake" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1760036003/mzge9s1fhkhnx38y1a3i.png" />
&lt;p&gt;&lt;/p&gt;
&lt;center&gt;&lt;i&gt;The application deployed in Snowflake&lt;/i&gt;&lt;/center&gt;</p>
<h2>The bigger picture</h2>
<p>This framework represents more than just a technical solution ‚Äî it's a paradigm shift toward treating data applications as first-class citizens in your enterprise (data) architecture.
By providing structure without sacrificing flexibility, the GitLab Data team created an environment where anyone in the company with minimal technical knowledge can innovate rapidly while maintaining the highest standards of security and compliance.</p>
<h3>What's next?</h3>
<p>We're continuing to enhance the framework based on user feedback and emerging needs. Future improvements include expanded template libraries, enhanced monitoring capabilities, more flexibility, and a smoother user experience.
<strong>The goal isn't just to solve today's problems, but to create a foundation that scales with your organization's growing data application needs.</strong></p>
<h2>Summary</h2>
<p><a href="https://handbook.gitlab.com/handbook/enterprise-data/">The GitLab Data Team</a> transformed dozens of scattered, insecure Streamlit applications with no standardization into a unified, enterprise-grade framework that separates roles cleanly:</p>
<ol>
<li><strong>Maintainers</strong> handle infrastructure and security.</li>
<li><strong>Creators</strong> focus on building applications without deployment headaches.</li>
<li><strong>Viewers</strong> access polished, compliant apps.</li>
</ol>
<p>And we used these building blocks:</p>
<ol>
<li>Automated <strong>CI/CD</strong> pipelines</li>
<li>Fully collaborative and versioned code in <strong>git</strong></li>
<li><strong>Template-based</strong> development</li>
<li>Built-in <strong>security</strong> compliance, testing</li>
<li><strong>Poetry-managed</strong> environments
We eliminated the maintenance nightmare while enabling rapid innovation ‚Äî proving that you can have both structure and flexibility when you treat data applications as first-class enterprise assets rather than throwaway prototypes.</li>
</ol>
- [Streamline enterprise artifact management with GitLab](https://about.gitlab.com/blog/streamline-enterprise-artifact-management-with-gitlab/) ‚Äî <p>For the past six years, I've worked on artifact management at GitLab and have had hundreds of conversations with platform engineers trying to solve the same challenge: managing artifacts when they've become a sprawling, expensive mess. What started as simple Docker registries and Maven repositories has evolved into a complex web of tools, policies, and operational overhead that's consuming more time and budget than anyone anticipated.</p>
<p>I recently spoke with a platform engineer at a Fortune 500 company who told me, &quot;I spend more time managing artifact repositories than I do on actual platform improvements.&quot; That conversation reminded me why we need an honest discussion about the real costs of fragmented artifact management ‚Äî and what platform teams can realistically do about it. This article will help you better understand the problem and how GitLab can help you solve it through strategic consolidation.</p>
<h2>Real-world impact: The numbers</h2>
<p>Based on data from our customers and industry research, fragmented artifact management typically results in the following costs for a midsize organization (500+ developers):</p>
<ul>
<li><strong>Licensing:</strong> $50,000-200,000 annually across multiple tools</li>
<li><strong>Operational overhead:</strong> 2-3 FTE's equivalent time spent on artifact management tasks</li>
<li><strong>Storage inefficiency:</strong> 20%-30% higher storage costs due to duplication and poor lifecycle management</li>
<li><strong>Developer productivity loss:</strong> 15-20 minutes daily per developer due to artifact-related friction</li>
</ul>
<p>For large enterprises, these numbers multiply significantly. One customer calculated they were spending over $500,000 annually just on the operational overhead of managing seven different artifact storage systems.</p>
<p>The hidden costs compound daily:</p>
<p><strong>Time multiplication:</strong> Every lifecycle policy, security rule, or access control change must be implemented across multiple systems. What should be a 15-minute configuration becomes hours of work.</p>
<p><strong>Security gap risks:</strong> Managing security policies across disparate systems creates blind spots. Vulnerability scanning, access controls, and audit trails become fragmented.</p>
<p><strong>Context switching tax:</strong> Developers lose productivity when they can't find artifacts or need to remember which system stores what.</p>
<h2>The multiplication problem</h2>
<p>The artifact management landscape has exploded. Where teams once managed a single Maven repository, today's platform engineers juggle:</p>
<ul>
<li>Container registries (Docker Hub, ECR, GCR, Azure ACR)</li>
<li>Package repositories (JFrog Artifactory, Sonatype Nexus)</li>
<li>Language-specific registries (npm, PyPI, NuGet, Conan)</li>
<li>Infrastructure artifacts (Terraform modules, Helm charts)</li>
<li>ML model registries (MLflow, Weights &amp; Biases)</li>
</ul>
<p>Each tool comes with its own authentication system, lifecycle policies, security scanning, and operational requirements. For organizations with hundreds or thousands of projects, this creates an exponential management burden.</p>
<h2>GitLab's strategic approach: Depth over breadth</h2>
<p>When we started building GitLab's artifact management capabilities six years ago, we faced a classic product decision: support every artifact format imaginable or go deep on the formats that matter most to enterprise teams. We chose depth, and that decision has shaped everything we've built since.</p>
<h3>Our core focus areas</h3>
<p>Instead of building shallow support for 20+ formats, we committed to delivering enterprise-grade capabilities for a strategic set:</p>
<ul>
<li><strong>Maven</strong> (Java ecosystem)</li>
<li><strong>npm</strong> (JavaScript/Node.js)</li>
<li><strong>Docker/OCI</strong> (container images)</li>
<li><strong>PyPI</strong> (Python packages)</li>
<li><strong>NuGet</strong> (C#/.NET packages)</li>
<li><strong>Generic packages</strong> (any binary artifact)</li>
<li><strong>Terraform modules</strong> (infrastructure as code)</li>
</ul>
<p>These seven formats account for approximately 80% of artifact usage in enterprise environments, based on our customer data.</p>
<h3>What 'enterprise-grade' actually means</h3>
<p>By focusing on fewer formats, we can deliver capabilities that work in production environments with hundreds of developers, terabytes of artifacts, and strict compliance requirements:</p>
<p><strong><a href="https://docs.gitlab.com/user/packages/virtual_registry/">Virtual registries</a>:</strong> Proxy and cache upstream dependencies for reliable builds and supply chain control. Currently production-ready for Maven, with npm and Docker coming in early 2026.</p>
<p><strong>Lifecycle management</strong>: Automated cleanup policies that prevent storage costs from spiraling while preserving artifacts for compliance. Available at the project level today, organization-level policies planned for mid-2026.</p>
<p><strong><a href="https://docs.gitlab.com/user/application_security/">Security integration</a>:</strong> Built-in vulnerability scanning, dependency analysis, and policy enforcement. Our upcoming Dependency Firewall (planned for late 2026) will provide supply chain security control across all formats.</p>
<p><strong><a href="https://docs.gitlab.com/ci/">Deep CI/CD integration</a>:</strong> Complete traceability from source commit to deployed artifact, with build provenance and security scan results embedded in artifact metadata.</p>
<h2>Current capabilities: Battle-tested features</h2>
<p><strong>Maven virtual registries:</strong> Our flagship enterprise capability, proven with 15+ enterprise customers. Most complete <a href="https://about.gitlab.com/blog/tutorial-secure-and-optimize-your-maven-repository-in-gitlab/">Maven virtual registry</a> setup within two months, with minimal GitLab support required.</p>
<p><strong>Locally-hosted repositories:</strong> All seven supported formats offer complete upload, download, versioning, and access control capabilities supporting critical workloads at organizations with thousands of developers.</p>
<p><strong>Protected artifacts:</strong> Comprehensive protection preventing unauthorized modifications, supporting fine-grained access controls across all formats.</p>
<p><strong>Project-level lifecycle policies:</strong> Automated cleanup and retention policies for storage cost control and compliance.</p>
<h3>Performance and scale characteristics</h3>
<p>Based on current production deployments:</p>
<ul>
<li><strong>Throughput:</strong> 10,000+ artifact downloads per minute/per instance</li>
<li><strong>Storage:</strong> Customers successfully managing 50+ TB of artifacts</li>
<li><strong>Concurrent users:</strong> 1,000+ developers accessing artifacts simultaneously</li>
<li><strong>Availability:</strong> 99.99% uptime for <a href="http://GitLab.com">GitLab.com</a> for more than 2 years</li>
</ul>
<h2>Strategic roadmap: Next 18 months</h2>
<h3>Q1 2026</h3>
<ul>
<li><strong>npm virtual registries:</strong> Enterprise proxy/cache for JavaScript packages</li>
<li><strong>Docker virtual registries:</strong> Container registry proxy capabilities</li>
</ul>
<h3>Q2 2026</h3>
<ul>
<li><strong>Organization-level lifecycle policies (Beta):</strong> Centralized cleanup policies with project overrides</li>
<li><strong>NuGet virtual registries (Beta):</strong> .NET package proxy support</li>
<li><strong>PyPI virtual registries (Beta):</strong> Completing virtual registry support for Python</li>
</ul>
<h3>Q3 2026</h3>
<ul>
<li><strong>Advanced Analytics Dashboard:</strong> Storage optimization and usage insights</li>
</ul>
<h3>Q4 2026</h3>
<ul>
<li><strong>Dependency Firewall (Beta):</strong> Supply chain security control for all artifact types</li>
</ul>
<h2>When to choose GitLab: Decision framework</h2>
<p><strong>GitLab is likely the right choice if:</strong></p>
<ul>
<li>80%+ of your artifacts are in our seven supported formats</li>
<li>You're already using GitLab for source code or CI/CD</li>
<li>You value integrated workflows over standalone feature richness</li>
<li>You want to reduce the operational complexity of managing multiple systems</li>
<li>You need complete traceability from source to deployment</li>
</ul>
<h3>Migration considerations</h3>
<p><strong>Typical timeline:</strong> 2-4 months for complete migration from Artifactory/Nexus</p>
<p><strong>Common challenges:</strong> Virtual registry configuration, access control mapping, and developer workflow changes</p>
<p><strong>Success factors:</strong> Phased approach, comprehensive testing, and developer training</p>
<p>Most successful migrations follow this pattern:</p>
<ol>
<li><strong>Assessment</strong> (2-4 weeks): Catalog current artifacts and usage patterns</li>
<li><strong>Pilot</strong> (4-6 weeks): Migrate one team/project end-to-end</li>
<li><strong>Rollout</strong> (6-12 weeks): Gradual migration with parallel systems</li>
<li><strong>Optimization</strong> (ongoing): Implement advanced features and policies</li>
</ol>
<h2>Better artifact management can start today</h2>
<p>GitLab's artifact management isn't trying to be everything to everyone. We've made strategic trade-offs: deep capabilities for core enterprise formats rather than shallow support for everything.</p>
<p>If your artifact needs align with our supported formats and you value integrated workflows, we can significantly reduce your operational overhead while improving developer experience.</p>
<p>Our goal is to help you make informed decisions about your artifact management strategy with a clear understanding of capabilities and our roadmap.</p>
<p>Please reach out to me at <a href="mailto:trizzi@gitlab.com">trizzi@gitlab.com</a> to learn more about GitLab artifact management. I can discuss specific requirements and connect you with our technical team for a deeper evaluation.</p>
<p><em>This blog contains information related to upcoming products, features, and functionality. It is important to note that the information in this blog post is for informational purposes only. Please do not rely on this information for purchasing or planning purposes. As with all projects, the items mentioned in this blog and linked pages are subject to change or delay. The development, release, and timing of any products, features, or functionality remain at the sole discretion of GitLab.</em></p>
- [Atlassian ending Data Center as GitLab maintains deployment choice](https://about.gitlab.com/blog/atlassian-ending-data-center-as-gitlab-maintains-deployment-choice/) ‚Äî <p>Change is never easy, especially when it's not your choice. Atlassian's announcement that <a href="https://www.atlassian.com/blog/announcements/atlassian-ascend">all Data Center products will reach end-of-life by March 28, 2029</a>, means thousands of organizations must now reconsider their DevSecOps deployment and infrastructure. But you don't have to settle for deployment options that don't fit your needs. GitLab maintains your freedom to choose ‚Äî whether you need self-managed for compliance, cloud for convenience, or hybrid for flexibility ‚Äî all within a single AI-powered DevSecOps platform that respects your requirements.</p>
<p>While other vendors force migrations to cloud-only architectures, GitLab remains committed to supporting the deployment choices that match your business needs. Whether you're managing sensitive government data, operating in air-gapped environments, or simply prefer the control of self-managed deployments, we understand that one size doesn't fit all.</p>
<h2>The cloud isn't the answer for everyone</h2>
<p>For the many companies that invested millions of dollars in Data Center deployments, including those that migrated to Data Center <a href="https://about.gitlab.com/blog/atlassian-server-ending-move-to-a-single-devsecops-platform/">after its Server products were discontinued</a>, this announcement represents more than a product sunset. It signals a fundamental shift away from customer-centric architecture choices, forcing enterprises into difficult positions: accept a deployment model that doesn't fit their needs, or find a vendor that respects their requirements.</p>
<p>Many of the organizations requiring self-managed deployments represent some of the world's most important organizations: healthcare systems protecting patient data, financial institutions managing trillions in assets, government agencies safeguarding national security, and defense contractors operating in air-gapped environments.</p>
<p>These organizations don't choose self-managed deployments for convenience; they choose them for compliance, security, and sovereignty requirements that cloud-only architectures simply cannot meet. Organizations operating in closed environments with restricted or no internet access aren't exceptions ‚Äî they represent a significant portion of enterprise customers across various industries.</p>
<p><img alt="GitLab vs. Atlassian comparison table" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1759928476/ynl7wwmkh5xyqhszv46m.jpg" /></p>
<h2>The real cost of forced cloud migration goes beyond dollars</h2>
<p>While cloud-only vendors frame mandatory migrations as &quot;upgrades,&quot; organizations face substantial challenges beyond simple financial costs:</p>
<ul>
<li>
<p><strong>Lost integration capabilities:</strong> Years of custom integrations with legacy systems, carefully crafted workflows, and enterprise-specific automations become obsolete. Organizations with deep integrations to legacy systems often find cloud migration technically infeasible.</p>
</li>
<li>
<p><strong>Regulatory constraints:</strong> For organizations in regulated industries, cloud migration isn't just complex ‚Äî it's often not permitted. Data residency requirements, air-gapped environments, and strict regulatory frameworks don't bend to vendor preferences. The absence of single-tenant solutions in many cloud-only approaches creates insurmountable compliance barriers.</p>
</li>
<li>
<p><strong>Productivity impacts:</strong> Cloud-only architectures often require juggling multiple products: separate tools for planning, code management, CI/CD, and documentation. Each tool means another context switch, another integration to maintain, another potential point of failure. GitLab research shows <a href="https://about.gitlab.com/developer-survey/">30% of developers spend at least 50% of their job maintaining and/or integrating their DevSecOps toolchain</a>. Fragmented architectures exacerbate this challenge rather than solving it.</p>
</li>
</ul>
<h2>GitLab offers choice, commitment, and consolidation</h2>
<p>Enterprise customers deserve a trustworthy technology partner. That's why we've committed to supporting a range of deployment options ‚Äî whether you need on-premises for compliance, hybrid for flexibility, or cloud for convenience, the choice remains yours. That commitment continues with <a href="https://about.gitlab.com/gitlab-duo/">GitLab Duo</a>, our AI solution that supports developers at every stage of their workflow.</p>
<p>But we offer more than just deployment flexibility. While other vendors might force you to cobble together their products into a fragmented toolchain, GitLab provides everything in a <strong>comprehensive AI-native DevSecOps platform</strong>. Source code management, CI/CD, security scanning, Agile planning, and documentation are all managed within a single application and a single vendor relationship.</p>
<p>This isn't theoretical. When <a href="https://about.gitlab.com/customers/airbus/">Airbus</a> and <a href="https://about.gitlab.com/customers/iron-mountain/">Iron Mountain</a> evaluated their existing fragmented toolchains, they consistently identified challenges: poor user experience, missing functionalities like built-in security scanning and review apps, and management complexity from plugin troubleshooting. <strong>These aren't minor challenges; they're major blockers for modern software delivery.</strong></p>
<h2>Your migration path: Simpler than you think</h2>
<p>We've helped thousands of organizations migrate from other vendors, and we've built the tools and expertise to make your transition smooth:</p>
<ul>
<li>
<p><strong>Automated migration tools:</strong> Our <a href="https://docs.gitlab.com/user/project/import/bitbucket_server/">Bitbucket Server importer</a> brings over repositories, pull requests, comments, and even Large File Storage (LFS) objects. For Jira, our <a href="https://docs.gitlab.com/user/project/import/jira/">built-in importer</a> handles issues, descriptions, and labels, with professional services available for complex migrations.</p>
</li>
<li>
<p><strong>Proven at scale:</strong> A 500 GiB repository with 13,000 pull requests, 10,000 branches, and 7,000 tags is likely to <a href="https://docs.gitlab.com/user/project/import/bitbucket_server/">take just 8 hours to migrate</a> from Bitbucket to GitLab using parallel processing.</p>
</li>
<li>
<p><strong>Immediate ROI:</strong> A <a href="https://about.gitlab.com/resources/study-forrester-tei-gitlab-ultimate/">Forrester Consulting Total Economic Impact‚Ñ¢ study commissioned by GitLab</a> found that investing in GitLab Ultimate confirms these benefits translate to real bottom-line impact, with a three-year 483% ROI, 5x time saved in security related activities, and 25% savings in software toolchain costs.</p>
</li>
</ul>
<h2>Start your journey to a unified DevSecOps platform</h2>
<p>Forward-thinking organizations aren't waiting for vendor-mandated deadlines. They're evaluating alternatives now, while they have time to migrate thoughtfully to platforms that protect their investments and deliver on promises.</p>
<p>Organizations invest in self-managed deployments because they need control, compliance, and customization. When vendors deprecate these capabilities, they remove not just features but the fundamental ability to choose environments matching business requirements.</p>
<p>Modern DevSecOps platforms should offer complete functionality that respects deployment needs, consolidates toolchains, and accelerates software delivery, without forcing compromises on security or data sovereignty.</p>
<p><a href="https://about.gitlab.com/sales/">Talk to our sales team</a> today about your migration options, or explore our <a href="https://about.gitlab.com/move-to-gitlab-from-atlassian/">comprehensive migration resources</a> to see how thousands of organizations have already made the switch.</p>
<p>You also can <a href="https://about.gitlab.com/free-trial/devsecops/">try GitLab Ultimate with GitLab Duo Enterprise</a> for free for 30 days to see what a unified DevSecOps platform can do for your organization.</p>
- [Agentic AI guides and resources](https://about.gitlab.com/blog/agentic-ai-guides-and-resources/) ‚Äî <h2>Defining agentic AI</h2>
<p>Agentic AI is a type of artificial intelligence that leverages advanced language models and natural language processing to take independent action. Unlike traditional generative AI tools that require constant human direction, these systems can understand requests, make decisions, and execute multi-step plans to achieve goals. They tackle complex tasks by breaking them into manageable steps and employ adaptive learning to modify their approach when facing challenges.</p>
<p><a href="https://about.gitlab.com/topics/agentic-ai/">Learn more about agentic AI</a></p>
<h2>Agentic AI insights</h2>
<ul>
<li><a href="https://about.gitlab.com/the-source/ai/transform-development-with-agentic-ai-the-enterprise-guide/">Transform development with agentic AI: The enterprise guide</a></li>
<li><a href="https://about.gitlab.com/blog/gitlab-18-4-ai-native-development-with-automation-and-insight/">GitLab 18.4: AI-native development with automation and insight</a> With GitLab 18.4, teams create custom agents, unlock Knowledge Graph context, and auto-fix pipelines so developers stay focused and in flow.</li>
<li><a href="https://about.gitlab.com/blog/gitlab-18-3-expanding-ai-orchestration-in-software-engineering/">GitLab 18.3: Expanding AI orchestration in software engineering</a> Learn how we're advancing human-AI collaboration with enhanced Flows, enterprise governance, and seamless tool integration.</li>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-agent-platform-public-beta/">GitLab Duo Agent Platform Public Beta: Next-gen AI orchestration and more</a> ‚Äî Introducing the DevSecOps orchestration platform designed to unlock asynchronous collaboration between developers and AI agents.</li>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-agent-platform-what-is-next-for-intelligent-devsecops/">GitLab Duo Agent Platform: What's next for intelligent DevSecOps</a> ‚Äî GitLab Duo Agent Platform, a DevSecOps orchestration platform for humans and AI agents, leverages agentic AI for collaboration across the software development lifecycle.</li>
<li><a href="https://about.gitlab.com/the-source/ai/from-vibe-coding-to-agentic-ai-a-roadmap-for-technical-leaders/">From vibe coding to agentic AI: A roadmap for technical leaders</a> ‚Äî Discover how to implement vibe coding and agentic AI in your development process to increase productivity while maintaining code quality and security.</li>
<li><a href="https://about.gitlab.com/the-source/ai/emerging-agentic-ai-trends-reshaping-software-development/">Emerging agentic AI trends reshaping software development</a> ‚Äî Discover how agentic AI transforms development from isolated coding to intelligent workflows that enhance productivity while maintaining security.</li>
<li><a href="https://about.gitlab.com/the-source/ai/agentic-ai-unlocking-developer-potential-at-scale/">Agentic AI: Unlocking developer potential at scale</a> ‚Äî Explore how agentic AI is transforming software development, moving beyond code completion to create AI partners that proactively tackle complex tasks.</li>
<li><a href="https://about.gitlab.com/the-source/ai/ai-trends-for-2025-agentic-ai-self-hosted-models-and-more/">Agentic AI, self-hosted models, and more: AI trends for 2025</a> ‚Äî Discover key trends in AI for software development, from on-premises model deployments to intelligent, adaptive AI agents.</li>
<li><a href="https://about.gitlab.com/the-source/ai/how-agentic-ai-unlocks-platform-engineering-potential/">How agentic AI unlocks platform engineering potential</a> ‚Äî Explore how agentic AI elevates platform engineering by automating complex workflows and scaling standardization.</li>
</ul>
<h2>The agentic AI ecosystem</h2>
<ul>
<li><a href="https://about.gitlab.com/topics/agentic-ai/ai-code-analysis/">AI-driven code analysis: The new frontier in code security</a></li>
<li><a href="https://about.gitlab.com/topics/agentic-ai/devops-automation-ai-agents/">DevOps automation &amp; AI agents</a></li>
<li><a href="https://about.gitlab.com/topics/agentic-ai/ai-augmented-software-development/">AI-augmented software development: Agentic AI for DevOps</a></li>
</ul>
<h2>Best practices for implementing agentic AI</h2>
<ul>
<li><a href="https://about.gitlab.com/the-source/ai/implementing-effective-guardrails-for-ai-agents/">Implementing effective guardrails for AI agents</a> ‚Äî Discover essential security guardrails for AI agents in DevSecOps, from compliance controls and infrastructure protection to user access management.</li>
</ul>
<h2>GitLab's agentic AI offerings</h2>
<h3>GitLab Duo with Amazon Q</h3>
<ul>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-with-amazon-q-agentic-ai-optimized-for-aws/">GitLab Duo with Amazon Q: Agentic AI optimized for AWS generally available</a> ‚Äî The comprehensive AI-powered DevSecOps platform combined with the deepest set of cloud computing capabilities speeds dev cycles, increases automation, and improves code quality.</li>
<li><a href="https://about.gitlab.com/blog/devsecops-agentic-ai-now-on-gitlab-self-managed-ultimate-on-aws/">DevSecOps + Agentic AI: Now on GitLab Self-Managed Ultimate on AWS</a> ‚Äî Start using AI-powered, DevSecOps-enhanced agents in your AWS GitLab Self-Managed Ultimate instance. Enjoy the benefits of GitLab Duo and Amazon Q in your organization.</li>
<li><a href="https://about.gitlab.com/partners/technology-partners/aws/">GitLab Duo with Amazon Q partner page</a></li>
</ul>
<p>Watch GitLab Duo with Amazon Q in action:</p>
<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1075753390?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;Technical Demo: GitLab Duo with Amazon Q&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>
<h4>Guided tour</h4>
<p>Click on the image to start a tour of GitLab Duo with Amazon Q:</p>
<p><a href="https://gitlab.navattic.com/duo-with-q"><img alt="GitLab Duo with Amazon Q interactive tour" src="https://res.cloudinary.com/about-gitlab-com/image/upload/v1749673568/Blog/Content%20Images/Screenshot_2025-05-07_at_7.24.45_AM.png" /></a></p>
<h4>GitLab Duo with Amazon Q tutorials</h4>
<ul>
<li><a href="https://about.gitlab.com/blog/enhance-application-quality-with-ai-powered-test-generation/">Enhance application quality with AI-powered test generation</a> ‚Äî Learn how GitLab Duo with Amazon Q improves the QA process by automatically generating comprehensive unit tests.</li>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-amazon-q-transform-ideas-into-code-in-minutes/">GitLab Duo + Amazon Q: Transform ideas into code in minutes</a> ‚Äî The new GitLab Duo with Amazon Q integration analyzes your issue descriptions and automatically generates complete working code solutions, accelerating development workflows.</li>
<li><a href="https://about.gitlab.com/blog/accelerate-code-reviews-with-gitlab-duo-and-amazon-q/">Accelerate code reviews with GitLab Duo and Amazon Q</a> ‚Äî Use AI-powered agents to optimize code reviews by automatically analyzing merge requests and providing comprehensive feedback on bugs, readability, and coding standards.</li>
<li><a href="https://about.gitlab.com/blog/speed-up-code-reviews-let-ai-handle-the-feedback-implementation/">Speed up code reviews: Let AI handle the feedback implementation</a> ‚Äî Discover how GitLab Duo with Amazon Q automates the implementation of code review feedback through AI, transforming a time-consuming manual process into a streamlined workflow.</li>
</ul>
<h3>GitLab Duo Agent Platform</h3>
<ul>
<li><a href="https://about.gitlab.com/blog/gitlab-duo-chat-gets-agentic-ai-makeover/">GitLab Duo Chat gets agentic AI makeover</a> ‚Äî Our new Duo Chat experience, currently an experimental release, helps developers onboard to projects, understand assignments, implement changes, and more.
Watch GitLab Duo Agent Platform in action:
&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1095679084?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;Agent Platform Demo Clip&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</li>
</ul>
<h4>GitLab Agent Platform tutorials and use cases</h4>
<ul>
<li><a href="https://about.gitlab.com/blog/vibe-coding-with-gitlab-duo-agent-platform-issue-to-mr-flow/">Vibe coding with GitLab Duo Agent Platform: Issue to MR Flow</a> - Learn how to update your application in minutes with our newest agent Flow that takes developers from idea to code.</li>
<li><a href="https://about.gitlab.com/blog/get-started-with-gitlab-duo-agentic-chat-in-the-web-ui/">Get started with GitLab Duo Agentic Chat in the web UI</a> - Learn about our new GitLab Duo AI feature that automates tasks by breaking down complex problems and executing operations across multiple sources.</li>
<li><a href="https://about.gitlab.com/blog/custom-rules-duo-agentic-chat-deep-dive/">Custom rules in GitLab Duo Agentic Chat for greater developer efficiency</a> ‚Äî Discover how AI can understand your codebase, follow your conventions, and generate production-ready code with minimal review cycles.</li>
<li><a href="https://about.gitlab.com/blog/accelerate-learning-with-gitlab-duo-agent-platform/">Accelerate learning with GitLab Duo Agent Platform</a> ‚Äî Learn how agentic AI helped generate comprehensive gRPC documentation in minutes, not hours.</li>
<li><a href="https://about.gitlab.com/blog/fast-and-secure-ai-agent-deployment-to-google-cloud-with-gitlab/">Fast and secure AI agent deployment to Google Cloud with GitLab</a></li>
</ul>
<h2>Learn more with GitLab University</h2>
<ul>
<li><a href="https://university.gitlab.com/pages/ai">Get Started with GitLab Duo coursework</a></li>
<li><a href="https://university.gitlab.com/learning-paths/gitlab-duo-enterprise-learning-path">GitLab Duo Enterprise Learning Path</a></li>
</ul>
<h2>More AI resources</h2>
<ul>
<li><a href="https://about.gitlab.com/developer-survey/2024/ai/">2024 Global DevSecOps Survey: Navigating AI maturity in DevSecOps</a></li>
<li><a href="https://about.gitlab.com/topics/devops/the-role-of-ai-in-devops/">The Role of AI in DevOps</a></li>
<li><a href="https://about.gitlab.com/blog/categories/ai-ml/">The latest AI/ML articles from GitLab</a></li>
<li><a href="https://about.gitlab.com/gitlab-duo/">GitLab Duo</a></li>
<li><a href="https://about.gitlab.com/gitlab-duo/agent-platform/">GitLab Duo Agent Platform</a></li>
</ul>
- [Making your business resilient against Cloudflare like outages | Blog](https://www.harness.io/blog/making-your-business-resilient-against-cloudflare-like-outages) ‚Äî Cloudflare-like outages can cost your business a significant amount of money. This week‚Äôs Cloudflare global outage is a wake-up call for business resilience. You can stay resilient against such outages by doing resilience testing regularly with Harness Chaos Engineering. | Blog
- [Automating Chaos Engineering with Terraform | Blog](https://www.harness.io/blog/automating-chaos-engineering-with-terraform) ‚Äî Automate chaos engineering with Terraform: version-controlled infrastructure, service discovery, security governance, and ChaosHub management for resilient systems. | Blog
- [KubeCon NA 2025 Recap: The Dawn of the AI Native Era | Blog](https://www.harness.io/blog/kubecon-2025-recap) ‚Äî Explore top insights from KubeCon NA 2025. From the Kubernetes AI Conformance Program to Adobe‚Äôs Agent Economy and Apple Containerization, discover how the community and Harness are defining the future of AI Native software delivery. | Blog
- [When Cloud Providers Have an Outage, Your Feature Flags Shouldn‚Äôt | Blog](https://www.harness.io/blog/when-cloud-providers-have-an-outage-your-feature-flags-shouldnt) ‚Äî Feature flag reliability matters. Learn how Harness ensures 100% uptime and resilience during cloud outages. | Blog
- [The AI Knowledge Agent: Making Internal Developer Portals Smarter | Blog](https://www.harness.io/blog/the-ai-knowledge-agent-making-internal-developer-portals-smarter) ‚Äî Introducing the Harness IDP Knowledge Agent, an AI-powered assistant that helps developers find answers, automate workflows, and improve onboarding. | Blog
- [Running Chaos Engineering on GKE Autopilot Just Got Easier | Blog](https://www.harness.io/blog/running-chaos-engineering-on-gke-autopilot-just-got-easier) ‚Äî Harness Chaos Engineering now supports GKE Autopilot. Run resilience tests on Google's managed Kubernetes without compromising security or flexibility. | Blog
- [Harness patent for hybrid YAML editor enhances CI/CD workflows | Blog](https://www.harness.io/blog/harness-granted-patent-for-hybrid-yaml-editor) ‚Äî Harness earned a patent for it's unified pipeline editor which makes it easy to configure pipelines whether they are for CI, CD, IaC, database migrations, service onboarding or other DevSecOps activities.  | Blog
- [Harness Q3 2025 update on continuous delivery enhancements | Blog](https://www.harness.io/blog/q3-2025-product-update-accelerating-continuous-delivery-cd-innovation) ‚Äî Harness CD advances in Q3 2025 with new platform support, enhanced GitOps, pipeline resiliency, and infrastructure flexibility improvements. | Blog
- [Transform your DevSecOps with Harness AI and Google Cloud | Blog](https://www.harness.io/blog/google-cloud-partnership) ‚Äî Discover how Harness AI leverages Google Cloud Vertex AI to automate CI/CD pipelines, cut test times by 80%, and deliver secure software faster with privacy-first intelligence and real-time security scanning. | Blog
- [Monitoring Chaos Experiments with New Relic Probe in Harness | Blog](https://www.harness.io/blog/monitoring-chaos-experiments-with-new-relic-probe-in-harness) ‚Äî Learn how to use New Relic probes in Harness Chaos Engineering to automatically validate system performance against SLOs during chaos experiments. | Blog
- [Mastering load testing with Locust in chaos engineering | Blog](https://www.harness.io/blog/load-testing-at-scale-understanding-locust-loadgen-in-harness-chaos-engineering) ‚Äî Load test your apps with Locust loadgen in Harness Chaos Engineering. Simulate traffic, monitor in Grafana, and build resilient systems. | Blog
- [DevSecOps Summit 2025: AI Security From Pipeline to Production | Blog](https://www.harness.io/blog/devsecops-summit-2025-ai-security-from-pipeline-to-production) ‚Äî The DevSecOps Summit 2025 will bring industry leaders, security practitioners, and AI innovators together to tackle the most pressing challenge of our generation: securing AI systems from the first line of code to production deployment and beyond. | Blog
- [How Harness addresses the challenges of AI in software delivery | Blog](https://www.harness.io/blog/the-ai-velocity-paradox) ‚Äî In our 2025 State of AI in Software Engineering report, we explored how widespread AI adoption is accelerating code delivery in software engineering, but also creating new bottlenecks in testing, security, and deployment. Learn how the Harness AI platform helps organizations automate the entire software delivery lifecycle, bridging the gap between speed and resilience. | Blog
- [Harness GitOps: Enterprise-Ready GitOps on Argo CD | Blog](https://www.harness.io/blog/scaling-argo-cd-with-enterprise-grade-control) ‚Äî Harness GitOps enhances Argo CD with RBAC, audit logs, unified dashboards, and multi-cluster pipelines, scaling GitOps from single clusters to enterprise fleets without losing the native reconciliation loop. | Blog
- [AI-Powered Chaos Engineering with Harness MCP Server and Cursor | Blog](https://www.harness.io/blog/ai-powered-chaos-engineering-with-harness-mcp-server-and-cursor) ‚Äî Democratize chaos engineering with Harness MCP + Cursor. Execute resilience tests through natural language prompts in your AI-powered IDE. | Blog
- [AI-Powered Resilience Testing with Harness MCP Server and Windsurf | Blog](https://www.harness.io/blog/ai-powered-resilience-testing-with-harness-mcp-server-and-windsurf) ‚Äî Learn how to set up AI-powered chaos engineering using Harness MCP Server with Windsurf IDE. Execute resilience tests through natural language prompts. | Blog
- [Resilience Testing using Harness Chaos Engineering | Blog](https://www.harness.io/blog/resilience-testing-using-harness) ‚Äî Resilience testing is made easy to onboard and scalable with Harness Chaos Engineering. Harness provides all the required capabilities to practice resilience testing in SDLC. | Blog
- [Lessons from the NPM Attack | Blog](https://www.harness.io/blog/lessons-from-the-npm-attack) ‚Äî A massive NPM supply chain attack compromised 18+ widely used JavaScript packages with 2B+ weekly downloads after a maintainer fell victim to a phishing email. Malicious code was injected to hijack crypto wallets and blockchain transactions, underscoring the need for stronger OSS defenses. | Blog
- [Harness the Power of Platform Engineering | Blog](https://www.harness.io/blog/harness-the-power-of-platform-engineering) ‚Äî Discover how platform engineering is revolutionizing software delivery by accelerating innovation, embedding quality and security, and optimizing costs. Learn why Harness is the preferred platform for creating scalable, secure Internal Developer Platforms that drive developer experience and operational excellence. | Blog
- [Explore Q2 feature updates to enhance your DevOps processes | Blog](https://www.harness.io/blog/unlocking-innovation-harness-q2-feature-releases-that-accelerate-your-devops-journey) ‚Äî Explore Q2 updates from Harness, including GitOps UX improvements, smarter rollbacks, Lambda canary support, and more across your delivery pipelines. | Blog
- [Harness Honored with Inaugural Wiz Integrations Partner Award | Blog](https://www.harness.io/blog/harness-honored-with-inaugural-wiz-integrations-win-partner-award) ‚Äî Harness celebrates its WINvaluable Award from Wiz! Discover how our certified partnership embeds AI-powered remediation and cloud security into your CI/CD pipeline to create a new standard for DevSecOps. | Blog
- [How Harness is Using AI to Simplify Chaos Engineering Adoption | Blog](https://www.harness.io/blog/how-harness-is-using-ai-to-simplify-chaos-engineering-adoption) ‚Äî Learn how Harness AI Reliability Agent uses artificial intelligence to automate chaos engineering experiments, provide intelligent remediation guidance, and make reliability testing accessible to teams at any skill level. | Blog
- [Harness introduces MCP tools for enhanced chaos engineering | Blog](https://www.harness.io/blog/harness-launches-mcp-tools-to-enhance-its-ai-powered-chaos-engineering-capabilities) ‚Äî Chaos Tools on Harness MCP Server provide the opportunity to do the resilience testing using AI Agents and AI Tools with natural language prompts. Users can discover the resilience test capabilities, learn more about them and run the tests to measure the resilience data of the business critical applications or services. Chaos Tools lay the foundation for AI Powered Chaos Engineering using Harness in the Enterprise's journey towards building a strong resilience testing culture. | Blog
- [Integrating Checkmarx One with Harness for CI security scanning | Blog](https://www.harness.io/blog/harness-sto-checkmarxone-orchestrating-security-scanning-in-your-ci-pipeline) ‚Äî Automate security scans and enforce policies across your pipelines with Harness STO and Checkmarx One - seamless, scalable, and built for DevSecOps. | Blog
- [Database DevOps: Fix Git Before It Breaks Production | Blog](https://www.harness.io/blog/how-git-strategy-can-break-your-database-pipeline) ‚Äî Learn why per-environment Git branching breaks database deployments and how a trunk-based, context-driven GitOps approach restores reliability, speed, and confidence. | Blog
- [Explore the key features of Harness Open Source v3.2.0 | Blog](https://www.harness.io/blog/whats-new-in-harness-open-source-v320) ‚Äî Explore what‚Äôs new in Harness Open Source v3.2.0 ‚Äî from Git LFS support and one-click PR reverts to smarter Code Owner assignments and tighter branch review rules. Upgrade your GitOps and CI/CD workflows with these developer-focused enhancements. | Blog
- [Harness IDP enhances developer portals for enterprise scalability | Blog](https://www.harness.io/blog/new-harness-idp-release-powering-developer-portals-at-scale) ‚Äî New Harness IDP release extends the Backstage framework for enterprise scale with powerful new features for control, security, and developer adoption. | Blog
- [How APM Probes enhance monitoring during chaos experiments | Blog](https://www.harness.io/blog/understanding-apm-probes-how-to-monitor-your-apps-during-chaos-experiments) ‚Äî Learn how APM Probes enable objective measurement during chaos experiments by integrating with monitoring systems like Prometheus, AppDynamics, and Splunk to validate application resilience in Kubernetes environments. | Blog
- [The Latest Q1 GitOps Enhancements from Harness | Blog](https://www.harness.io/blog/the-latest-q1-gitops-enhancements-from-harness) ‚Äî Explore all the latest GitOps features from Q1 2025-sync updates, rollback support, OpenShift integration, and improved visibility. | Blog
- [Q1 2025 Product Update: All the Latest Features Delivered by Harness | Blog](https://www.harness.io/blog/q1-2025-product-update-all-the-latest-features-delivered-by-harness) ‚Äî Explore all the major features shipped in Q1 2025 from Harness‚Äîincluding Kubernetes diff previews, ECS Blue-Green traffic shifting, flexible Helm deployments, and enhanced GitOps workflows. This roundup highlights how these updates simplify deployments and improve governance across your delivery pipeline. | Blog
- [DevSecOps in the Age of AI ‚Äì Assess Your Readiness Now | Blog](https://www.harness.io/blog/devsecops-in-the-age-of-ai---assess-your-readiness-now) ‚Äî Explore how AI is reshaping DevSecOps and assess your organization's readiness with our updated Engineering Excellence Maturity Assessment. This assessment has been redesigned to tackle modern software development lifecycle, security, and productivity challenges. | Blog
- [Explore Traceable Cloud WAAP for modern application security | Blog](https://www.harness.io/blog/introducing-traceable-cloud-waap-built-for-the-way-applications-work-today) ‚Äî Discover Traceable Cloud WAAP, the first unified DevSecOps innovation from Harness + Traceable. Purpose-built to secure modern, API-driven applications with deep context, real-time protection, and seamless deployment across any environment. | Blog
- [Using Feature Flags with Open Telemetry | Blog](https://www.harness.io/blog/using-feature-flags-with-open-telemetry) ‚Äî Learn how to use feature flags with Open Telemetry to improve observability following a step by step example. | Blog
- [Harness Named a Leader in GigaOm Radar for GitOps | Blog](https://www.harness.io/blog/harness-named-a-leader-in-gigaom-radar-for-gitops) ‚Äî Harness earns top scores and Leader placement in the 2025 GigaOm Radar for GitOps, recognized for innovation, enterprise-grade security, and AI-powered deployment verification. Download the full report to learn more.  | Blog
- [GitOps Made Easy: Your Onboarding Adventure with Harness GitOps | Blog](https://www.harness.io/blog/gitops-made-easy-your-onboarding-adventure-with-harness-gitops) ‚Äî Kickstart your GitOps journey with Harness! Explore real-world samples, from PR pipelines to automated rollbacks, and simplify deployments with ease and confidence. | Blog
- [Azure DevOps and GitHub Repositories ‚Äî Next Steps in the Path to Agentic AI](https://devblogs.microsoft.com/devops/azure-devops-and-github-repositories-next-steps-in-the-path-to-agentic-ai/) ‚Äî <p>In May, we talked about the evolution of GitHub Copilot from a coding assistant into an AI powered peer programmer. Since then, GitHub has taken a major step forward &#8211; becoming an open platform for agentic development, where Agent HQ enables developers to orchestrate any agent, anytime, anywhere. Agent‚ÄØHQ provides observability, governance, and security controls [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/azure-devops-and-github-repositories-next-steps-in-the-path-to-agentic-ai/">Azure DevOps and GitHub Repositories ‚Äî Next Steps in the Path to Agentic AI</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [Azure Boards integration with GitHub Copilot (Private Preview)](https://devblogs.microsoft.com/devops/azure-boards-integration-with-github-copilot-private-preview/) ‚Äî <p>As of October 16, 2025, we are no longer accepting organization signups for the private preview. Our focus is now on completing the feature and preparing it for general availability in the coming weeks. Several months ago, GitHub introduced the public preview of its Copilot coding agent, a powerful new capability that allows you to [&#8230;]</p>
<p>The post <a href="https://devblogs.microsoft.com/devops/azure-boards-integration-with-github-copilot-private-preview/">Azure Boards integration with GitHub Copilot (Private Preview)</a> appeared first on <a href="https://devblogs.microsoft.com/devops">Azure DevOps Blog</a>.</p>
- [The best DevOps platforms compared: The definitive guide for 2025](https://pieces.app/blog/devops-platforms) ‚Äî Jenkins vs GitHub Actions: Custom freedom vs modern simplicity ... Nagios vs CloudBees: Classic observability vs enterprise pipeline control.
- [DORA Metrics, Flow Metrics (Lead Time, Cycle Time, Throughput)](https://www.projectmanagement.com/wikis/1122558/modern-metrics--dora-metrics--flow-metrics--lead-time--cycle-time--throughput-) ‚Äî Purpose: DORA metrics connect engineering practices to business performance by balancing speed (frequency, lead time) and stability (failure ...
- [What Are DORA Metrics? A Guide for DevOps Teams - New Relic](https://newrelic.com/blog/best-practices/dora-metrics) ‚Äî Flow metrics complement DORA metrics by measuring how work moves through your development pipeline. They include flow velocity, time, efficiency ...
- [DevOps & DORA Metrics: The Complete Guide - Splunk](https://www.splunk.com/en_us/blog/learn/devops-metrics.html) ‚Äî Flow metrics are a framework for measuring how much value is being delivered by a product value stream and the rate at which it is delivered from start to ...
- [Flow & DORA Metrics Combined For Better Delivery - Mstone.ai](https://mstone.ai/question/flow-vs-dora-metrics-benefits-together/) ‚Äî Combine Flow and DORA metrics to boost software delivery, spot bottlenecks, and improve team performance and reliability.
- [What Are DORA Metrics? | Planview](https://www.planview.com/resources/articles/what-are-dora-metrics/) ‚Äî DORA metrics provide a framework for measuring software delivery throughput (speed) and reliability (quality). Flow Metrics: A Business Leader's Guide to ...
- [DORA Metrics - Datadog Docs](https://docs.datadoghq.com/dora_metrics/) ‚Äî DevOps Research and Assessment (DORA) metrics are four key metrics that indicate the velocity and stability of software development.
- [DORA Metrics: Measuring DevOps Performance - CloudBees](https://www.cloudbees.com/blog/cloudbees-platform-dora-metrics-tutorial) ‚Äî DORA metrics help measure and improve the performance of software delivery teams. They help companies understand how well their engineering teams are performing ...
- [DORA metrics: How high-performing teams track DevOps success](https://appfire.com/resources/blog/dora-metrics-with-flow) ‚Äî Learn how to track DORA metrics like deployment frequency and MTTR inside Jira ‚Äî with tools that automate the hard parts.
- [Top 10 Internal Developer Platforms - Qovery](https://www.qovery.com/blog/10-best-internal-developer-platforms) ‚Äî Top 10 Internal Developer Platforms ¬∑ 1. Scalability ¬∑ 2. Integration Capabilities ¬∑ 3. Ease of Use ¬∑ 4. Security and Compliance ¬∑ 5. Cost ¬∑ 6. Use Case Suitability.
- [Actual succesfull experiences with Internal Developer Platforms](https://www.reddit.com/r/devops/comments/1ae7l8r/actual_succesfull_experiences_with_internal/) ‚Äî Does anyone in this space have had a good interaction with / be boosted by an Internal Developer Platform? And if yes, how did it look like?
- [Top 6 Internal Developer Platforms for 2025 | Blog - Northflank](https://northflank.com/blog/top-six-internal-developer-platforms) ‚Äî Compare the top Internal Developer Platforms for 2025. Review Northflank, Backstage, Harness & more to boost developer productivity and ...
- [Top 20 Platform Engineering Tools to Use in 2025 - Spacelift](https://spacelift.io/blog/platform-engineering-tools) ‚Äî Explore a list of the 20 best platform engineering tools, technologies, and products for 2025.
- [Comparison of Internal Developer Platforms | by Anita Ihuman](https://medium.com/@Anita-ihuman/comparison-of-internal-developer-platforms-90de61db00e1) ‚Äî This article dives into what internal developer platforms are, provides a comprehensive comparison of IDP vendors and tools, and details the features they ...
- [Internal developer portals vs. internal developer platforms - Quali](https://www.quali.com/blog/internal-developer-portals-vs-internal-developer-platforms-comparison/) ‚Äî Key features of internal developer platforms include: DevOps process consolidation: The platform standardizes processes for everyone involved ...
- [Internal Developer Platforms: Top 5 Use Cases & 5 Key Components |](https://octopus.com/devops/platform-engineering/internal-developer-platform/) ‚Äî Platform teams or DevOps engineers configure and maintain the platform, freeing developers to focus on building and releasing features faster.

---

## üß† Insights
```markdown
### Customer & Community Signals (market reality)
- Many practitioners are struggling with the complexities of upgrading Jenkins and its plugins, often facing issues that prevent Jenkins from starting post-upgrade.
  - üîó Source: <Jenkins won't start after upgrade, complains about plugin versions ...> (https://community.jenkins.io/t/jenkins-wont-start-after-upgrade-complains-about-plugin-versions-even-though-i-updated-all-plugins-before-upgrading/30790)
  - üîó Source: <Issues upgrading jenkins - Ask a question> (https://community.jenkins.io/t/issues-upgrading-jenkins/17772)

- Users express frustration over the lack of clear guidance and support during Jenkins upgrades, particularly when managing numerous plugins.
  - üîó Source: <What are some common issues you've faced when upgrading ...> (https://www.reddit.com/r/jenkinsci/comments/1ftlaiz/what_are_some_common_issues_youve_faced_when/)
  - üîó Source: <Build Failures and Plugin Errors - Using Jenkins> (https://community.jenkins.io/t/build-failures-and-plugin-errors/16821)

- There is a growing sentiment that Jenkins is becoming less favorable compared to newer CI/CD solutions like GitLab CI and GitHub Actions, with some users suggesting Jenkins is "on its way out."
  - üîó Source: <GitLab CI vs Jenkins vs GitHub Actions : r/devops - Reddit> (https://www.reddit.com/r/devops/comments/105a2bn/gitlab_ci_vs_jenkins_vs_github_actions/)
  - üîó Source: <Jenkins vs. GitHub Actions vs. GitLab CI - DEV Community> (https://dev.to/574n13y/jenkins-vs-github-actions-vs-gitlab-ci-2k35)

- Developers are actively seeking lightweight, easy-to-implement CI/CD solutions, as evidenced by the interest in tools like Forgejo and Haloy, which simplify deployment processes.
  - üîó Source: <Show HN: Quick install script for self-hosted Forgejo (Git+CI) server> (https://wkoszek.github.io/easyforgejo/)
  - üîó Source: <Show HN: Haloy ‚Äì an open‚Äësource, lightweight deployment system for Docker apps> (https://github.com/haloydev/haloy)

- Embedded firmware developers are looking for specialized tools to track memory usage in CI/CD processes, indicating a demand for more tailored solutions in niche areas.
  - üîó Source: <Show HN: MemBrowse - CI/CD memory footprint tracking for embedded firmware> (https://membrowse.com)
  - üîó Source: <Three things I've learned about Git while building a CI/CD tool> (https://www.ocuroot.com/blog/things-i-learned-about-git/)

- Community discussions reveal a desire for better integration and tracking capabilities within CI/CD workflows, particularly for memory management and deployment efficiency.
  - üîó Source: <Docker Compose Continuous Deployment> (https://github.com/kimdre/doco-cd)
  - üîó Source: <Production-Grade Container Deployment with Podman Quadlets ‚Äì Larvitz Blog> (https://blog.hofstede.it/production-grade-container-deployment-with-podman-quadlets/index.html)

### Competitor Narratives (how vendors are trying to shape the market)
- GitLab positions itself as a comprehensive DevOps platform that integrates CI/CD seamlessly, emphasizing ease of use and collaboration.
  - üîó Source: <GitLab vs. GitHub vs. Harness vs. CloudBees vs Devtron> (https://devtron.ai/blog/gitlab-vs-github-vs-harness-vs-cloudbees-vs-devtron-choosing-the-right-devops-platform/)
  - üîó Source: <GitLab's Confusion - CloudBees> (https://www.cloudbees.com/blog/gitlabs-confusion)

- GitHub promotes GitHub Actions as a cloud-native solution that scales automatically, appealing to teams looking for a simplified, integrated CI/CD experience.
  - üîó Source: <GitHub Actions vs. Jenkins: Which one's right for your team? | Buildkite> (https://buildkite.com/resources/ci-cd-perspectives/github-actions-vs-jenkins-which-one-s-right-for-your-team/)
  - üîó Source: <Comparing GitHub Actions vs Jenkins: CI showdown - Pluralsight> (https://www.pluralsight.com/resources/blog/cloud/comparing-github-actions-vs-jenkins-ci-showdown)

- Harness focuses on continuous delivery and deployment, highlighting its ability to manage complex deployments with minimal configuration.
  - üîó Source: <How to Migrate Off Jenkins: The Road to Modern CI/CD | Blog> (https://www.harness.io/blog/how-to-migrate-off-jenkins-the-road-to-modern-ci-cd)
  - üîó Source: <Break Free From Jenkins - Modernize Pipelines in 1 Day - Harness> (https://www.harness.io/butler)

- AWS emphasizes the scalability and flexibility of its CI/CD tools, aiming to attract enterprises looking for robust cloud-based solutions.
  - üîó Source: <CloudBees vs GitHub Actions: Choosing a CI/CD Platform for Your ...> (https://www.cloudbees.com/blog/cloudbees-vs-github-actions-choosing-a-ci-cd-platform-for-your-business)
  - üîó Source: <GitHub vs. GitLab and other DevOps tools> (https://resources.github.com/devops/tools/compare/)

- Many vendors are pushing narratives around modernization and the need to transition from legacy systems like Jenkins to more agile, cloud-first approaches.
  - üîó Source: <Migrating From Jenkins to Harness CIE: A Journey | Blog> (https://www.harness.io/blog/jenkins-to-cie-journey)
  - üîó Source: <Migrating to a new host and upgrading - Community - Jenkins> (https://community.jenkins.io/t/migrating-to-a-new-host-and-upgrading/15526)

### Strategic Implications for CloudBees
- Lean into the narrative of safe Jenkins modernization by offering clear, actionable upgrade paths and support for existing Jenkins users facing upgrade challenges.
  - üîó Source: <Issues with Jenkins Upgrade with huge number of plugins> (https://stackoverflow.com/questions/75553296/issues-with-jenkins-upgrade-with-huge-number-of-plugins)
  - üîó Source: <Update Center best practices for plugin management> (https://docs.cloudbees.com/docs/cloudbees-ci-kb/latest/best-practices/update-center-best-practices-for-plugin-management)

- Clarify CloudBees' stance on the evolving CI/CD landscape, particularly regarding the integration of AI and automation in pipelines to address emerging developer needs.
  - üîó Source: <An Overview of CI/CD Pipeline and tools ‚Äî Jenkins, Cloudbees ...> (https://hanwenzhang123.medium.com/an-overview-of-cicd-pipeline-with-tools-jenkins-cloudbees-rancher-af246250dce9)
  - üîó Source: <GitHub promotes GitHub Actions as a cloud-native solution ...> (https://buildkite.com/resources/ci-cd-perspectives/github-actions-vs-jenkins-which-one-s-right-for-your-team/)

- Simplify the migration story from Jenkins to CloudBees, providing resources and tools that make the transition less daunting for users considering alternatives.
  - üîó Source: <Migrating From Jenkins to Harness CIE: A Journey | Blog> (https://www.harness.io/blog/jenkins-to-cie-journey)
  - üîó Source: <How to Migrate Off Jenkins: The Road to Modern CI/CD | Blog> (https://www.harness.io/blog/how-to-migrate-off-jenkins-the-road-to-modern-ci-cd)

- Highlight the unique strengths of CloudBees in managing complex CI/CD workflows, particularly for enterprise-scale applications, to differentiate from competitors.
  - üîó Source: <CloudBees vs GitHub Actions: Choosing a CI/CD Platform for Your ...> (https://www.cloudbees.com/blog/cloudbees-vs-github-actions-choosing-a-ci-cd-platform-for-your-business)
  - üîó Source: <GitLab vs. GitHub vs. Harness vs. CloudBees vs Devtron> (https://devtron.ai/blog/gitlab-vs-github-vs-harness-vs-cloudbees-vs-devtron-choosing-the-right-devops-platform/)

- Engage with the community to gather feedback on pain points and desired features, ensuring that CloudBees evolves in alignment with practitioner needs and market trends.
  - üîó Source: <Moving from Jenkins to Harness, any advice and experience you ...> (https://www.reddit.com/r/devops/comments/1lqynqk/moving_from_jenkins_to_harness_any_advice_and/)
  - üîó Source: <Ask HN: Where to Migrate as an IT Support/DevOps Engineer for Work?> (https://news.ycombinator.com/item?id=45942862)
```

---

