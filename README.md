# ğŸ› ï¸ CloudBees Market Watch Agent

A lightweight, VS Codeâ€“friendly DevOps marketâ€‘intelligence agent. It scrapes public sources, extracts trends and sentiment, and generates daily Markdown reports.

This repo now reflects the **latest simplified architecture**:

* **Scraping sources:** Hacker News, competitor blogs, Google Search via **Serper.dev**
* **No Reddit**, **no LinkedIn**, **no SerpAPI**
* **Twoâ€‘phase workflow:** scrape â†’ summarize
* **Safe trend classification**
* **Clean Markdown reporting**

---

## ğŸ“¦ Project Structure

```
CloudBees-Market-Watcher/
â”œâ”€â”€ scrape_only.py          # Collects all posts into data/posts.json
â”œâ”€â”€ summarize_only.py       # Generates summaries + insights
â”œâ”€â”€ summarizer.py           # GPT-based summarization + insight extraction
â”œâ”€â”€ utils.py                # Grouping and report writer
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ competitor.py       # RSS competitor feeds
â”‚   â”œâ”€â”€ google_watcher.py   # Serper.dev search â†’ recent/postive signals
â”‚   â”œâ”€â”€ hn.py               # Hacker News RSS
â”‚   â”œâ”€â”€ trend_classifier.py # Regex-based trend matching
â”‚   â”œâ”€â”€ competitors.yaml    # Feed list
â”‚   â”œâ”€â”€ hn.yaml             # Feed list
â”‚   â””â”€â”€ reddit.yaml         # (unused)
â””â”€â”€ data/
â””â”€â”€ reports/
```

---

## ğŸš€ What the Agent Does

### 1. Scrapes:

* **Hacker News** (filtered feeds)
* **Competitor blogs/changelogs**
* **Google Search results** via **Serper.dev** using targeted queries:

  * CloudBees vs GitHub/GitLab
  * Jenkins upgrade issues
  * DORA metrics
  * Internal Developer Platforms
  * Migration patterns (Jenkins â†’ Harness, etc.)

### 2. Normalizes all posts into structured JSON

Each item contains:

```
{
  title,
  url,
  summary,
  source,
  type (Product Update / Social Buzz / Trend),
  is_trend: true/false
}
```

### 3. Summarizes into humanâ€‘readable Markdown

* Product Updates
* Social Buzz
* Trends
* Insights (pain points, sentiment, opportunities)

### 4. Outputs a daily report at:

```
reports/YYYY-MM-DD.md
```

---

## ğŸ§ª Local Setup

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Add required environment variables

This agent now uses **Serper.dev** (NOT SerpAPI).

GitHub Actions â†’ Settings â†’ Secrets â†’ Actions:

```
SERPER_API_KEY=<your-serper-dev-key>
OPENAI_API_KEY=<your-openai-key>
```

### 3. Run manually

```bash
python scrape_only.py
python summarize_only.py
```

---

## ğŸ§  Notes on Architecture

### Why no Reddit/LinkedIn?

* We shifted to **Google â†’ Reddit/LinkedIn/Medium/YouTube** discovery using Serper.
* No direct scraping reduces breakage and TOS issues.

### Why two phases?

* Cloud/CI runs can fail midâ€‘scrape; separating summarization keeps reports deterministic.

### Trend classifier

Uses keyword hits from:

* GitOps
* Platform Engineering / IDP
* DORA/Flow Metrics
* K8s, DevSecOps, AI-inâ€‘DevOps

You can extend this in `scraper/trend_classifier.py`.

---

## ğŸ“¤ GitHub Actions Automation

The Action runs:

1. `python scrape_only.py`
2. Saves `data/posts.json`
3. `python summarize_only.py`
4. Uploads report artifact

A scheduled workflow (e.g., daily UTC) is recommended.

---

## ğŸ§© Next Improvements

* Sentiment scoring per post
* Notion/Supabase sync
* Weekly delta comparison
* Auto-deduping Google organic results

---

## ğŸ¤ Contributions

PRs welcomeâ€”especially additional feed sources or report enhancements.

If you want help wiring CI, adding Slack notifications, or expanding trend logic, just ask! ğŸš€
