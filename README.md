# ğŸ› ï¸ CloudBees Market Watch Agent

A lightweight DevOps market-intelligence agent that scrapes public sources, extracts trends and sentiment, and generates clean Markdown reports.  
Designed for **VS Code**, **GitHub Actions**, and **absolute reliability** in unattended daily runs.

This repo uses the **latest simplified + hardened architecture**, including:

- **Scraping sources:** Hacker News, competitor blogs, Google Search via **Serper.dev**
- **No Reddit**, **no LinkedIn**, **no SerpAPI**
- **Two-phase pipeline:**  
  `scrape_only.py` â†’ `summarize_only.py`
- **HTML-cleaned + truncated summaries** (fast + clean GPT prompts)
- **Safe trend classifier**
- **Dedupe + retry-hardened scrapers**
- **Consistent Markdown reporting**

---

## ğŸ“¦ Project Structure

CloudBees-Market-Watcher/
â”œâ”€â”€ scrape_only.py # Collects posts â†’ data/posts.json
â”œâ”€â”€ summarize_only.py # Summaries + insights â†’ reports/YYYY-MM-DD.md
â”œâ”€â”€ summarizer.py # GPT summarization + insight extraction
â”œâ”€â”€ utils.py # Grouping + Markdown report writer
â”œâ”€â”€ main.py # (Optional) combined pipeline for local runs
â”œâ”€â”€ scraper/
â”‚ â”œâ”€â”€ competitor.py # Hardened RSS competitor scraper (HTML-safe)
â”‚ â”œâ”€â”€ google_watcher.py # Serper.dev search, deduped + retried
â”‚ â”œâ”€â”€ hn.py # Hacker News RSS + HTML cleanup
â”‚ â”œâ”€â”€ trend_classifier.py # Keyword-based trend tagging
â”‚ â”œâ”€â”€ competitors.yaml # Feed list
â”‚ â”œâ”€â”€ hn.yaml # Feed list
â”‚ â””â”€â”€ reddit.yaml # (Unused â€” historical)
â””â”€â”€ data/
â””â”€â”€ reports/

yaml
Copy code

---

## ğŸš€ What the Agent Does

### 1. Scrapes:

- **Hacker News** (filtered CI/CD/DevOps topics)
- **Competitor blogs** (GitHub/GitLab/CircleCI/Harness/etc.)
- **Google Search** (Serper.dev) using targeted queries:
  - Jenkins upgrade issues
  - CloudBees vs GitHub/GitLab
  - Migration patterns (Jenkins â†’ Harness)
  - DORA metrics / flow metrics
  - Internal Developer Platform (IDP) ecosystem
  - DevOps tooling reviews

### 2. Cleans & normalizes into structured JSON

Each item includes:

```json
{
  "title": "...",
  "url": "...",
  "summary": "clean text...",
  "source": "Google | Competitors | HackerNews",
  "type": "ğŸš€ Product Updates | ğŸ’¬ Social Buzz | ğŸ“ˆ Trends",
  "is_trend": true/false
}
All summaries are:

HTML-stripped

Truncated to ~300 chars

Safe for GPT input

3. Summarizes into Markdown
Organized into:

ğŸš€ Product Updates

ğŸ’¬ Social Buzz

ğŸ“ˆ Trends

ğŸ§  Insights (AI-generated)

Key Trends

Pain Points

Opportunities for CloudBees

Market Sentiment Indicators

4. Outputs a daily report:
css
Copy code
reports/YYYY-MM-DD.md
ğŸ§ª Local Setup
1. Install dependencies
bash
Copy code
pip install -r requirements.txt
2. Add environment variables
In .env or shell:

bash
Copy code
export SERPER_API_KEY=<your-serper-dev-key>
export OPENAI_API_KEY=<your-openai-key>
3. Run manually
bash
Copy code
python scrape_only.py
python summarize_only.py
(Optional) Use combined runner:
bash
Copy code
python main.py
ğŸ§  Architecture Notes
Why HTML cleanup?
Competitor RSS feeds often embed full blog HTML.
We now clean all HTML + truncate long summaries â†’
Cleaner reports + faster + cheaper GPT calls.

Why dedupe?
Google can repeat the same result across multiple queries.
We now dedupe globally per run.

Why two phases?
Scrape failures shouldnâ€™t block summarization.
Artifacts allow debugging raw scraped data.

Trend classifier
Simple keyword-based classifier covering:

GitOps

Platform Engineering

Internal Developer Platforms (IDP)

AI-in-DevOps

Supply chain / SBOM

DORA/Flow metrics

Migration/modernization

Extend in scraper/trend_classifier.py.

ğŸ“¤ GitHub Actions Automation
The workflow:

Checks out repo

Installs dependencies

Runs scrape_only.py

Uploads data/posts.json for debugging

Runs summarize_only.py

Uploads final report

Secrets required:

nginx
Copy code
SERPER_API_KEY
OPENAI_API_KEY
Schedule example:

yaml
Copy code
schedule:
  - cron: "0 9 * * *"
ğŸ§© Future Improvements
Slack/Teams notifications

Notion/Supabase sync

Weekly trend deltas

ML-based sentiment scoring

Auto-tagging of topics

ğŸ¤ Contributions
PRs welcomeâ€”especially:

New blog feeds

Better trend rules

New search queries

Report formatting improvements

If you want help extending CI, adding alerts, or plugging into databases, just ask! ğŸš€

yaml
Copy code

---

# ğŸ‰ README is done  
If you want:  
âœ… a badge for GHA status  
âœ… auto-commit reports back to the repo  
âœ… Slack notifications  
âœ… or a cleaner TOC

Just tell me!
