import os
import json
from datetime import datetime
from summarizer import generate_summary, extract_insights_from_social
from utils import group_posts_by_topic, write_report

# Load scraped posts
with open("data/raw_posts.json", "r") as f:
    posts = json.load(f)

print("âœï¸ Generating summary...")

# Group posts by topic
grouped = group_posts_by_topic(posts)

# Log number of posts per section
print(f"ğŸš€ Product Updates: {len(grouped.get('ğŸš€ Product Updates', []))} posts")
print(f"ğŸ’¬ Social Buzz: {len(grouped.get('ğŸ’¬ Social Buzz', []))} posts")
print(f"ğŸ“ˆ Trends: {len(grouped.get('ğŸ“ˆ Trends', []))} posts")

# Generate summaries for each category
summary_sections = {
    "ğŸš€ Product Updates": generate_summary(grouped.get("ğŸš€ Product Updates", [])),
    "ğŸ’¬ Social Buzz": generate_summary(grouped.get("ğŸ’¬ Social Buzz", [])),
    "ğŸ“ˆ Trends": generate_summary(grouped.get("ğŸ“ˆ Trends", [])),
    "ğŸ§  Insights": extract_insights_from_social(grouped.get("ğŸ’¬ Social Buzz", []))
}

# Write markdown report and get path
report_path = write_report(summary_sections)

print(f"\nâœ… Report written to {report_path}")
print("\n===== ğŸ“° Final Market Watch Report =====\n")

# Print contents of report
if report_path:
    with open(report_path, "r") as f:
        print(f.read())

print("âœ… Summary report generated!")
