import os
import json
from datetime import datetime
from summarizer import generate_summary, extract_insights_from_social
from utils import group_posts_by_topic, write_report

# Load scraped posts
with open("data/raw_posts.json", "r") as f:
    posts = json.load(f)

# Normalize keys: ensure every post has 'link' and 'category'
for post in posts:
    if "link" not in post and "url" in post:
        post["link"] = post["url"]
    if "category" not in post and "type" in post:
        post["category"] = post["type"]

print("âœï¸ Generating summary...")

# Group posts by category
grouped = group_posts_by_topic(posts)

# Debug: print how many posts in each category
print(f"ğŸš€ Product Updates: {len(grouped.get('ğŸš€ Product Updates', []))} posts")
print(f"ğŸ’¬ Social Buzz: {len(grouped.get('ğŸ’¬ Social Buzz', []))} posts")
print(f"ğŸ“ˆ Trends: {len(grouped.get('ğŸ“ˆ Trends', []))} posts")

# Collect and print all post links (for debug)
print("ğŸ“Œ Collected Links:")
for post in posts:
    title = post.get("title", "No title")
    link = post.get("link", "[No link]")
    print(f"- {title}: {link}")

# Generate summaries
summary_sections = {
    "ğŸš€ Product Updates": generate_summary(grouped.get("ğŸš€ Product Updates", [])),
    "ğŸ’¬ Social Buzz": generate_summary(grouped.get("ğŸ’¬ Social Buzz", [])),
    "ğŸ“ˆ Trends": generate_summary(grouped.get("ğŸ“ˆ Trends", [])),
    "ğŸ§  Insights": extract_insights_from_social(grouped.get("ğŸ’¬ Social Buzz", []))
}

# Write markdown report
report_path = write_report(summary_sections)

# Print the report path and content
if report_path:
    print(f"âœ… Report written to {report_path}\n")
    with open(report_path, "r") as f:
        print("===== ğŸ“° Final Market Watch Report =====\n")
        print(f.read())
else:
    print("âš ï¸ Report path not returned.")

print("âœ… Summary report generated!")
