from utils import group_posts_by_topic, write_report
from summarizer import generate_summary, extract_insights_from_social
from scraper.trend_classifier import classify_trends
import json
import os


print("âœï¸ Generating summary...")


# Load scraped posts
with open("data/posts.json", "r", encoding="utf-8") as f:
posts = json.load(f)


# Re-classify trends (defensive)
for post in posts:
matches = classify_trends([post])
post["is_trend"] = len(matches) > 0


# Group posts
grouped = group_posts_by_topic(posts)


summary_sections = {
"ğŸš€ Product Updates": generate_summary(grouped.get("ğŸš€ Product Updates", [])),
"ğŸ’¬ Social Buzz": generate_summary(grouped.get("ğŸ’¬ Social Buzz", [])),
"ğŸ“ˆ Trends": generate_summary(grouped.get("ğŸ“ˆ Trends", [])),
}


# Insights section
summary_sections["ğŸ§  Insights"] = extract_insights_from_social(posts)


# Write report
report_path = write_report(summary_sections)
print(f"âœ… Report written to {report_path}")
